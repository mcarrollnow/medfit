(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[4914],{23822:(e,a,t)=>{"use strict";t.d(a,{Editor:()=>n});var i=t(66195),r=t(57793),o=t(11292);function n(e){var a,t,n;let{session:s,requestCount:l,playgroundId:p,initialChatData:m,isModelDatabaseEnabled:u}=e;globalThis.marker="mark";let{data:d}=(0,r.Do)(),c=new URL(null!==(n=null==s?void 0:null===(a=s.user)||void 0===a?void 0:a.avatar)&&void 0!==n?n:"https://vercel.com/api/www/avatar");return c.searchParams.set("s","56"),(0,i.jsx)(o.UI,{session:s,chatIds:d,requestCount:l,initialChatData:m,readonly:!1,chatAvatar:(null==s?void 0:null===(t=s.user)||void 0===t?void 0:t.username)?c.toString():void 0,playgroundId:p,isModelDatabaseEnabled:u})}n.displayName="Playground"},11292:(e,a,t)=>{"use strict";t.d(a,{UI:()=>aW});var i=t(66195),r=t(73762),o=t(35146),n=t(60038);function s(e){let{ref:a,header:t,id:o,footer:s,className:m,children:u,overscrollBehavior:d="auto",shouldAutoScroll:c}=e,g=(0,r.useRef)(0),h=(0,r.useRef)(null),v=(0,r.useRef)(null),[f,w]=(0,r.useState)(!1),b=l(o),x=p(o),y=(0,r.useRef)(c);(0,r.useImperativeHandle)(a,()=>({startAutoScroll:()=>{y.current=!0,k()}}));let k=(0,r.useCallback)(()=>{let e=h.current;e&&(e.scrollTop=e.scrollHeight+1e5)},[]);return(0,r.useEffect)(()=>{let e=h.current,a=()=>{if(!e)return;let a=e.scrollHeight-e.scrollTop,t=5;a>g.current+t&&a>e.clientHeight+t?y.current=!1:e.scrollHeight-e.scrollTop<=e.clientHeight+t&&(y.current=!0),g.current=a};return null==e||e.addEventListener("scroll",a),()=>{null==e||e.removeEventListener("scroll",a)}},[]),(0,r.useEffect)(()=>{let e=new MutationObserver(()=>{c&&y.current&&k()});return v.current&&e.observe(v.current,{childList:!0,subtree:!0,characterData:!0}),()=>e.disconnect()},[]),(0,r.useEffect)(()=>{w(!0),c&&k()},[]),(0,i.jsx)(i.Fragment,{children:(0,i.jsx)("div",{className:(0,n.cn)("h-full w-full overflow-hidden",m),children:(0,i.jsxs)("div",{id:b,className:(0,n.cn)("flex flex-col flex-no-wrap h-full overflow-y-auto overscroll-y-none"),style:{overflowAnchor:"none"},ref:h,children:[t&&(0,i.jsx)("div",{className:"sticky top-0 z-10 flex-shrink-0 min-w-0 min-h-0",children:t}),(0,i.jsx)("div",{className:"flex-1 min-w-0",children:(0,i.jsx)("div",{id:x,ref:v,className:(0,n.cn)("scrolling-touch scrolling-gpu h-full w-full relative overflow-auto",{"overscroll-y-auto":"auto"===d,"overscroll-y-none":"none"===d,invisible:!f&&c}),children:u})}),s&&(0,i.jsx)("div",{className:"sticky bottom-0 flex-shrink-0 min-w-0 min-h-0",children:s})]})})})}function l(e){return"scroll-container-".concat(e)}function p(e){return"scroll-container-inner-".concat(e)}function m(e){let{shouldAutoScroll:a,ids:t,children:r}=e;return(0,i.jsxs)(i.Fragment,{children:[r,(0,i.jsx)("script",{dangerouslySetInnerHTML:{__html:"\n          if (".concat(a,") {\n            document.addEventListener('DOMContentLoaded', function() {\n              [").concat(t.map(e=>"'".concat(e,"'")),"].forEach(function(id) {\n                const scrollContainerId = 'scroll-container-' + id;\n                const scrollContainerInnerId = 'scroll-container-inner-' + id;\n                var element = document.getElementById(scrollContainerId);\n                var scrolled = document.getElementById(scrollContainerInnerId);\n                if (element && scrolled) {\n                  element.scrollTop = 100000;\n                  scrolled.classList.remove(\"invisible\");\n                }\n              });\n            });\n          }\n        ")}})]})}var u=t(1475),d=t(97299),c=t(29157);let g=(0,r.memo)(function(e){let{disabled:a,value:t,onKeyDown:o,onChange:n,placeholder:s,className:l,...p}=e,m=(0,r.useCallback)(e=>{null==n||n(e)},[n]);return(0,i.jsx)(c.Z,{disabled:a,value:t,onKeyDown:o,onChange:m,placeholder:s,className:l,spellCheck:!1,...p})},(e,a)=>e.value===a.value&&e.disabled===a.disabled&&e.className===a.className&&e.onChange===a.onChange&&e.onKeyDown===a.onKeyDown);var h=t(77476),v=t(44869),f=t(34711),w=t(20751),b=t(19567);function x(e){return(0,i.jsx)(b.u,{delay:!0,text:e.loading?"Stop Generating":"Send Message",position:"top",tabIndex:-1,desktopOnly:!0,children:(0,i.jsx)(w.Button,{type:"tertiary",size:"small",shape:"square",svgOnly:!0,"aria-label":e.loading?"Stop Generating":"Send Message",...e.loading?{onClick:e.onStop}:{onClick:e.onClick},disabled:e.disabled,children:(0,i.jsx)("span",{className:"flex p-2 text-gray-900 rounded-md pointer-events-none",children:e.loading?(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(v.$,{className:"size-4"}),(0,i.jsx)(h.Z,{className:"size-4 p-1.5"})]}):(0,i.jsx)(i.Fragment,{children:(0,i.jsx)(f.$,{className:"size-4"})})})})})}let y=(0,r.memo)(x,(e,a)=>e.loading===a.loading&&e.disabled===a.disabled);var k=t(96915),P=t(13458),C=t(32964),M=t(60719),N=t(62953),A=t(60168);function S(e){return(0,i.jsx)("svg",{viewBox:"0 0 16 16",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",...e,children:(0,i.jsx)("path",{fillRule:"evenodd",clipRule:"evenodd",d:"M12.0607 6.74999L11.5303 7.28032L8.7071 10.1035C8.31657 10.4941 7.68341 10.4941 7.29288 10.1035L4.46966 7.28032L3.93933 6.74999L4.99999 5.68933L5.53032 6.21966L7.99999 8.68933L10.4697 6.21966L11 5.68933L12.0607 6.74999Z",fill:"currentColor"})})}var T=t(29374);function U(e){let{children:a}=e;return(0,i.jsx)(A.m.div,{animate:{backgroundPosition:["100% 50%","-100% 50%"]},transition:{duration:1.5,repeat:Number.POSITIVE_INFINITY,ease:"linear"},style:{background:"linear-gradient(90deg, hsl(var(--gray-400)) 0%, hsl(var(--gray-400)) 35%, hsl(var(--gray-900)) 50%, hsl(var(--gray-400)) 65%, hsl(var(--gray-400)) 100%)",backgroundSize:"200% 100%",WebkitBackgroundClip:"text",backgroundClip:"text"},className:"flex items-center text-transparent",children:a})}function q(e){let{content:a,modelName:t,finishedReasoning:o,seconds:s}=e,[l,p]=(0,r.useState)(o?void 0:"thinking"),m=o?void 0:l;if(!a||0===a.trim().length)return null;let u=void 0!==s?Math.floor(s):void 0,d=0===u||void 0===u?"some time":"".concat(u," ").concat(1===u?"second":"seconds"),c=o?(0,i.jsxs)(i.Fragment,{children:[t," thought for ",d]}):(0,i.jsxs)(i.Fragment,{children:[t," is thinking"]}),g=(0,i.jsxs)("div",{className:(0,n.cn)("pt-[2.5px] group/thinking flex w-full min-w-0 flex-1 flex-row items-center gap-1"),children:[(0,i.jsx)("div",{className:"transition-colors group-hover/thinking:text-gray-900 text-left",children:c}),a?(0,i.jsx)("div",{className:"translate-y-[1px] transition-all group-[&[data-state=open]]/trigger:rotate-180",children:(0,i.jsx)(S,{className:"!text-gray-800 group-hover/thinking:!text-gray-900"})}):null]});return(0,i.jsx)(M.fC,{className:"text-gray-800 pb-[1.25em]",type:"single",collapsible:!0,value:m,onValueChange:e=>p(e),children:(0,i.jsxs)(M.ck,{value:"thinking",children:[(0,i.jsx)(M.xz,{className:(0,n.cn)("group/trigger w-full",!a&&"pointer-events-none"),children:(0,i.jsx)(N.M,{mode:"wait",children:(0,i.jsx)(A.m.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.2},className:"flex",children:o?g:(0,i.jsx)(U,{children:g})},t||"loading")})}),(a||"thinking"===m)&&(0,i.jsx)(M.VY,{className:"group/content data-[state=open]:animate-accordion-down data-[state=closed]:animate-accordion-up overflow-hidden pt-2",children:(0,i.jsx)("div",{className:"transition-[filter,opacity] duration-500 group-data-[state=closed]/content:opacity-0 group-data-[state=open]/content:opacity-100 group-data-[state=closed]/content:blur-lg group-data-[state=open]/content:blur-0",children:(0,i.jsx)(T.MemoizedReactMarkdown,{className:(0,n.cn)("message-markdown dark:prose-invert prose-sm sm:prose-base prose-pre:bg-transparent prose-pre:p-0 prose-p:whitespace-pre-wrap prose-p:break-words w-full flex-1 leading-6 prose-p:leading-7 max-w-full"),children:a})})})]})})}var H=t(1392),I=t(39955),L=t(55189),O=t(55865),D=t(48565),B=t(18431);let j="​";function z(){return{role:"assistant",parts:[{type:"text",text:j}],id:Math.random().toString()}}var G=t(38040),W=t(86156),V=t(7245),E=t(4291),F=t(14616);let _=e=>{var a;let{attachment:t}=e,[o,n]=(0,r.useState)(!1);return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)("div",{className:"w-48 h-32 rounded-lg border relative cursor-pointer",onClick:()=>{n(!0)},children:(0,i.jsx)(B.default,{src:t.url,unoptimized:!0,fill:!0,className:"object-cover rounded-lg hover:brightness-95 transition-all duration-150 ease-in-out",alt:t.name||"Upload image"})},t.name),(0,i.jsxs)(V.u.Modal,{active:o,onClickOutside:()=>n(!1),children:[(0,i.jsx)(V.u.Body,{children:(0,i.jsx)("div",{className:"w-full h-96 rounded-lg relative cursor-pointer flex flex-row justify-center items-center",onClick:()=>{n(!0)},children:(0,i.jsx)("img",{src:t.url,className:"object-contain rounded-lg h-full scale-90",alt:null!==(a=t.name)&&void 0!==a?a:"Attachment preview"})},t.name)}),(0,i.jsx)(V.u.Actions,{children:(0,i.jsx)(V.u.Action,{onClick:()=>n(!1),type:"default",fullWidth:!0,children:"Close"})})]})]})},K=e=>{var a;let{attachment:t,removeAttachment:o=()=>{}}=e,[s,l]=(0,r.useState)(!1);return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)("div",{className:"w-20 h-16 rounded-lg relative cursor-pointer",children:[(0,i.jsx)(B.default,{src:t.url,unoptimized:!0,fill:!0,className:(0,n.cn)("object-cover rounded-lg border hover:brightness-95 transition-all duration-150 ease-in-out",{"brightness-90":"uploading"===t.status}),alt:"Uploaded image",onClick:()=>{l(!0)}}),"uploading"===t.status&&(0,i.jsx)("div",{className:"absolute left-0 top-0 flex flex-row justify-center items-center size-full",children:(0,i.jsx)(E.$,{})}),(0,i.jsx)("button",{type:"button",className:"p-0.5 bg-white border rounded-full absolute -top-2 -right-2 dark:bg-zinc-800",onClick:()=>{o(t)},children:(0,i.jsx)(F.U,{})})]},t.name),(0,i.jsxs)(V.u.Modal,{active:s,onClickOutside:()=>l(!1),children:[(0,i.jsx)(V.u.Body,{children:(0,i.jsx)("div",{className:"w-full h-96 rounded-lg relative cursor-pointer flex flex-row justify-center items-center",onClick:()=>{l(!0)},children:(0,i.jsx)("img",{src:t.url,className:"object-contain rounded-lg h-full scale-90",alt:null!==(a=t.name)&&void 0!==a?a:"Attachment preview"})},t.name)}),(0,i.jsx)(V.u.Actions,{children:(0,i.jsx)(V.u.Action,{onClick:()=>l(!1),type:"default",fullWidth:!0,children:"Close"})})]})]})},Q=(0,r.lazy)(()=>Promise.resolve().then(t.bind(t,29374)).then(e=>({default:e.MemoizedReactMarkdown}))),R=(0,r.memo)(function(e){var a,t;let{message:o,modelName:s,modelId:l,modelIcon:p,chatAvatar:m,onSubmit:u,modelSupportsVision:g,readonly:h,messageLoading:v,isLastMessage:f,messages:w,chatId:b,playgroundId:x,session:y,supportsDownvote:M}=e,[N,A]=(0,r.useState)((0,G.RR)(o)),[S,T]=(0,r.useState)(!1),{formRef:U,onKeyDown:z}=(0,d.F)(),{isCopied:V,copyToClipboard:E}=(0,D.m)({timeout:2e3}),F=(0,r.useRef)(),K=M&&"assistant"===o.role&&f;(0,r.useEffect)(()=>{A((0,G.RR)(o))},[o]),(0,r.useEffect)(()=>{!o.parts.filter(e=>"text"===e.type).length&&v||F.current||(F.current=Date.now())},[o,v]);let R=o.parts.filter(e=>"file"===e.type);return(0,i.jsxs)("div",{className:(0,n.cn)("px-3 @md:py-4 py-2.5 group transition-opacity message",{"bg-zinc-100 dark:bg-zinc-900":"user"===o.role,"message-editing":S}),children:[(0,i.jsxs)("div",{className:"flex items-start max-w-2xl mx-auto space-x-3",children:["user"===o.role?m?(0,i.jsx)(B.default,{className:"size-6 shrink-0 mt-[2px] rounded-full select-none",src:m,width:24,height:24,alt:"Avatar",priority:!1}):(0,i.jsx)(H.Z,{className:"size-6 shrink-0 mt-[2px]"}):(0,i.jsx)("div",{className:"size-6 flex shrink-0 justify-center items-center mt-[2px]",children:p}),S?(0,i.jsx)(i.Fragment,{children:(0,i.jsxs)("form",{className:"flex-1 w-full text-sm sm:text-base",ref:U,onSubmit:e=>{e.preventDefault(),u(N),T(!1)},children:[(0,i.jsx)(c.Z,{value:N,onKeyDown:z,onChange:e=>A(e.target.value),placeholder:"Type your message…",className:(0,n.cn)("flex-1 text-sm sm:text-base w-full leading-7 sm:leading-7 border-0 bg-transparent focus:ring-0 p-0 resize-none"),style:{fontFamily:P.W3},spellCheck:!1,autoFocus:!0}),(0,i.jsxs)("div",{className:"flex items-center justify-center gap-2 mt-2",children:[(0,i.jsx)(C.u,{content:"Save and continue conversation from this point",children:(0,i.jsx)("div",{children:(0,i.jsxs)(k.z,{type:"submit",className:"gap-1 font-medium shadow-sm cursor-default hover:bg-zinc-50",children:[(0,i.jsx)(I.Z,{className:"size-4"}),"Save & Submit"]})})}),(0,i.jsx)(k.z,{type:"button",className:"font-medium shadow-sm cursor-default hover:bg-zinc-50",onClick:()=>{A((0,G.RR)(o)),T(!1)},children:"Cancel"})]})]})}):(0,i.jsx)(i.Fragment,{children:(0,i.jsxs)("div",{className:(0,n.cn)("w-full min-w-0 text-sm sm:text-base",{"message-streaming":v&&"user"!==o.role,"animate-pulse":v&&"user"!==o.role&&(0,G.RR)(o)===j}),style:{fontFamily:P.W3},children:[o.parts.filter(e=>"reasoning"===e.type).length?(0,i.jsx)(q,{content:(0,G.lG)(o),modelName:s,seconds:(null!==(t=null===(a=o.metadata)||void 0===a?void 0:a.reasoningDurationInMs)&&void 0!==t?t:0)/1e3,finishedReasoning:!v}):null,(0,G.RR)(o)&&(0,i.jsx)(Q,{className:(0,n.cn)("message-markdown prose dark:prose-invert prose-sm sm:prose-base prose-pre:bg-transparent prose-pre:p-0 prose-p:whitespace-pre-wrap prose-p:break-words w-full flex-1 leading-6 prose-p:leading-7 max-w-full"),children:(0,G.RR)(o)})]})}),h?null:(0,i.jsxs)("div",{className:"flex items-start mt-[2px] self-end",children:["user"===o.role?(0,i.jsx)(C.u,{content:"Edit this message",children:(0,i.jsx)("div",{className:"",children:(0,i.jsx)("button",{type:"button",className:"p-1 text-gray-900 transition-opacity rounded opacity-0 cursor-pointer group-hover:delay-200 group-hover:opacity-100 group-hover:hover:text-zinc-700",onClick:()=>{A((0,G.RR)(o)),T(!0)},disabled:S,"aria-label":"Edit",children:(0,i.jsx)(L.Z,{className:"size-4"})})})}):null,K&&(0,i.jsx)(C.u,{content:"Downvote this message",children:(0,i.jsx)(W.o,{chatId:b,messages:w,playgroundId:x,userId:null==y?void 0:y.user.id,username:void 0,className:(0,n.cn)("opacity-0",{"opacity-100":!v})})}),(0,i.jsx)(C.u,{content:"Copy message to clipboard",children:(0,i.jsx)("button",{type:"button",className:(0,n.cn)("p-1 transition-opacity rounded opacity-0 cursor-pointer group-hover:delay-200 text-gray-900 group-hover:hover:text-zinc-700",{"opacity-100":f&&!v,"group-hover:opacity-100":!v}),"aria-label":"Copy",onClick:()=>E((0,G.RR)(o)),children:V?(0,i.jsx)(I.Z,{className:"size-4"}):(0,i.jsx)(O.Z,{className:"size-4"})})})]})]}),g&&R.length>0?(0,i.jsx)("div",{className:"flex gap-2 max-w-2xl mt-4 mx-auto px-9",children:R.map(e=>{let{url:a,mediaType:t}=e,r={url:a,contentType:t,name:"User Uploaded Image"};if(t.startsWith("image"))return(0,i.jsx)(_,{attachment:r},a)})}):null]})},(e,a)=>!1);var Z=t(52662),Y=t(45217),J=t(63528),$=t(57793),X=t(42813);let ee="\n\nHuman:",ea=[{id:"alibaba:qwen3-next-80b-a3b-instruct",name:"Qwen3 Next 80B A3B Instruct",provider:"alibaba",providerHumanName:"Alibaba Cloud",makerHumanName:"Alibaba Cloud",modelApiName:"qwen3-next-80b-a3b-instruct",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"alibaba",model:{primary:"qwen3-next-80b-a3b-instruct"}},info:{description:"A new generation of open-source, non-thinking mode model powered by Qwen3. This version demonstrates superior Chinese text understanding, augmented logical reasoning, and enhanced capabilities in text generation tasks over the previous iteration (Qwen3-235B-A22B-Instruct-2507).",website:"https://www.alibabacloud.com/product/model-studio",modelUrl:"https://modelstudio.console.alibabacloud.com/?tab=doc#/doc/?type=model&url=2840914_2&modelId=qwen3-next-80b-a3b-instruct",contextWindow:131072,pricing:{pricingUrl:"https://www.alibabacloud.com/help/en/model-studio/models",inputCostPerMil:"0.5",outputCostPerMil:"2"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:16384,range:[0,32768]},topP:{value:1,range:[0,1]},topK:{value:50,range:[0,100]},presencePenalty:{value:0,range:[-2,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-09-12"},{id:"alibaba:qwen3-next-80b-a3b-thinking",name:"Qwen3 Next 80B A3B Thinking",provider:"alibaba",providerHumanName:"Alibaba Cloud",makerHumanName:"Alibaba Cloud",modelApiName:"qwen3-next-80b-a3b-thinking",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"alibaba",model:{primary:"qwen3-next-80b-a3b-thinking"}},info:{description:"A new generation of Qwen3-based open-source thinking mode models. This version offers improved instruction following and streamlined summary responses over the previous iteration (Qwen3-235B-A22B-Thinking-2507).",website:"https://www.alibabacloud.com/product/model-studio",modelUrl:"https://modelstudio.console.alibabacloud.com/?tab=doc#/doc/?type=model&url=2840914_2&modelId=qwen3-next-80b-a3b-thinking",contextWindow:131072,pricing:{pricingUrl:"https://www.alibabacloud.com/help/en/model-studio/models",inputCostPerMil:"0.5",outputCostPerMil:"6"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:16384,range:[0,32768]},topP:{value:1,range:[0,1]},topK:{value:50,range:[0,100]},presencePenalty:{value:0,range:[-2,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-09-12"},{id:"alibaba:qwen3-max-preview",name:"Qwen3 Max Preview",provider:"alibaba",providerHumanName:"Alibaba Cloud",makerHumanName:"Alibaba Cloud",modelApiName:"qwen3-max-preview",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"alibaba",model:{primary:"qwen3-max-preview"}},info:{description:"Qwen3-Max-Preview shows substantial gains over the 2.5 series in overall capability, with significant enhancements in Chinese-English text understanding, complex instruction following, handling of subjective open-ended tasks, multilingual ability, and tool invocation; model knowledge hallucinations are reduced.",website:"https://www.alibabacloud.com/product/model-studio",modelUrl:"https://modelstudio.console.alibabacloud.com/?tab=doc#/doc/?type=model&url=2840914_2&modelId=qwen3-max-preview",contextWindow:262144,pricing:{pricingUrl:"https://www.alibabacloud.com/help/en/model-studio/models#161e7d77cfv1n",inputCostPerMil:"1.2",outputCostPerMil:"6",cachedInputCostPerMil:"0.24"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:16384,range:[0,32768]},topP:{value:1,range:[0,1]},topK:{value:50,range:[0,100]},presencePenalty:{value:0,range:[-2,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-09-24"},{id:"alibaba:qwen3-max",name:"Qwen3 Max",provider:"alibaba",providerHumanName:"Alibaba Cloud",makerHumanName:"Alibaba Cloud",modelApiName:"qwen3-max",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"alibaba",model:{primary:"qwen3-max"}},info:{description:"The Qwen 3 series Max model has undergone specialized upgrades in agent programming and tool invocation compared to the preview version. The officially released model this time has achieved state-of-the-art (SOTA) performance in its field and is better suited to meet the demands of agents operating in more complex scenarios.",website:"https://www.alibabacloud.com/product/model-studio",modelUrl:"https://modelstudio.console.alibabacloud.com/?tab=doc#/doc/?type=model&url=2840914_2&modelId=qwen3-max",contextWindow:262144,pricing:{pricingUrl:"https://www.alibabacloud.com/help/en/model-studio/models#161e7d77cfv1n",inputCostPerMil:"1.2",outputCostPerMil:"6",cachedInputCostPerMil:"0.24"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:16384,range:[0,32768]},topP:{value:1,range:[0,1]},topK:{value:50,range:[0,100]},presencePenalty:{value:0,range:[-2,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-09-24"},{id:"alibaba:qwen3-vl-235b-a22b-instruct",name:"Qwen3 VL 235B A22B Instruct",provider:"alibaba",providerHumanName:"Alibaba Cloud",makerHumanName:"Alibaba Cloud",modelApiName:"qwen3-vl-235b-a22b-instruct",supportsStructuredOutput:!0,supportsVision:!0,gateway:{chef:"alibaba",model:{primary:"qwen3-vl-instruct",secondary:["qwen3-vl-235b-a22b-instruct"]}},info:{description:"The Qwen3 series VL models has been comprehensively upgraded in areas such as visual coding and spatial perception. Its visual perception and recognition capabilities have significantly improved, supporting the understanding of ultra-long videos, and its OCR functionality has undergone a major enhancement.",website:"https://www.alibabacloud.com/product/model-studio",modelUrl:"https://modelstudio.console.alibabacloud.com/?tab=doc#/doc/?type=model&url=2840914_2&modelId=qwen3-vl-235b-a22b-instruct",contextWindow:131072,pricing:{pricingUrl:"https://www.alibabacloud.com/help/en/model-studio/models#161e7d77cfv1n",inputCostPerMil:"0.7",outputCostPerMil:"2.8"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:16384,range:[0,129024]},topP:{value:1,range:[0,1]},topK:{value:50,range:[0,100]},presencePenalty:{value:0,range:[-2,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-09-24"},{id:"alibaba:qwen3-vl-235b-a22b-thinking",name:"Qwen3 VL 235B A22B Thinking",provider:"alibaba",providerHumanName:"Alibaba Cloud",makerHumanName:"Alibaba Cloud",modelApiName:"qwen3-vl-235b-a22b-thinking",supportsStructuredOutput:!0,supportsVision:!0,gateway:{chef:"alibaba",model:{primary:"qwen3-vl-thinking",secondary:["qwen3-vl-235b-a22b-thinking"]}},info:{description:"Qwen3 series VL models feature significantly enhanced multimodal reasoning capabilities, with a particular focus on optimizing the model for STEM and mathematical reasoning. Visual perception and recognition abilities have been comprehensively improved, and OCR capabilities have undergone a major upgrade.",website:"https://www.alibabacloud.com/product/model-studio",modelUrl:"https://modelstudio.console.alibabacloud.com/?tab=doc#/doc/?type=model&url=2840914_2&modelId=qwen3-vl-235b-a22b-thinking",contextWindow:131072,pricing:{pricingUrl:"https://www.alibabacloud.com/help/en/model-studio/models#161e7d77cfv1n",inputCostPerMil:"0.7",outputCostPerMil:"8.4"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:16384,range:[0,129024]},topP:{value:1,range:[0,1]},topK:{value:50,range:[0,100]},presencePenalty:{value:0,range:[-2,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-09-24"},{id:"alibaba:qwen3-coder-plus",name:"Qwen3 Coder Plus",provider:"alibaba",providerHumanName:"Alibaba Cloud",makerHumanName:"Alibaba Cloud",modelApiName:"qwen3-coder-plus-2025-09-23",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"alibaba",model:{primary:"qwen3-coder-plus",secondary:["qwen3-coder-plus-2025-09-23"]}},info:{description:"Powered by Qwen3, this is a powerful Coding Agent that excels in tool calling and environment interaction to achieve autonomous programming. It combines outstanding coding proficiency with versatile general-purpose abilities.",website:"https://www.alibabacloud.com/product/model-studio",modelUrl:"https://modelstudio.console.alibabacloud.com/?tab=doc#/doc/?type=model&url=2840914_2&modelId=qwen3-coder-plus",contextWindow:1e6,pricing:{pricingUrl:"https://www.alibabacloud.com/help/en/model-studio/models#161e7d77cfv1n",inputCostPerMil:"1",outputCostPerMil:"5"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:16384,range:[0,1e6]},topP:{value:1,range:[0,1]},topK:{value:50,range:[0,100]},presencePenalty:{value:0,range:[-2,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-09-24"},{id:"anthropic:claude-instant-v1",provider:"anthropic",providerHumanName:"Anthropic",modelApiName:"claude-instant-1.2",makerHumanName:"Anthropic",minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,disabled:!0,info:{description:"A faster, cheaper yet still very capable version of Claude, which can handle a range of tasks including casual dialogue, text analysis, summarization, and document comprehension.",website:"https://www.anthropic.com",modelUrl:"https://docs.anthropic.com/en/docs/about-claude/models#legacy-models",contextWindow:1e5,pricing:{pricingUrl:"https://www.anthropic.com/pricing#anthropic-api",inputCostPerMil:"1.63",outputCostPerMil:"5.51"}},parameters:{temperature:{value:1,range:[0,1]},maximumLength:{value:200,range:[50,1024]},topP:{value:1,range:[.1,1]},topK:{value:1,range:[1,500]},presencePenalty:{value:1,range:[0,1]},frequencyPenalty:{value:1,range:[0,1]},stopSequences:{value:[ee],range:[]}},name:"claude-instant-1.2",lastModifiedDate:"2023-03-14"},{id:"anthropic:claude-v1",provider:"anthropic",providerHumanName:"Anthropic",modelApiName:"claude-1.3",makerHumanName:"Anthropic",minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,disabled:!0,info:{description:"An older version of Anthropic's Claude model that excels at a wide range of tasks from sophisticated dialogue and creative content generation to detailed instruction. It is good for complex reasoning, creativity, thoughtful dialogue, coding, and detailed content creation.",website:"https://www.anthropic.com",modelUrl:"https://docs.anthropic.com/en/docs/about-claude/models#legacy-models",contextWindow:1e5,pricing:{pricingUrl:"https://www.anthropic.com/pricing#anthropic-api",inputCostPerMil:"11.02",outputCostPerMil:"32.62"}},parameters:{temperature:{value:1,range:[0,1]},maximumLength:{value:200,range:[50,1024]},topP:{value:1,range:[.1,1]},topK:{value:1,range:[1,500]},presencePenalty:{value:1,range:[0,1]},frequencyPenalty:{value:1,range:[0,1]},stopSequences:{value:[ee],range:[]}},name:"claude-1",lastModifiedDate:"2023-03-14"},{id:"anthropic:claude-v2",provider:"anthropic",providerHumanName:"Anthropic",modelApiName:"claude-2.1",makerHumanName:"Anthropic",minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!1,disabled:!0,info:{description:"Anthropic's most powerful model that excels at a wide range of tasks from sophisticated dialogue and creative content generation to detailed instruction. It is good for complex reasoning, creativity, thoughtful dialogue, coding,and detailed content creation.",website:"https://www.anthropic.com",modelUrl:"https://docs.anthropic.com/en/docs/about-claude/models#legacy-models",contextWindow:1e5,pricing:{pricingUrl:"https://www.anthropic.com/pricing#anthropic-api"}},parameters:{temperature:{value:1,range:[0,1]},maximumLength:{value:200,range:[50,1024]},topP:{value:1,range:[.1,1]},topK:{value:1,range:[1,500]},presencePenalty:{value:1,range:[0,1]},frequencyPenalty:{value:1,range:[0,1]},stopSequences:{value:[ee],range:[]}},name:"Claude 2",lastModifiedDate:"2025-02-19"},{id:"anthropic:claude-v3-opus",provider:"anthropic",providerHumanName:"Anthropic",modelApiName:"claude-3-opus-20240229",makerHumanName:"Anthropic",minBillingTier:"pro",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"Claude 3 Opus is Anthropic's most intelligent model, with best-in-market performance on highly complex tasks. It can navigate open-ended prompts and sight-unseen scenarios with remarkable fluency and human-like understanding. Opus shows us the outer limits of what's possible with generative AI.",website:"https://www.anthropic.com",modelUrl:"https://docs.anthropic.com/claude/docs/models-overview",contextWindow:2e5,pricing:{pricingUrl:"https://www.anthropic.com/pricing#anthropic-api",inputCostPerMil:"15",outputCostPerMil:"75",cachedInputCostPerMil:"1.5",cacheCreationInputCostPerMil:"18.75"}},parameters:{temperature:{value:1,range:[0,1]},maximumLength:{value:1024,range:[50,4096]},topP:{value:1,range:[.1,1]},topK:{value:1,range:[1,500]},presencePenalty:{value:1,range:[0,1]},frequencyPenalty:{value:1,range:[0,1]},stopSequences:{value:[ee],range:[]}},name:"Claude 3 Opus",lastModifiedDate:"2025-02-19"},{id:"anthropic:claude-v3-sonnet",provider:"anthropic",providerHumanName:"Anthropic",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,modelApiName:"claude-3-sonnet-20240229",makerHumanName:"Anthropic",minBillingTier:"pro",disabled:!0,disableInGateway:!0,info:{description:"Claude 3 Sonnet strikes the ideal balance between intelligence and speed—particularly for enterprise workloads. It delivers strong performance at a lower cost compared to its peers, and is engineered for high endurance in large-scale AI deployments.",website:"https://www.anthropic.com",modelUrl:"https://docs.anthropic.com/claude/docs/models-overview",contextWindow:2e5,pricing:{pricingUrl:"https://www.anthropic.com/pricing#anthropic-api",inputCostPerMil:"3",outputCostPerMil:"15"}},parameters:{temperature:{value:1,range:[0,1]},maximumLength:{value:1024,range:[50,4096]},topP:{value:1,range:[.1,1]},topK:{value:1,range:[1,500]},presencePenalty:{value:1,range:[0,1]},frequencyPenalty:{value:1,range:[0,1]},stopSequences:{value:[ee],range:[]}},name:"Claude 3 Sonnet",lastModifiedDate:"2025-02-19"},{id:"anthropic:claude-v3.5-sonnet",provider:"anthropic",providerHumanName:"Anthropic",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,modelApiName:"claude-3-5-sonnet-20241022",makerHumanName:"Anthropic",minBillingTier:"pro",info:{description:"Claude 3.5 Sonnet strikes the ideal balance between intelligence and speed—particularly for enterprise workloads. It delivers strong performance at a lower cost compared to its peers, and is engineered for high endurance in large-scale AI deployments.",website:"https://www.anthropic.com",modelUrl:"https://docs.anthropic.com/claude/docs/models-overview",contextWindow:2e5,pricing:{pricingUrl:"https://www.anthropic.com/pricing#anthropic-api",inputCostPerMil:"3",outputCostPerMil:"15",cachedInputCostPerMil:"0.3",cacheCreationInputCostPerMil:"3.75"}},parameters:{temperature:{value:1,range:[0,1]},maximumLength:{value:1024,range:[50,4096]},topP:{value:1,range:[.1,1]},topK:{value:1,range:[1,500]},presencePenalty:{value:1,range:[0,1]},frequencyPenalty:{value:1,range:[0,1]},stopSequences:{value:[ee],range:[]}},name:"Claude 3.5 Sonnet",lastModifiedDate:"2025-02-19"},{id:"anthropic:claude-v3-haiku",provider:"anthropic",providerHumanName:"Anthropic",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,modelApiName:"claude-3-haiku-20240307",makerHumanName:"Anthropic",minBillingTier:"pro",info:{description:"Claude 3 Haiku is Anthropic's fastest model yet, designed for enterprise workloads which often involve longer prompts. Haiku to quickly analyze large volumes of documents, such as quarterly filings, contracts, or legal cases, for half the cost of other models in its performance tier.",website:"https://www.anthropic.com",modelUrl:"https://docs.anthropic.com/claude/docs/models-overview",contextWindow:2e5,pricing:{pricingUrl:"https://www.anthropic.com/pricing#anthropic-api",inputCostPerMil:"0.25",outputCostPerMil:"1.25",cachedInputCostPerMil:"0.03",cacheCreationInputCostPerMil:"0.3"}},parameters:{temperature:{value:1,range:[0,1]},maximumLength:{value:1024,range:[50,4096]},topP:{value:1,range:[.1,1]},topK:{value:1,range:[1,500]},presencePenalty:{value:1,range:[0,1]},frequencyPenalty:{value:1,range:[0,1]},stopSequences:{value:[ee],range:[]}},name:"Claude 3 Haiku",lastModifiedDate:"2025-02-19"},{id:"anthropic:claude-3.5-haiku",provider:"anthropic",providerHumanName:"Anthropic",supportsVision:!1,supportsStructuredOutput:!0,supportsToolCalling:!0,modelApiName:"claude-3-5-haiku-20241022",makerHumanName:"Anthropic",minBillingTier:"pro",name:"Claude 3.5 Haiku",info:{description:"Claude 3.5 Haiku is the next generation of our fastest model. For a similar speed to Claude 3 Haiku, Claude 3.5 Haiku improves across every skill set and surpasses Claude 3 Opus, the largest model in our previous generation, on many intelligence benchmarks.",website:"https://www.anthropic.com",modelUrl:"https://www.anthropic.com/claude/haiku",contextWindow:2e5,pricing:{pricingUrl:"https://www.anthropic.com/pricing#anthropic-api",inputCostPerMil:"0.8",outputCostPerMil:"4",cachedInputCostPerMil:"0.08",cacheCreationInputCostPerMil:"1"}},parameters:{temperature:{value:1,range:[0,1]},maximumLength:{value:1024,range:[50,4096]},topP:{value:1,range:[.1,1]},topK:{value:1,range:[1,500]},presencePenalty:{value:1,range:[0,1]},frequencyPenalty:{value:1,range:[0,1]},stopSequences:{value:[ee],range:[]}},lastModifiedDate:"2025-02-19"},{id:"anthropic:claude-3.7-sonnet",provider:"anthropic",providerHumanName:"Anthropic",name:"Claude 3.7 Sonnet",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,modelApiName:"claude-3-7-sonnet-20250219",makerHumanName:"Anthropic",minBillingTier:"pro",new:!0,info:{description:"Claude 3.7 Sonnet is the first hybrid reasoning model and Anthropic's most intelligent model to date. It delivers state-of-the-art performance for coding, content generation, data analysis, and planning tasks, building upon its predecessor Claude 3.5 Sonnet's capabilities in software engineering and computer use.",website:"https://www.anthropic.com",modelUrl:"https://www.anthropic.com/claude/sonnet",contextWindow:2e5,pricing:{pricingUrl:"https://www.anthropic.com/pricing#anthropic-api",inputCostPerMil:"3",outputCostPerMil:"15",cachedInputCostPerMil:"0.3",cacheCreationInputCostPerMil:"3.75"}},parameters:{temperature:{value:1,range:[0,1]},maximumLength:{value:1024,range:[50,4096]},topP:{value:1,range:[.1,1]},topK:{value:1,range:[1,500]},presencePenalty:{value:1,range:[0,1]},frequencyPenalty:{value:1,range:[0,1]},stopSequences:{value:[ee],range:[]}},lastModifiedDate:"2025-02-24"},{id:"anthropic:claude-3.7-sonnet-reasoning",provider:"anthropic",providerHumanName:"Anthropic",name:"Claude 3.7 Sonnet Reasoning",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,modelApiName:"claude-3-7-sonnet-20250219",makerHumanName:"Anthropic",minBillingTier:"pro",new:!0,info:{description:"Claude 3.7 Sonnet is the first hybrid reasoning model and Anthropic's most intelligent model to date. It delivers state-of-the-art performance for coding, content generation, data analysis, and planning tasks, building upon its predecessor Claude 3.5 Sonnet's capabilities in software engineering and computer use.",website:"https://www.anthropic.com",modelUrl:"https://www.anthropic.com/claude/sonnet",contextWindow:2e5,pricing:{pricingUrl:"https://www.anthropic.com/pricing#anthropic-api",inputCostPerMil:"3",outputCostPerMil:"15",cachedInputCostPerMil:"0.3",cacheCreationInputCostPerMil:"3.75"}},parameters:{temperature:{value:1,range:[0,1]},maximumLength:{value:1024,range:[50,4096]},topP:{value:1,range:[.1,1]},topK:{value:1,range:[1,500]},presencePenalty:{value:1,range:[0,1]},frequencyPenalty:{value:1,range:[0,1]},stopSequences:{value:[ee],range:[]}},lastModifiedDate:"2025-02-24"},{id:"anthropic:claude-4-opus-20250514",provider:"anthropic",providerHumanName:"Anthropic",modelApiName:"claude-opus-4-20250514",makerHumanName:"Anthropic",minBillingTier:"pro",supportsVision:!1,supportsStructuredOutput:!0,supportsToolCalling:!0,new:!0,info:{description:"Claude Opus 4 is Anthropic's most powerful model yet and the best coding model in the world, leading on SWE-bench (72.5%) and Terminal-bench (43.2%). It delivers sustained performance on long-running tasks that require focused effort and thousands of steps, with the ability to work continuously for several hours—dramatically outperforming all Sonnet models and significantly expanding what AI agents can accomplish.",website:"https://www.anthropic.com",modelUrl:"https://docs.anthropic.com/claude/docs/models-overview",contextWindow:2e5,pricing:{pricingUrl:"https://www.anthropic.com/pricing#anthropic-api",inputCostPerMil:"15",outputCostPerMil:"75",cachedInputCostPerMil:"1.5",cacheCreationInputCostPerMil:"18.75"}},parameters:{temperature:{value:1,range:[0,1]},maximumLength:{value:1024,range:[50,4096]},topP:{value:1,range:[.1,1]},topK:{value:1,range:[1,500]},presencePenalty:{value:1,range:[0,1]},frequencyPenalty:{value:1,range:[0,1]},stopSequences:{value:[ee],range:[]}},name:"Claude Opus 4",lastModifiedDate:"2025-08-12"},{id:"anthropic:claude-4-sonnet-20250514",provider:"anthropic",providerHumanName:"Anthropic",name:"Claude Sonnet 4",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,modelApiName:"claude-sonnet-4-20250514",makerHumanName:"Anthropic",minBillingTier:"pro",new:!0,info:{description:"Claude Sonnet 4 significantly improves on Sonnet 3.7's industry-leading capabilities, excelling in coding with a state-of-the-art 72.7% on SWE-bench. The model balances performance and efficiency for internal and external use cases, with enhanced steerability for greater control over implementations. While not matching Opus 4 in most domains, it delivers an optimal mix of capability and practicality.",website:"https://www.anthropic.com",modelUrl:"https://www.anthropic.com/claude/sonnet",contextWindow:2e5,pricing:{pricingUrl:"https://www.anthropic.com/pricing#anthropic-api",inputCostPerMil:"3",outputCostPerMil:"15",cachedInputCostPerMil:"0.3",cacheCreationInputCostPerMil:"3.75"}},parameters:{temperature:{value:1,range:[0,1]},maximumLength:{value:1024,range:[50,4096]},topP:{value:1,range:[.1,1]},topK:{value:1,range:[1,500]},presencePenalty:{value:1,range:[0,1]},frequencyPenalty:{value:1,range:[0,1]},stopSequences:{value:[ee],range:[]}},lastModifiedDate:"2025-08-12"},{id:"anthropic:claude-opus-4-1-20250805",provider:"anthropic",providerHumanName:"Anthropic",supportsVision:!1,supportsStructuredOutput:!0,supportsToolCalling:!0,modelApiName:"claude-opus-4-1-20250805",makerHumanName:"Anthropic",minBillingTier:"pro",new:!0,info:{description:"Claude Opus 4.1 is a drop-in replacement for Opus 4 that delivers superior performance and precision for real-world coding and agentic tasks. Opus 4.1 advances state-of-the-art coding performance to 74.5% on SWE-bench Verified, and handles complex, multi-step problems with more rigor and attention to detail.",website:"https://www.anthropic.com",modelUrl:"https://docs.anthropic.com/claude/docs/models-overview",contextWindow:2e5,pricing:{pricingUrl:"https://www.anthropic.com/pricing#anthropic-api",inputCostPerMil:"15",outputCostPerMil:"75",cachedInputCostPerMil:"1.5",cacheCreationInputCostPerMil:"18.75"}},parameters:{temperature:{value:1,range:[0,1]},maximumLength:{value:1024,range:[50,32e3]},topK:{value:1,range:[1,500]},presencePenalty:{value:1,range:[0,1]},frequencyPenalty:{value:1,range:[0,1]},stopSequences:{value:[ee],range:[]}},name:"Claude Opus 4.1",lastModifiedDate:"2025-08-12"},{id:"anthropic:claude-4.5-sonnet-20250929",provider:"anthropic",providerHumanName:"Anthropic",name:"Claude Sonnet 4.5",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,modelApiName:"claude-sonnet-4-5-20250929",makerHumanName:"Anthropic",minBillingTier:"hobby",new:!0,gateway:{chef:"anthropic",model:{primary:"claude-sonnet-4.5",secondary:["claude-4.5-sonnet","claude-4.5-sonnet-20250929","claude-sonnet-4.5-20250929","claude-4.5-sonnet-20250929-v1","claude-sonnet-4.5-20250929-v1","anthropic.claude-sonnet-4.5-20250929-v1:0","claude-sonnet-4.5@20250929"]}},info:{description:"Claude Sonnet 4.5 is the newest model in the Sonnet series, offering improvements and updates over Sonnet 4.",website:"https://www.anthropic.com",modelUrl:"https://www.anthropic.com/claude/sonnet",contextWindow:2e5,pricing:{pricingUrl:"https://www.anthropic.com/pricing#anthropic-api",inputCostPerMil:"3",outputCostPerMil:"15",cachedInputCostPerMil:"0.3",cacheCreationInputCostPerMil:"3.75"}},parameters:{temperature:{value:1,range:[0,1]},maximumLength:{value:1024,range:[50,64e3]},topK:{value:1,range:[1,500]},presencePenalty:{value:1,range:[0,1]},frequencyPenalty:{value:1,range:[0,1]},stopSequences:{value:[ee],range:[]}},lastModifiedDate:"2025-09-29"},{id:"anthropic:claude-haiku-4.5-20251001",provider:"anthropic",providerHumanName:"Anthropic",name:"Claude Haiku 4.5",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,modelApiName:"claude-haiku-4-5-20251001",makerHumanName:"Anthropic",minBillingTier:"hobby",new:!0,gateway:{chef:"anthropic",model:{primary:"claude-haiku-4.5",secondary:["claude-4.5-haiku","claude-4.5-haiku-20251001","claude-haiku-4.5-20251001","claude-4.5-haiku-20251001-v1","claude-haiku-4.5-20251001-v1","anthropic.claude-haiku-4.5-20251001-v1:0","claude-haiku-4.5@20251001"]}},info:{description:"Claude Haiku 4.5 matches Sonnet 4's performance on coding, computer use, and agent tasks at substantially lower cost and faster speeds. It delivers near-frontier performance and Claude’s unique character at a price point that works for scaled sub-agent deployments, free tier products, and intelligence-sensitive applications with budget constraints.",website:"https://www.anthropic.com",modelUrl:"https://www.anthropic.com/claude/haiku",contextWindow:2e5,pricing:{pricingUrl:"https://www.anthropic.com/pricing#anthropic-api",inputCostPerMil:"1",outputCostPerMil:"5",cacheCreationInputCostPerMil:"1.25",cachedInputCostPerMil:"0.1"}},parameters:{temperature:{value:1,range:[0,1]},maximumLength:{value:1024,range:[50,64e3]},topK:{value:1,range:[1,500]},presencePenalty:{value:1,range:[0,1]},frequencyPenalty:{value:1,range:[0,1]},stopSequences:{value:[ee],range:[]}},lastModifiedDate:"2025-10-14"},{id:"azure:o3-mini",modelApiName:"o3-mini",provider:"azure",providerHumanName:"Azure",makerHumanName:"OpenAI",name:"o3-mini",supportsVision:!1,minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"openai",model:{primary:"o3-mini"}},info:{description:"o3-mini is OpenAI's most recent small reasoning model, providing high intelligence at the same cost and latency targets of o1-mini.",modelUrl:"https://platform.openai.com/docs/models/o3-mini",website:"https://openai.com",contextWindow:2e5,pricing:{inputCostPerMil:"1.1",outputCostPerMil:"4.4",pricingUrl:"https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service",cachedInputCostPerMil:"0.55",cacheCreationInputCostPerMil:"0"}},parameters:{maximumLength:{value:4096,range:[50,1e5]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-07-09"},{id:"azure:o3",modelApiName:"o3",provider:"azure",providerHumanName:"Azure",makerHumanName:"OpenAI",name:"o3",supportsVision:!0,minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,new:!0,gateway:{chef:"openai",model:{primary:"o3"}},disabled:!0,info:{description:"OpenAI's o3 is their most powerful reasoning model, setting new state-of-the-art benchmarks in coding, math, science, and visual perception. It excels at complex queries requiring multi-faceted analysis, with particular strength in analyzing images, charts, and graphics.",modelUrl:"https://platform.openai.com/docs/models/o3",website:"https://openai.com",contextWindow:2e5,pricing:{inputCostPerMil:"2",outputCostPerMil:"8",pricingUrl:"https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service"}},parameters:{maximumLength:{value:16384,range:[50,1e5]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-07-09"},{id:"azure:o4-mini",modelApiName:"o4-mini",provider:"azure",providerHumanName:"Azure",makerHumanName:"OpenAI",name:"o4-mini",supportsVision:!0,minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,new:!0,gateway:{chef:"openai",model:{primary:"o4-mini"}},info:{description:"OpenAI's o4-mini delivers fast, cost-efficient reasoning with exceptional performance for its size, particularly excelling in math (best-performing on AIME benchmarks), coding, and visual tasks.",modelUrl:"https://platform.openai.com/docs/models/o4-mini",website:"https://openai.com",contextWindow:2e5,pricing:{inputCostPerMil:"1.1",outputCostPerMil:"4.4",pricingUrl:"https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service",cachedInputCostPerMil:"0.275",cacheCreationInputCostPerMil:"0"}},parameters:{maximumLength:{value:16384,range:[50,1e5]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-07-09"},{id:"azure:gpt-4.1",modelApiName:"gpt-4.1",provider:"azure",providerHumanName:"Azure",makerHumanName:"OpenAI",name:"GPT-4.1",supportsVision:!0,minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,new:!0,gateway:{chef:"openai",model:{primary:"gpt-4.1"}},info:{description:"GPT 4.1 is OpenAI's flagship model for complex tasks. It is well suited for problem solving across domains.",modelUrl:"https://platform.openai.com/docs/models/gpt-4.1",website:"https://openai.com",contextWindow:1047576,pricing:{inputCostPerMil:"2",outputCostPerMil:"8",pricingUrl:"https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service",cachedInputCostPerMil:"0.5",cacheCreationInputCostPerMil:"0"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,16384]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-07-09"},{id:"azure:gpt-4.1-mini",modelApiName:"gpt-4.1-mini",provider:"azure",providerHumanName:"Azure",makerHumanName:"OpenAI",name:"GPT-4.1 mini",supportsVision:!0,minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,new:!0,gateway:{chef:"openai",model:{primary:"gpt-4.1-mini"}},info:{description:"GPT 4.1 mini provides a balance between intelligence, speed, and cost that makes it an attractive model for many use cases.",modelUrl:"https://platform.openai.com/docs/models/gpt-4.1-mini",website:"https://openai.com",contextWindow:1047576,pricing:{inputCostPerMil:"0.4",outputCostPerMil:"1.6",pricingUrl:"https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service",cachedInputCostPerMil:"0.1",cacheCreationInputCostPerMil:"0"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,32768]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-07-09"},{id:"azure:gpt-4.1-nano",modelApiName:"gpt-4.1-nano",provider:"azure",providerHumanName:"Azure",makerHumanName:"OpenAI",name:"GPT-4.1 nano",supportsVision:!0,minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,new:!0,gateway:{chef:"openai",model:{primary:"gpt-4.1-nano"}},info:{description:"GPT-4.1 nano is the fastest, most cost-effective GPT 4.1 model.",modelUrl:"https://platform.openai.com/docs/models/gpt-4.1-nano",website:"https://openai.com",contextWindow:1047576,pricing:{inputCostPerMil:"0.1",outputCostPerMil:"0.4",pricingUrl:"https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service",cachedInputCostPerMil:"0.025",cacheCreationInputCostPerMil:"0"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,32768]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-07-09"},{id:"azure:gpt-4o",modelApiName:"gpt-4o",provider:"azure",providerHumanName:"Azure",makerHumanName:"OpenAI",name:"GPT-4o",supportsVision:!0,minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"openai",model:{primary:"gpt-4o"}},info:{description:"GPT-4o from OpenAI has broad general knowledge and domain expertise allowing it to follow complex instructions in natural language and solve difficult problems accurately. It matches GPT-4 Turbo performance with a faster and cheaper API.",modelUrl:"https://platform.openai.com/docs/models/gpt-4o",website:"https://openai.com",contextWindow:128e3,pricing:{inputCostPerMil:"2.5",outputCostPerMil:"10",pricingUrl:"https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service",cachedInputCostPerMil:"1.25",cacheCreationInputCostPerMil:"0"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:1024,range:[50,16384]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-07-09"},{id:"azure:gpt-4o-mini",modelApiName:"gpt-4o-mini",provider:"azure",providerHumanName:"Azure",makerHumanName:"OpenAI",name:"GPT-4o mini",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"openai",model:{primary:"gpt-4o-mini",secondary:["gpt-4o-mini-2024-07-18"]}},info:{description:"GPT-4o mini from OpenAI is their most advanced and cost-efficient small model. It is multi-modal (accepting text or image inputs and outputting text) and has higher intelligence than gpt-3.5-turbo but is just as fast.",modelUrl:"https://platform.openai.com/docs/models/gpt-4o-mini",website:"https://openai.com",contextWindow:128e3,pricing:{inputCostPerMil:"0.15",outputCostPerMil:"0.6",pricingUrl:"https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service",cachedInputCostPerMil:"0.075",cacheCreationInputCostPerMil:"0"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:1024,range:[50,16384]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-07-09"},{id:"azure:gpt-4-turbo",modelApiName:"gpt-4-turbo",provider:"azure",providerHumanName:"Azure",makerHumanName:"OpenAI",supportsVision:!0,name:"GPT-4 Turbo",minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,disabled:!0,gateway:{chef:"openai",model:{primary:"gpt-4-turbo"}},info:{description:"gpt-4-turbo from OpenAI has broad general knowledge and domain expertise allowing it to follow complex instructions in natural language and solve difficult problems accurately. It has a knowledge cutoff of April 2023 and a 128,000 token context window.",modelUrl:"https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4",website:"https://openai.com",contextWindow:128e3,pricing:{inputCostPerMil:"10",outputCostPerMil:"30",pricingUrl:"https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:1024,range:[1,4096]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-07-09"},{id:"azure:gpt-3.5-turbo",modelApiName:"gpt-3.5-turbo",provider:"azure",providerHumanName:"Azure",makerHumanName:"OpenAI",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"openai",model:{primary:"gpt-3.5-turbo"}},info:{description:"OpenAI's most capable and cost effective model in the GPT-3.5 family optimized for chat purposes, but also works well for traditional completions tasks.",modelUrl:"https://platform.openai.com/docs/models/gpt-3-5",contextWindow:16384,website:"https://openai.com",pricing:{pricingUrl:"https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service",inputCostPerMil:"0.5",outputCostPerMil:"1.5"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:500,range:[1,16384]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},name:"GPT-3.5 Turbo",lastModifiedDate:"2025-07-09"},{id:"azure:gpt-3.5-turbo-instruct",modelApiName:"gpt-3.5-turbo-instruct",provider:"azure",providerHumanName:"Azure",makerHumanName:"OpenAI",supportsStructuredOutput:!0,supportsToolCalling:!1,name:"GPT-3.5 Turbo Instruct",gateway:{chef:"openai",model:{primary:"gpt-3.5-turbo-instruct"}},info:{description:"Similar capabilities as GPT-3 era models. Compatible with legacy Completions endpoint and not Chat Completions.",modelUrl:"https://platform.openai.com/docs/models/gpt-3-5",contextWindow:4096,website:"https://openai.com",pricing:{pricingUrl:"https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service",inputCostPerMil:"1.5",outputCostPerMil:"2"}},parameters:{temperature:{value:.5,range:[.1,1]},maximumLength:{value:200,range:[50,4096]},topP:{value:1,range:[.1,1]},presencePenalty:{value:0,range:[0,1]},frequencyPenalty:{value:0,range:[0,1]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-07-09"},{id:"azure:o1",modelApiName:"o1",provider:"azure",providerHumanName:"Azure",makerHumanName:"OpenAI",name:"o1",supportsVision:!1,minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,new:!0,gateway:{chef:"openai",model:{primary:"o1"}},info:{description:"o1 is OpenAI's flagship reasoning model, designed for complex problems that require deep thinking. It provides strong reasoning capabilities with improved accuracy for complex multi-step tasks.",modelUrl:"https://platform.openai.com/docs/models/o1",website:"https://openai.com",contextWindow:2e5,pricing:{inputCostPerMil:"15",outputCostPerMil:"60",pricingUrl:"https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service",cachedInputCostPerMil:"7.5",cacheCreationInputCostPerMil:"0"}},parameters:{maximumLength:{value:4096,range:[50,1e5]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-07-09"},{id:"baseten:deepseek-r1-0528",modelApiName:"deepseek-ai/DeepSeek-R1-0528",provider:"baseten",providerHumanName:"Baseten",makerHumanName:"DeepSeek",name:"DeepSeek R1 0528",supportsVision:!1,minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"deepseek",model:{primary:"deepseek-r1",secondary:["deepseek-r1-0528"]}},info:{description:"The latest revision of DeepSeek's first-generation reasoning model",modelUrl:"https://www.baseten.co/library/deepseek-r1/",website:"https://www.baseten.co",contextWindow:163840,pricing:{inputCostPerMil:"2.55",outputCostPerMil:"5.95",pricingUrl:"https://www.baseten.co/pricing/"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,16384]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-08-04"},{id:"baseten:deepseek-v3",modelApiName:"deepseek-ai/DeepSeek-V3-0324",provider:"baseten",providerHumanName:"Baseten",makerHumanName:"DeepSeek",name:"DeepSeek V3 0324",supportsVision:!1,minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"deepseek",model:{primary:"deepseek-v3",secondary:["deepseek-v3-0324"]}},info:{description:"Fast general-purpose LLM with enhanced reasoning capabilities",modelUrl:"https://www.baseten.co/library/deepseek-v3/",website:"https://www.baseten.co",contextWindow:163840,pricing:{inputCostPerMil:"0.77",outputCostPerMil:"0.77",pricingUrl:"https://www.baseten.co/pricing/"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,16384]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-08-04"},{id:"baseten:kimi-k2",modelApiName:"moonshotai/Kimi-K2-Instruct",provider:"baseten",providerHumanName:"Baseten",makerHumanName:"Moonshot AI",name:"Kimi K2",supportsVision:!1,minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,disabled:!0,gateway:{chef:"moonshotai",model:{primary:"kimi-k2",secondary:["kimi-k2-instruct"]}},info:{description:"State of the art language model for agentic and coding tasks",modelUrl:"https://www.baseten.co/library/kimi-v2/",website:"https://www.baseten.co",contextWindow:131072,pricing:{inputCostPerMil:"0.60",outputCostPerMil:"2.50",pricingUrl:"https://www.baseten.co/pricing/"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,16384]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-08-04"},{id:"baseten:qwen3-coder-480b-a35b-instruct",modelApiName:"Qwen/Qwen3-Coder-480B-A35B-Instruct",provider:"baseten",providerHumanName:"Baseten",makerHumanName:"Alibaba",name:"Qwen3 Coder 480B A35B Instruct",supportsVision:!1,minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"alibaba",model:{primary:"qwen3-coder",secondary:["qwen3-coder-480b","qwen3-coder-480b-a35b-instruct"]}},info:{description:"Mixture-of-experts LLM with advanced coding and reasoning capabilities",modelUrl:"https://www.baseten.co/library/qwen3-coder-480b-a25b-instruct/",website:"https://www.baseten.co",contextWindow:262144,pricing:{inputCostPerMil:"1.7",outputCostPerMil:"1.7",pricingUrl:"https://www.baseten.co/pricing/"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,66536]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-08-04"},{id:"baseten:llama-4-maverick-17b-128e-instruct",modelApiName:"meta-llama/Llama-4-Maverick-17B-128E-Instruct",provider:"baseten",providerHumanName:"Baseten",makerHumanName:"Meta",name:"Llama 4 Maverick 17B 128E Instruct",supportsVision:!1,minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,disabled:!0,gateway:{chef:"meta",model:{primary:"llama-4-maverick",secondary:["llama-4-maverick-17b-128e-instruct"]}},info:{description:"High-efficiency language processing",modelUrl:"https://www.baseten.co/library/llama-4-maverick/",website:"https://www.baseten.co",contextWindow:1e6,pricing:{inputCostPerMil:"0.19",outputCostPerMil:"0.72",pricingUrl:"https://www.baseten.co/pricing/"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,8192]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-08-04"},{id:"baseten:llama-4-scout-17b-16e-instruct",modelApiName:"meta-llama/Llama-4-Scout-17B-16E-Instruct",provider:"baseten",providerHumanName:"Baseten",makerHumanName:"Meta",name:"Llama 4 Scout 17B 16E Instruct",supportsVision:!1,minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,disabled:!0,gateway:{chef:"meta",model:{primary:"llama-4-scout",secondary:["llama-4-scout-17b-16e-instruct"]}},info:{description:"Precise context understanding with efficient reasoning capabilities",modelUrl:"https://www.baseten.co/library/llama-4-scout/",website:"https://www.baseten.co",contextWindow:1e6,pricing:{inputCostPerMil:"0.13",outputCostPerMil:"0.5",pricingUrl:"https://www.baseten.co/pricing/"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,8192]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-08-04"},{id:"baseten:qwen3-235b-a22b-instruct-2507",modelApiName:"Qwen/Qwen3-235B-A22B-Instruct-2507",provider:"baseten",providerHumanName:"Baseten",makerHumanName:"Alibaba",name:"Qwen3 235B A22B Instruct 2507",supportsVision:!1,minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"alibaba",model:{primary:"qwen-3-235b",secondary:["qwen-3-235b-a22b","qwen-3-235b-a22b-instruct-2507"]}},info:{description:"Mixture-of-experts LLM with math and reasoning capabilities",modelUrl:"https://www.baseten.co/library/qwen3-235b-a22b-instruct-2507/",website:"https://www.baseten.co",contextWindow:262144,pricing:{inputCostPerMil:"0.22",outputCostPerMil:"0.8",pricingUrl:"https://www.baseten.co/pricing/"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,131072]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-08-04"},{id:"baseten:openai/gpt-oss-120b",modelApiName:"openai/gpt-oss-120b",provider:"baseten",providerHumanName:"Baseten",makerHumanName:"OpenAI",name:"gpt-oss-120b",supportsVision:!1,minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"openai",model:{primary:"gpt-oss",secondary:["gpt-oss-120b"]}},info:{description:"Extremely capable general-purpose LLM with strong, controllable reasoning capabilities",modelUrl:"https://www.baseten.co/library/gpt-oss-120b/",website:"https://www.baseten.co",contextWindow:131072,pricing:{inputCostPerMil:"0.1",outputCostPerMil:"0.5",pricingUrl:"https://www.baseten.co/pricing/"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,131072]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-08-04"},{id:"bedrock:amazon.nova-pro-v1:0",provider:"bedrock",providerHumanName:"Bedrock",modelApiName:"us.amazon.nova-pro-v1:0",makerHumanName:"Amazon",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!1,info:{description:"A highly capable multimodal model with the best combination of accuracy, speed, and cost for a wide range of tasks.",website:"https://aws.amazon.com/ai/generative-ai/nova",modelUrl:"https://docs.aws.amazon.com/nova/latest/userguide/what-is-nova.html",contextWindow:3e5,pricing:{pricingUrl:"https://aws.amazon.com/bedrock/pricing/",inputCostPerMil:"0.8",outputCostPerMil:"3.2"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"Nova Pro",lastModifiedDate:"2025-02-19"},{id:"bedrock:amazon.nova-lite-v1:0",provider:"bedrock",providerHumanName:"Bedrock",modelApiName:"us.amazon.nova-lite-v1:0",makerHumanName:"Amazon",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"A very low cost multimodal model that is lightning fast for processing image, video, and text inputs.",website:"https://aws.amazon.com/ai/generative-ai/nova",modelUrl:"https://docs.aws.amazon.com/nova/latest/userguide/what-is-nova.html",contextWindow:3e5,pricing:{pricingUrl:"https://aws.amazon.com/bedrock/pricing/",inputCostPerMil:"0.06",outputCostPerMil:"0.24"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"Nova Lite",lastModifiedDate:"2025-01-15"},{id:"bedrock:amazon.nova-micro-v1:0",provider:"bedrock",providerHumanName:"Bedrock",modelApiName:"us.amazon.nova-micro-v1:0",makerHumanName:"Amazon",supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"A text-only model that delivers the lowest latency responses at very low cost.",website:"https://aws.amazon.com/ai/generative-ai/nova",modelUrl:"https://docs.aws.amazon.com/nova/latest/userguide/what-is-nova.html",contextWindow:128e3,pricing:{pricingUrl:"https://aws.amazon.com/bedrock/pricing/",inputCostPerMil:"0.035",outputCostPerMil:"0.14"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"Nova Micro",lastModifiedDate:"2025-01-15"},{id:"bedrock:claude-3-7-sonnet-20250219",provider:"bedrock",providerHumanName:"Bedrock",modelApiName:"us.anthropic.claude-3-7-sonnet-20250219-v1:0",makerHumanName:"Amazon",supportsStructuredOutput:!0,supportsToolCalling:!0,supportsVision:!0,minBillingTier:"pro",info:{description:"Claude 3.7 Sonnet is Anthropic's most intelligent model to date and the first Claude model to offer extended thinking—the ability to solve complex problems with careful, step-by-step reasoning. Anthropic is the first AI lab to introduce a single model where users can balance speed and quality by choosing between standard thinking for near-instant responses or extended thinking or advanced reasoning. Claude 3.7 Sonnet is state-of-the-art for coding, and delivers advancements in computer use, agentic capabilities, complex reasoning, and content generation. With frontier performance and more control over speed, Claude 3.7 Sonnet is the ideal choice for powering AI agents, especially customer-facing agents, and complex AI workflows.",website:"https://aws.amazon.com/bedrock/claude/",modelUrl:"https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-37.html",contextWindow:2e5,pricing:{pricingUrl:"https://aws.amazon.com/bedrock/pricing/",inputCostPerMil:"3",outputCostPerMil:"15",cachedInputCostPerMil:"0.3",cacheCreationInputCostPerMil:"3.75"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"Claude 3.7 Sonnet (Bedrock)",lastModifiedDate:"2025-03-28"},{id:"bedrock:claude-3-5-haiku-20241022",provider:"bedrock",providerHumanName:"Bedrock",modelApiName:"us.anthropic.claude-3-5-haiku-20241022-v1:0",makerHumanName:"Amazon",supportsStructuredOutput:!0,supportsToolCalling:!0,minBillingTier:"pro",info:{description:"Claude 3 Haiku is Anthropic's fastest, most compact model for near-instant responsiveness. It answers simple queries and requests with speed. Customers will be able to build seamless AI experiences that mimic human interactions. Claude 3 Haiku can process images and return text outputs, and features a 200K context window.",website:"https://aws.amazon.com/bedrock/claude/",modelUrl:"https://aws.amazon.com/about-aws/whats-new/2024/11/anthropics-claude-3-5-haiku-model-amazon-bedrock/",contextWindow:2e5,pricing:{pricingUrl:"https://aws.amazon.com/bedrock/pricing/",inputCostPerMil:"0.8",outputCostPerMil:"4",cachedInputCostPerMil:"0.08",cacheCreationInputCostPerMil:"1"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"Claude 3.5 Haiku (Bedrock)",lastModifiedDate:"2025-05-18"},{id:"bedrock:claude-3-5-sonnet-20241022-v2",provider:"bedrock",providerHumanName:"Bedrock",modelApiName:"us.anthropic.claude-3-5-sonnet-20241022-v2:0",makerHumanName:"Amazon",supportsStructuredOutput:!0,supportsToolCalling:!0,supportsVision:!0,minBillingTier:"pro",info:{description:"The upgraded Claude 3.5 Sonnet is now state-of-the-art for a variety of tasks including real-world software engineering, agentic capabilities and computer use. The new Claude 3.5 Sonnet delivers these advancements at the same price and speed as its predecessor.",website:"https://aws.amazon.com/bedrock/claude/",modelUrl:"https://aws.amazon.com/bedrock/claude/",contextWindow:2e5,pricing:{pricingUrl:"https://aws.amazon.com/bedrock/pricing/",inputCostPerMil:"3",outputCostPerMil:"15",cachedInputCostPerMil:"0.3",cacheCreationInputCostPerMil:"3.75"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"Claude 3.5 Sonnet (Bedrock)",lastModifiedDate:"2025-10-03"},{id:"bedrock:claude-3-5-sonnet-20240620-v1",provider:"bedrock",providerHumanName:"Bedrock",modelApiName:"us.anthropic.claude-3-5-sonnet-20240620-v1:0",makerHumanName:"Amazon",supportsStructuredOutput:!0,supportsToolCalling:!0,supportsVision:!0,minBillingTier:"pro",info:{description:"Claude 3.5 Sonnet raises the industry bar for intelligence, outperforming competitor models and Claude 3 Opus on a wide range of evaluations, with the speed and cost of our mid-tier model, Claude 3 Sonnet.",website:"https://aws.amazon.com/bedrock/claude/",modelUrl:"https://aws.amazon.com/bedrock/claude/",contextWindow:2e5,pricing:{pricingUrl:"https://aws.amazon.com/bedrock/pricing/",inputCostPerMil:"3",outputCostPerMil:"15"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"Claude 3.5 Sonnet 2024-06-20 (Bedrock)",lastModifiedDate:"2025-10-03"},{id:"bedrock:claude-3-haiku-20240307-v1",provider:"bedrock",providerHumanName:"Bedrock",modelApiName:"us.anthropic.claude-3-haiku-20240307-v1:0",makerHumanName:"Amazon",supportsStructuredOutput:!0,supportsToolCalling:!0,supportsVision:!0,minBillingTier:"pro",info:{description:"Claude 3 Haiku is Anthropic's fastest, most compact model for near-instant responsiveness. It answers simple queries and requests with speed. Customers will be able to build seamless AI experiences that mimic human interactions. Claude 3 Haiku can process images and return text outputs, and features a 200K context window.",website:"https://aws.amazon.com/bedrock/claude/",modelUrl:"https://aws.amazon.com/bedrock/claude/",contextWindow:2e5,pricing:{pricingUrl:"https://aws.amazon.com/bedrock/pricing/",inputCostPerMil:"0.25",outputCostPerMil:"1.25"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"Claude 3 Haiku (Bedrock)",lastModifiedDate:"2025-05-18"},{id:"bedrock:claude-4-opus-20250514-v1",provider:"bedrock",providerHumanName:"Bedrock",modelApiName:"us.anthropic.claude-opus-4-20250514-v1:0",makerHumanName:"Amazon",supportsStructuredOutput:!0,supportsToolCalling:!0,supportsVision:!0,minBillingTier:"pro",new:!0,info:{description:"Claude Opus 4 is Anthropic's most intelligent model and is state-of-the-art for coding and agent capabilities, especially agentic search. It excels for customers needing frontier intelligence: Advanced coding: Independently plan and execute complex development tasks end-to-end. It adapts to your style and maintains high code quality throughout.",website:"https://aws.amazon.com/bedrock/claude/",modelUrl:"https://aws.amazon.com/bedrock/claude/",contextWindow:2e5,pricing:{pricingUrl:"https://aws.amazon.com/bedrock/pricing/",inputCostPerMil:"15",outputCostPerMil:"75",cachedInputCostPerMil:"1.5",cacheCreationInputCostPerMil:"18.75"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"Claude Opus 4 (Bedrock)",lastModifiedDate:"2025-08-12"},{id:"bedrock:claude-4-sonnet-20250514-v1",provider:"bedrock",providerHumanName:"Bedrock",modelApiName:"us.anthropic.claude-sonnet-4-20250514-v1:0",makerHumanName:"Amazon",supportsStructuredOutput:!0,supportsToolCalling:!0,supportsVision:!0,minBillingTier:"pro",new:!0,info:{description:"Claude Sonnet 4 balances impressive performance for coding with the right speed and cost for high-volume use cases: Coding: Handle everyday development tasks with enhanced performance-power code reviews, bug fixes, API integrations, and feature development with immediate feedback loops.",website:"https://aws.amazon.com/bedrock/claude/",modelUrl:"https://aws.amazon.com/bedrock/claude/",contextWindow:2e5,pricing:{pricingUrl:"https://aws.amazon.com/bedrock/pricing/",inputCostPerMil:"3",outputCostPerMil:"15",cachedInputCostPerMil:"0.3",cacheCreationInputCostPerMil:"3.75"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"Claude Sonnet 4 (Bedrock)",lastModifiedDate:"2025-08-12"},{id:"bedrock:meta.llama4-maverick-17b-instruct-v1",provider:"bedrock",providerHumanName:"Bedrock",modelApiName:"us.meta.llama4-maverick-17b-instruct-v1:0",makerHumanName:"Amazon",supportsStructuredOutput:!0,supportsToolCalling:!0,supportsVision:!0,minBillingTier:"pro",info:{description:"As a general purpose LLM, Llama 4 Maverick contains 17 billion active parameters, 128 experts, and 400 billion total parameters, offering high quality at a lower price compared to Llama 3.3 70B.",website:"https://aws.amazon.com/bedrock/meta/",modelUrl:"https://aws.amazon.com/bedrock/meta/",contextWindow:128e3,pricing:{pricingUrl:"https://aws.amazon.com/bedrock/pricing/",inputCostPerMil:"0.24",outputCostPerMil:"0.97"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"Llama 4 Maverick 17B Instruct (Bedrock)",lastModifiedDate:"2025-05-18"},{id:"bedrock:meta.llama4-scout-17b-instruct-v1",provider:"bedrock",providerHumanName:"Bedrock",modelApiName:"us.meta.llama4-scout-17b-instruct-v1:0",makerHumanName:"Amazon",supportsStructuredOutput:!0,supportsToolCalling:!0,supportsVision:!0,minBillingTier:"pro",info:{description:"Llama 4 Scout is the best multimodal model in the world in its class and is more powerful than our Llama 3 models, while fitting in a single H100 GPU. Additionally, Llama 4 Scout supports an industry-leading context window of up to 10M tokens.",website:"https://aws.amazon.com/bedrock/meta/",modelUrl:"https://aws.amazon.com/bedrock/meta/",contextWindow:128e3,pricing:{pricingUrl:"https://aws.amazon.com/bedrock/pricing/",inputCostPerMil:"0.17",outputCostPerMil:"0.66"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"Llama 4 Scout 17B Instruct (Bedrock)",lastModifiedDate:"2025-05-18"},{id:"bedrock:meta.llama3-3-70b-instruct-v1",provider:"bedrock",providerHumanName:"Bedrock",modelApiName:"us.meta.llama3-3-70b-instruct-v1:0",makerHumanName:"Amazon",supportsStructuredOutput:!0,supportsToolCalling:!0,minBillingTier:"pro",info:{description:"Where performance meets efficiency. This model supports high-performance conversational AI designed for content creation, enterprise applications, and research, offering advanced language understanding capabilities, including text summarization, classification, sentiment analysis, and code generation.",website:"https://aws.amazon.com/bedrock/meta/",modelUrl:"https://aws.amazon.com/bedrock/meta/",contextWindow:128e3,pricing:{pricingUrl:"https://aws.amazon.com/bedrock/pricing/",inputCostPerMil:"0.72",outputCostPerMil:"0.72"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"Llama 3.3 70B Instruct (Bedrock)",lastModifiedDate:"2025-05-18"},{id:"bedrock:meta.llama3-2-11b-instruct-v1",provider:"bedrock",providerHumanName:"Bedrock",modelApiName:"us.meta.llama3-2-11b-instruct-v1:0",makerHumanName:"Amazon",supportsStructuredOutput:!0,supportsToolCalling:!0,supportsVision:!0,minBillingTier:"pro",info:{description:"Instruction-tuned image reasoning generative model (text + images in / text out) optimized for visual recognition, image reasoning, captioning and answering general questions about the image.",website:"https://aws.amazon.com/bedrock/meta/",modelUrl:"https://aws.amazon.com/bedrock/meta/",contextWindow:128e3,pricing:{pricingUrl:"https://aws.amazon.com/bedrock/pricing/",inputCostPerMil:"0.16",outputCostPerMil:"0.16"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"Llama 3.2 11B Vision Instruct (Bedrock)",lastModifiedDate:"2025-05-18"},{id:"bedrock:meta.llama3-2-1b-instruct-v1",provider:"bedrock",providerHumanName:"Bedrock",modelApiName:"us.meta.llama3-2-1b-instruct-v1:0",makerHumanName:"Amazon",supportsStructuredOutput:!0,supportsToolCalling:!0,minBillingTier:"pro",info:{description:"Text-only model, supporting on-device use cases such as multilingual local knowledge retrieval, summarization, and rewriting.",website:"https://aws.amazon.com/bedrock/meta/",modelUrl:"https://aws.amazon.com/bedrock/meta/",contextWindow:128e3,pricing:{pricingUrl:"https://aws.amazon.com/bedrock/pricing/",inputCostPerMil:"0.1",outputCostPerMil:"0.1"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"Llama 3.2 1B Instruct (Bedrock)",lastModifiedDate:"2025-05-18"},{id:"bedrock:meta.llama3-2-3b-instruct-v1",provider:"bedrock",providerHumanName:"Bedrock",modelApiName:"us.meta.llama3-2-3b-instruct-v1:0",makerHumanName:"Amazon",supportsStructuredOutput:!0,supportsToolCalling:!0,minBillingTier:"pro",info:{description:"Text-only model, fine-tuned for supporting on-device use cases such as multilingual local knowledge retrieval, summarization, and rewriting.",website:"https://aws.amazon.com/bedrock/meta/",modelUrl:"https://aws.amazon.com/bedrock/meta/",contextWindow:128e3,pricing:{pricingUrl:"https://aws.amazon.com/bedrock/pricing/",inputCostPerMil:"0.15",outputCostPerMil:"0.15"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"Llama 3.2 3B Instruct (Bedrock)",lastModifiedDate:"2025-05-18"},{id:"bedrock:meta.llama3-2-90b-instruct-v1",provider:"bedrock",providerHumanName:"Bedrock",modelApiName:"us.meta.llama3-2-90b-instruct-v1:0",makerHumanName:"Amazon",supportsStructuredOutput:!0,supportsToolCalling:!0,supportsVision:!0,minBillingTier:"pro",info:{description:"Instruction-tuned image reasoning generative model (text + images in / text out) optimized for visual recognition, image reasoning, captioning and answering general questions about the image.",website:"https://aws.amazon.com/bedrock/meta/",modelUrl:"https://aws.amazon.com/bedrock/meta/",contextWindow:128e3,pricing:{pricingUrl:"https://aws.amazon.com/bedrock/pricing/",inputCostPerMil:"0.72",outputCostPerMil:"0.72"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"Llama 3.2 90B Vision Instruct (Bedrock)",lastModifiedDate:"2025-05-18"},{id:"bedrock:meta.llama3-1-70b-instruct-v1",provider:"bedrock",providerHumanName:"Bedrock",modelApiName:"us.meta.llama3-1-70b-instruct-v1:0",makerHumanName:"Amazon",supportsStructuredOutput:!0,supportsToolCalling:!0,minBillingTier:"pro",info:{description:"An update to Meta Llama 3 70B Instruct that includes an expanded 128K context length, multilinguality and improved reasoning capabilities.",website:"https://aws.amazon.com/bedrock/meta/",modelUrl:"https://aws.amazon.com/bedrock/meta/",contextWindow:128e3,pricing:{pricingUrl:"https://aws.amazon.com/bedrock/pricing/",inputCostPerMil:"0.72",outputCostPerMil:"0.72"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"Llama 3.1 70B Instruct (Bedrock)",lastModifiedDate:"2025-05-18"},{id:"bedrock:meta.llama3-1-8b-instruct-v1",provider:"bedrock",providerHumanName:"Bedrock",modelApiName:"us.meta.llama3-1-8b-instruct-v1:0",makerHumanName:"Amazon",supportsStructuredOutput:!0,supportsToolCalling:!0,minBillingTier:"pro",info:{description:"An update to Meta Llama 3 8B Instruct that includes an expanded 128K context length, multilinguality and improved reasoning capabilities.",website:"https://aws.amazon.com/bedrock/meta/",modelUrl:"https://aws.amazon.com/bedrock/meta/",contextWindow:128e3,pricing:{pricingUrl:"https://aws.amazon.com/bedrock/pricing/",inputCostPerMil:"0.22",outputCostPerMil:"0.22"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"Llama 3.1 8B Instruct (Bedrock)",lastModifiedDate:"2025-05-18"},{id:"bedrock:deepseek.r1-v1",provider:"bedrock",providerHumanName:"Bedrock",modelApiName:"us.deepseek.r1-v1:0",makerHumanName:"Amazon",supportsStructuredOutput:!0,supportsToolCalling:!0,minBillingTier:"pro",info:{description:"DeepSeek-R1 provides customers a state-of-the-art reasoning model, optimized for general reasoning tasks, math, science, and code generation.",website:"https://aws.amazon.com/bedrock/meta/",modelUrl:"https://aws.amazon.com/bedrock/meta/",contextWindow:128e3,pricing:{pricingUrl:"https://aws.amazon.com/bedrock/pricing/",inputCostPerMil:"1.35",outputCostPerMil:"5.4"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"DeepSeek-R1 (Bedrock)",lastModifiedDate:"2025-05-18"},{id:"bedrock:deepseek.v3-v1",provider:"bedrock",providerHumanName:"Bedrock",modelApiName:"us.deepseek.v3-v1:0",makerHumanName:"Amazon",supportsStructuredOutput:!1,supportsToolCalling:!1,minBillingTier:"hobby",disabled:!0,gateway:{chef:"deepseek",model:{primary:"deepseek-v3.1"}},info:{description:"DeepSeek-V3.1 marks DeepSeek's first step toward the agent era with revolutionary hybrid inference capabilities. Operates in two modes: Think and Non-Think. The Think variant delivers faster reasoning compared to DeepSeek-R1-0528, reaching answers more efficiently while maintaining high-quality outputs. Enhanced through specialized post-training, the model excels at tool usage and complex multi-step agent tasks.",website:"https://aws.amazon.com/bedrock/deepseek/",modelUrl:"https://aws.amazon.com/bedrock/deepseek/",contextWindow:128e3,pricing:{pricingUrl:"https://aws.amazon.com/bedrock/pricing/",inputCostPerMil:"0.58",outputCostPerMil:"1.68"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"DeepSeek-V3.1",lastModifiedDate:"2025-09-19"},{id:"bedrock:openai.gpt-oss-20b",provider:"bedrock",providerHumanName:"Bedrock",modelApiName:"openai.gpt-oss-20b-1:0",makerHumanName:"OpenAI",supportsStructuredOutput:!1,supportsToolCalling:!0,supportsVision:!0,minBillingTier:"hobby",gateway:{chef:"openai",model:{primary:"gpt-oss-20b"}},info:{description:"A compact, open-weight language model optimized for low-latency and resource-constrained environments, including local and edge deployments",website:"https://aws.amazon.com/bedrock/openai/",modelUrl:"https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-openai.html",contextWindow:128e3,pricing:{pricingUrl:"https://aws.amazon.com/bedrock/pricing/",inputCostPerMil:"0.07",outputCostPerMil:"0.3"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"gpt-oss-20b",lastModifiedDate:"2025-09-19"},{id:"bedrock:openai.gpt-oss-120b",provider:"bedrock",providerHumanName:"Bedrock",modelApiName:"openai.gpt-oss-120b-1:0",makerHumanName:"OpenAI",supportsStructuredOutput:!1,supportsToolCalling:!0,supportsVision:!0,minBillingTier:"hobby",gateway:{chef:"openai",model:{primary:"gpt-oss-120b"}},info:{description:"A high-performance, open-weight language model designed for production-grade, general-purpose use cases.",website:"https://aws.amazon.com/bedrock/openai/",modelUrl:"https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-openai.html",contextWindow:128e3,pricing:{pricingUrl:"https://aws.amazon.com/bedrock/pricing/",inputCostPerMil:"0.15",outputCostPerMil:"0.6"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"gpt-oss-120b",lastModifiedDate:"2025-09-19"},{id:"bedrock:qwen-3-32b",provider:"bedrock",providerHumanName:"Bedrock",modelApiName:"qwen.qwen3-32b-v1:0",makerHumanName:"Alibaba",supportsStructuredOutput:!0,supportsToolCalling:!0,supportsVision:!1,minBillingTier:"hobby",gateway:{chef:"alibaba",model:{primary:"qwen-3-32b"}},info:{description:"Qwen3-32B is a world-class model with comparable quality to DeepSeek R1 while outperforming GPT-4.1 and Claude Sonnet 3.7. It excels in code-gen, tool-calling, and advanced reasoning, making it an exceptional model for a wide range of production use cases.",website:"https://aws.amazon.com/blogs/aws/qwen-models-are-now-available-in-amazon-bedrock/",modelUrl:"https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html",contextWindow:128e3,pricing:{pricingUrl:"https://aws.amazon.com/bedrock/pricing/",inputCostPerMil:"0.15",outputCostPerMil:"0.6"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"Qwen 3.32B",lastModifiedDate:"2025-09-19"},{id:"bedrock:qwen-3-coder-30b-a3b-instruct",provider:"bedrock",providerHumanName:"Bedrock",modelApiName:"qwen.qwen3-coder-30b-a3b-v1:0",makerHumanName:"Alibaba",supportsStructuredOutput:!0,supportsToolCalling:!0,supportsVision:!1,minBillingTier:"hobby",gateway:{chef:"alibaba",model:{primary:"qwen3-coder-30b-a3b"}},info:{description:"Efficient coding specialist balancing performance with cost-effectiveness for daily development tasks while maintaining strong tool integration capabilities.",website:"https://aws.amazon.com/blogs/aws/qwen-models-are-now-available-in-amazon-bedrock/",modelUrl:"https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html",contextWindow:262144,pricing:{pricingUrl:"https://aws.amazon.com/bedrock/pricing/",inputCostPerMil:"0.15",outputCostPerMil:"0.6"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"Qwen 3 Coder 30B A3B Instruct",lastModifiedDate:"2025-09-19"},{id:"bedrock:qwen-3-coder-480b-a35b-instruct",provider:"bedrock",providerHumanName:"Bedrock",modelApiName:"qwen.qwen3-coder-480b-a35b-v1:0",makerHumanName:"Alibaba",supportsStructuredOutput:!0,supportsToolCalling:!0,supportsVision:!1,minBillingTier:"hobby",disabled:!0,gateway:{chef:"alibaba",model:{primary:"qwen3-coder",secondary:["qwen3-coder-480b","qwen3-coder-480b-a35b-instruct"]}},info:{description:"Qwen3 Coder 480B is a specialized programming model designed for ultra-efficient agentic code generation with long context and state-of-the-art performance. It excels at writing, debugging, and explaining code across multiple programming languages.",website:"https://aws.amazon.com/blogs/aws/qwen-models-are-now-available-in-amazon-bedrock/",modelUrl:"https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html",contextWindow:262144,pricing:{pricingUrl:"https://aws.amazon.com/bedrock/pricing/",inputCostPerMil:"0.22",outputCostPerMil:"1.8"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"Qwen 3 Coder",lastModifiedDate:"2025-09-19"},{id:"bedrock:qwen3-235b-a22b-instruct-2507",provider:"bedrock",providerHumanName:"Bedrock",modelApiName:"qwen.qwen3-235b-a22b-2507-v1:0",makerHumanName:"Alibaba",supportsStructuredOutput:!0,supportsToolCalling:!0,supportsVision:!1,minBillingTier:"hobby",disabled:!0,gateway:{chef:"alibaba",model:{primary:"qwen-3-235b",secondary:["qwen-3-235b-a22b","qwen-3-235b-a22b-instruct-2507"]}},info:{description:"Advanced Mixture of Experts (MoE) hybrid reasoning model that excels at complex reasoning tasks and multilingual applications across 119 languages.",website:"https://aws.amazon.com/blogs/aws/qwen-models-are-now-available-in-amazon-bedrock/",modelUrl:"https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html",contextWindow:262144,pricing:{pricingUrl:"https://aws.amazon.com/bedrock/pricing/",inputCostPerMil:"0.22",outputCostPerMil:"0.88"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"Qwen3 235B A22B Instruct 2507",lastModifiedDate:"2025-09-19"},{id:"bedrock:claude-4.5-sonnet-20250929-v1",provider:"bedrock",providerHumanName:"Bedrock",modelApiName:"us.anthropic.claude-sonnet-4-5-20250929-v1:0",makerHumanName:"Anthropic",supportsStructuredOutput:!0,supportsToolCalling:!0,supportsVision:!0,minBillingTier:"hobby",new:!0,disabled:!0,gateway:{chef:"anthropic",model:{primary:"claude-sonnet-4.5",secondary:["claude-4.5-sonnet","claude-4.5-sonnet-20250929","claude-sonnet-4.5-20250929","claude-4.5-sonnet-20250929-v1","claude-sonnet-4.5-20250929-v1","anthropic.claude-sonnet-4.5-20250929-v1:0","claude-sonnet-4.5@20250929"]}},info:{description:"Claude Sonnet 4.5 is the newest model in the Sonnet series, offering improvements and updates over Sonnet 4.",website:"https://aws.amazon.com/bedrock/claude/",modelUrl:"https://aws.amazon.com/bedrock/claude/",contextWindow:2e5,pricing:{pricingUrl:"https://aws.amazon.com/bedrock/pricing/",inputCostPerMil:"3.3",outputCostPerMil:"16.5",cachedInputCostPerMil:"0.33",cacheCreationInputCostPerMil:"4.125"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"Claude Sonnet 4.5",lastModifiedDate:"2025-09-29"},{id:"bedrock:claude-haiku-4.5-20251001",provider:"bedrock",providerHumanName:"Bedrock",name:"Claude Haiku 4.5",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,modelApiName:"global.anthropic.claude-haiku-4-5-20251001-v1:0",makerHumanName:"Anthropic",minBillingTier:"hobby",new:!0,gateway:{chef:"anthropic",model:{primary:"claude-haiku-4.5",secondary:["claude-4.5-haiku","claude-4.5-haiku-20251001","claude-haiku-4.5-20251001","claude-4.5-haiku-20251001-v1","claude-haiku-4.5-20251001-v1","anthropic.claude-haiku-4.5-20251001-v1:0","claude-haiku-4.5@20251001"]}},info:{description:"Claude Haiku 4.5 matches Sonnet 4's performance on coding, computer use, and agent tasks at substantially lower cost and faster speeds. It delivers near-frontier performance and Claude’s unique character at a price point that works for scaled sub-agent deployments, free tier products, and intelligence-sensitive applications with budget constraints.",website:"https://aws.amazon.com/bedrock/claude/",modelUrl:"https://aws.amazon.com/bedrock/claude/",contextWindow:2e5,pricing:{pricingUrl:"https://aws.amazon.com/bedrock/pricing/",inputCostPerMil:"1",outputCostPerMil:"5",cacheCreationInputCostPerMil:"1.25",cachedInputCostPerMil:"0.1"}},parameters:{temperature:{value:1,range:[0,1]},maximumLength:{value:1024,range:[50,64e3]}},lastModifiedDate:"2025-10-14"},{id:"cerebras:llama-4-scout-17b-16e-instruct",modelApiName:"llama-4-scout-17b-16e-instruct",provider:"cerebras",providerHumanName:"Cerebras",makerHumanName:"Cerebras",name:"Llama 4 Scout",supportsVision:!1,minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,disabled:!1,info:{description:"The Llama-4-Scout-17B-16E-Instruct model is a state-of-the-art, instruction-tuned, multimodal AI model developed by Meta as part of the Llama 4 family. It is designed to handle both text and image inputs, making it suitable for a wide range of applications, including conversational AI, code generation, and visual reasoning.",modelUrl:"https://inference-docs.cerebras.ai/introduction",website:"https://inference-docs.cerebras.ai",contextWindow:128e3,pricing:{inputCostPerMil:"0.65",outputCostPerMil:"0.85",pricingUrl:"https://inference-docs.cerebras.ai/support/pricing"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,16384]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-05-05"},{id:"cerebras:llama3.1-8b",modelApiName:"llama3.1-8b",provider:"cerebras",providerHumanName:"Cerebras",makerHumanName:"Cerebras",name:"Llama 3.1 8B",supportsVision:!1,minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,disabled:!1,info:{description:"Llama 3.1 8B brings powerful performance in a smaller, more efficient package. With improved multilingual support, tool use, and a 128K context length, it enables sophisticated use cases like interactive agents and compact coding assistants while remaining lightweight and accessible.",modelUrl:"https://inference-docs.cerebras.ai/introduction",website:"https://inference-docs.cerebras.ai",contextWindow:128e3,pricing:{inputCostPerMil:"0.10",outputCostPerMil:"0.10",pricingUrl:"https://inference-docs.cerebras.ai/support/pricing"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,16384]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-05-05"},{id:"cerebras:llama-3.3-70b",modelApiName:"llama-3.3-70b",provider:"cerebras",providerHumanName:"Cerebras",makerHumanName:"Cerebras",name:"Llama 3.3 70B",supportsVision:!1,minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,disabled:!1,info:{description:"The upgraded Llama 3.1 70B model features enhanced reasoning, tool use, and multilingual abilities, along with a significantly expanded 128K context window. These improvements make it well-suited for demanding tasks such as long-form summarization, multilingual conversations, and coding assistance.",modelUrl:"https://inference-docs.cerebras.ai/introduction",website:"https://inference-docs.cerebras.ai",contextWindow:128e3,pricing:{inputCostPerMil:"0.85",outputCostPerMil:"1.20",pricingUrl:"https://inference-docs.cerebras.ai/support/pricing"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,16384]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-05-05"},{id:"cerebras:deepseek-r1-distill-llama-70b",modelApiName:"deepseek-r1-distill-llama-70b",provider:"cerebras",providerHumanName:"Cerebras",makerHumanName:"Cerebras",name:"DeepSeek R1 Distill Llama 70B",supportsVision:!1,minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,disabled:!1,info:{description:"DeepSeek-R1 is a state-of-the-art reasoning model trained with reinforcement learning and cold-start data, delivering strong performance across math, code, and complex reasoning tasks. It offers improved stability, readability, and multilingual handling compared to earlier versions, and is available alongside several high-quality distilled variants.",modelUrl:"https://inference-docs.cerebras.ai/introduction",website:"https://inference-docs.cerebras.ai",contextWindow:128e3,pricing:{inputCostPerMil:"2.20",outputCostPerMil:"2.50",pricingUrl:"https://inference-docs.cerebras.ai/support/pricing"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,16384]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-05-05"},{id:"cerebras:qwen-3-32b",modelApiName:"qwen-3-32b",provider:"cerebras",providerHumanName:"Cerebras",makerHumanName:"Cerebras",name:"Qwen 3.32B",supportsVision:!1,minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"Qwen3-32B is a world-class model with comparable quality to DeepSeek R1 while outperforming GPT-4.1 and Claude Sonnet 3.7. It excels in code-gen, tool-calling, and advanced reasoning, making it an exceptional model for a wide range of production use cases.",modelUrl:"https://www.cerebras.ai/blog/reasoning-in-one-second-try-qwen3-32b-on-cerebras",website:"https://inference-docs.cerebras.ai",contextWindow:128e3,pricing:{inputCostPerMil:"0.4",outputCostPerMil:"0.8",pricingUrl:"https://inference-docs.cerebras.ai/support/pricing"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,16384]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-05-18"},{id:"cerebras:qwen-3-coder-480b",modelApiName:"qwen-3-coder-480b",provider:"cerebras",providerHumanName:"Cerebras",makerHumanName:"Cerebras",name:"Qwen3 Coder",supportsVision:!1,minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"Qwen3 Coder 480B is a specialized programming model designed for ultra-efficient agentic code generation with long context and state-of-the-art performance. It excels at writing, debugging, and explaining code across multiple programming languages.",modelUrl:"https://www.cerebras.ai/blog/qwen3-coder-480b-is-live-on-cerebras",website:"https://inference-docs.cerebras.ai",contextWindow:131e3,pricing:{inputCostPerMil:"2",outputCostPerMil:"2",pricingUrl:"https://inference-docs.cerebras.ai/support/pricing"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,131e3]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-08-01"},{id:"cerebras:gpt-oss-120b",modelApiName:"gpt-oss-120b",provider:"cerebras",providerHumanName:"Cerebras",makerHumanName:"OpenAI",name:"gpt-oss-120b",supportsVision:!1,minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"openai",model:{primary:"gpt-oss-120b"}},info:{description:"This model excels at efficient reasoning across science, math, and coding applications. It’s ideal for real-time coding assistance, processing large documents for Q&A and summarization, agentic research workflows, and regulated on-premises workloads.",modelUrl:"https://inference-docs.cerebras.ai/models/openai-oss",website:"https://inference-docs.cerebras.ai",contextWindow:131072,pricing:{inputCostPerMil:"0.25",outputCostPerMil:"0.69",pricingUrl:"https://inference-docs.cerebras.ai/support/pricing"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,131e3]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-08-04"},{id:"chutes:deepseek-v3.1-base",name:"DeepSeek V3.1 Base",provider:"chutes",providerHumanName:"Chutes",makerHumanName:"DeepSeek",modelApiName:"deepseek-ai/DeepSeek-V3.1-Base",minBillingTier:"hobby",supportsStructuredOutput:!1,supportsToolCalling:!1,gateway:{chef:"deepseek",model:{primary:"deepseek-v3.1-base",secondary:["deepseek-v3-1-base","deepseek-v-3-1-base"]}},info:{description:"DeepSeek V3.1 Base is an improved version of the DeepSeek V3 model.",website:"https://chutes.ai",modelUrl:"https://chutes.ai/app/chute/ee7987b4-43c6-57d6-9f0c-f90c762586e2?tab=playground",contextWindow:128e3,pricing:{pricingUrl:"https://chutes.ai/app/chute/ee7987b4-43c6-57d6-9f0c-f90c762586e2?tab=playground",inputCostPerMil:"0.1999",outputCostPerMil:"0.8001"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:1024,range:[0,8192]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-08-19"},{id:"chutes:deepseek-v3.1",name:"DeepSeek V3.1",provider:"chutes",providerHumanName:"Chutes",makerHumanName:"DeepSeek",modelApiName:"deepseek-ai/DeepSeek-V3.1",minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"deepseek",model:{primary:"deepseek-v3.1"}},info:{description:"DeepSeek-V3.1 is post-trained on the top of DeepSeek-V3.1-Base, which is built upon the original V3 base checkpoint through a two-phase long context extension approach, following the methodology outlined in the original DeepSeek-V3 report. DeepSeek has expanded their dataset by collecting additional long documents and substantially extending both training phases.",website:"https://chutes.ai",modelUrl:"https://chutes.ai/app/chute/07cb1b3a-ec4d-594a-96c2-b547fddcadb0?tab=playground",contextWindow:163840,pricing:{pricingUrl:"https://chutes.ai/app/chute/07cb1b3a-ec4d-594a-96c2-b547fddcadb0?tab=playground",inputCostPerMil:"0.1999",outputCostPerMil:"0.8001"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:1024,range:[0,8192]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-08-21"},{id:"chutes:longcat-flash-chat",name:"LongCat Flash Chat",provider:"chutes",providerHumanName:"Chutes",makerHumanName:"Meituan",modelApiName:"meituan-longcat/LongCat-Flash-Chat-FP8",minBillingTier:"hobby",supportsStructuredOutput:!1,supportsToolCalling:!0,gateway:{chef:"meituan",model:{primary:"longcat-flash-chat",secondary:["longcat-flash-chat-fp8"]}},info:{description:"LongCat-Flash-Chat is a high-throughput MoE chat model (128k context) optimized for agentic tasks.",website:"https://chutes.ai",modelUrl:"https://chutes.ai/app/chute/2a6173bd-6d60-5ca6-8601-9ece77f055e4?tab=api",contextWindow:128e3,pricing:{pricingUrl:"https://chutes.ai/app/chute/2a6173bd-6d60-5ca6-8601-9ece77f055e4?tab=api",inputCostPerMil:"0",outputCostPerMil:"0"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:1024,range:[0,8192]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-09-22"},{id:"chutes:longcat-flash-thinking",name:"LongCat Flash Thinking",provider:"chutes",providerHumanName:"Chutes",makerHumanName:"Meituan",modelApiName:"meituan-longcat/LongCat-Flash-Thinking-FP8",minBillingTier:"hobby",supportsStructuredOutput:!1,supportsToolCalling:!0,gateway:{chef:"meituan",model:{primary:"longcat-flash-thinking",secondary:["longcat-flash-thinking-fp8"]}},info:{description:"LongCat-Flash-Thinking is a high-throughput MoE reasoning model (128k context) optimized for agentic tasks.",website:"https://chutes.ai",modelUrl:"https://chutes.ai/app/chute/5b3f1789-3247-5315-a7cd-78526725455e",contextWindow:128e3,pricing:{pricingUrl:"https://chutes.ai/app/chute/5b3f1789-3247-5315-a7cd-78526725455e",inputCostPerMil:"0.15",outputCostPerMil:"1.5"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:1024,range:[0,8192]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-09-22"},{id:"cohere:command-a",provider:"cohere",providerHumanName:"Cohere",makerHumanName:"Cohere",modelApiName:"command-a-03-2025",name:"Command A",supportsStructuredOutput:!1,supportsToolCalling:!0,info:{description:"Command A is Cohere's most performant model to date, excelling at tool use, agents, retrieval augmented generation (RAG), and multilingual use cases. Command A has a context length of 256K, only requires two GPUs to run, and has 150% higher throughput compared to Command R+ 08-2024.",modelUrl:"https://docs.cohere.com/v2/docs/command-a",contextWindow:256e3,website:"https://cohere.com",pricing:{pricingUrl:"https://cohere.com/pricing",inputCostPerMil:"2.5",outputCostPerMil:"10.0"}},parameters:{temperature:{value:.9,range:[0,2]},maximumLength:{value:4e3,range:[50,8e3]},topP:{value:1,range:[0,1]},topK:{value:1,range:[0,500]},presencePenalty:{value:0,range:[0,1]},frequencyPenalty:{value:0,range:[0,1]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-03-13"},{id:"cohere:command-r-plus",provider:"cohere",providerHumanName:"Cohere",makerHumanName:"Cohere",modelApiName:"command-r-plus",name:"Command R+",supportsStructuredOutput:!1,supportsToolCalling:!0,info:{description:"Command R+ is Cohere's newest large language model, optimized for conversational interaction and long-context tasks. It aims at being extremely performant, enabling companies to move beyond proof of concept and into production.",modelUrl:"https://docs.cohere.com/docs/command-r-plus",contextWindow:128e3,website:"https://cohere.com",pricing:{pricingUrl:"https://cohere.com/pricing",inputCostPerMil:"2.5",outputCostPerMil:"10.0"}},parameters:{temperature:{value:.9,range:[0,2]},maximumLength:{value:1024,range:[50,4096]},topP:{value:1,range:[0,1]},topK:{value:1,range:[0,500]},presencePenalty:{value:0,range:[0,1]},frequencyPenalty:{value:0,range:[0,1]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-02-19"},{id:"cohere:command-r",provider:"cohere",providerHumanName:"Cohere",makerHumanName:"Cohere",modelApiName:"command-r",name:"Command R",supportsStructuredOutput:!1,supportsToolCalling:!0,info:{description:'Command R is a large language model optimized for conversational interaction and long context tasks. It targets the "scalable" category of models that balance high performance with strong accuracy, enabling companies to move beyond proof of concept and into production.',modelUrl:"https://docs.cohere.com/docs/command-r",contextWindow:128e3,website:"https://cohere.com",pricing:{pricingUrl:"https://cohere.com/pricing",inputCostPerMil:"0.15",outputCostPerMil:"0.6"}},parameters:{temperature:{value:.9,range:[0,2]},maximumLength:{value:1024,range:[50,4096]},topP:{value:1,range:[0,1]},topK:{value:1,range:[0,500]},presencePenalty:{value:0,range:[0,1]},frequencyPenalty:{value:0,range:[0,1]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-02-19"},{id:"cohere:command-light-nightly",provider:"cohere",providerHumanName:"Cohere",makerHumanName:"Cohere",modelApiName:"command-light-nightly",name:"Command Light Nightly",supportsStructuredOutput:!1,supportsToolCalling:!0,disableInGateway:!0,disabled:!0,info:{description:"A smaller and faster version of Cohere's command model with almost as much capability but improved speed.",modelUrl:"https://docs.cohere.com/docs/command-beta",contextWindow:4096,website:"https://cohere.com",pricing:{pricingUrl:"https://cohere.com/pricing"}},parameters:{temperature:{value:.9,range:[0,2]},maximumLength:{value:200,range:[50,1024]},topP:{value:1,range:[0,1]},topK:{value:0,range:[0,500]},presencePenalty:{value:0,range:[0,1]},frequencyPenalty:{value:0,range:[0,1]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-02-19"},{id:"cohere:command-nightly",provider:"cohere",providerHumanName:"Cohere",makerHumanName:"Cohere",modelApiName:"command-nightly",name:"Command Nightly",supportsStructuredOutput:!1,supportsToolCalling:!0,disableInGateway:!0,disabled:!0,info:{description:"An instruction-following conversational model by Cohere that performs language tasks with high quality and reliability while providing longer context compared to generative models.",modelUrl:"https://docs.cohere.com/docs/command-beta",contextWindow:4096,website:"https://cohere.com",pricing:{pricingUrl:"https://cohere.com/pricing"}},parameters:{temperature:{value:.9,range:[0,2]},maximumLength:{value:200,range:[50,1024]},topP:{value:1,range:[0,1]},topK:{value:0,range:[0,500]},presencePenalty:{value:0,range:[0,1]},frequencyPenalty:{value:0,range:[0,1]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-02-19"},{id:"deepinfra:llama-4-maverick-17b-128e-instruct-fp8",name:"Llama 4 Maverick 17B 128E Instruct FP8",provider:"deepinfra",providerHumanName:"DeepInfra",makerHumanName:"DeepInfra",modelApiName:"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",minBillingTier:"pro",new:!0,supportsStructuredOutput:!1,supportsToolCalling:!1,supportsVision:!0,info:{description:"The Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding. Llama 4 Maverick, a 17 billion parameter model with 128 experts. Served by DeepInfra.",website:"https://deepinfra.com",modelUrl:"https://deepinfra.com/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",contextWindow:131072,pricing:{pricingUrl:"https://deepinfra.com/pricing",inputCostPerMil:"0.15",outputCostPerMil:"0.6"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-04-05"},{id:"deepinfra:llama-4-scout-17b-16e-instruct",name:"Llama 4 Scout 17B 16E Instruct",provider:"deepinfra",providerHumanName:"DeepInfra",makerHumanName:"DeepInfra",modelApiName:"meta-llama/Llama-4-Scout-17B-16E-Instruct",minBillingTier:"pro",new:!0,supportsStructuredOutput:!1,supportsToolCalling:!1,supportsVision:!0,info:{description:"The Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding. Llama 4 Scout, a 17 billion parameter model with 16 experts. Served by DeepInfra.",website:"https://deepinfra.com",modelUrl:"https://deepinfra.com/meta-llama/Llama-4-Scout-17B-16E-Instruct",contextWindow:131072,pricing:{pricingUrl:"https://deepinfra.com/pricing",inputCostPerMil:"0.08",outputCostPerMil:"0.3"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-04-05"},{id:"deepinfra:qwen3-235b-a22b",name:"Qwen3-235B-A22B",provider:"deepinfra",providerHumanName:"DeepInfra",makerHumanName:"DeepInfra",modelApiName:"Qwen/Qwen3-235B-A22B",minBillingTier:"pro",new:!0,supportsStructuredOutput:!1,supportsToolCalling:!1,supportsVision:!0,info:{description:"Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support",website:"https://deepinfra.com",modelUrl:"https://deepinfra.com/Qwen/Qwen3-235B-A22B",contextWindow:40960,pricing:{pricingUrl:"https://deepinfra.com/pricing",inputCostPerMil:"0.13",outputCostPerMil:"0.6"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:8192,range:[0,16384]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-04-29"},{id:"deepinfra:qwen3-30b-a3b",name:"Qwen3-30B-A3B",provider:"deepinfra",providerHumanName:"DeepInfra",makerHumanName:"DeepInfra",modelApiName:"Qwen/Qwen3-30B-A3B",minBillingTier:"pro",new:!0,supportsStructuredOutput:!1,supportsToolCalling:!1,supportsVision:!0,info:{description:"Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support",website:"https://deepinfra.com",modelUrl:"https://deepinfra.com/Qwen/Qwen3-235B-A22B",contextWindow:40960,pricing:{pricingUrl:"https://deepinfra.com/pricing",inputCostPerMil:"0.08",outputCostPerMil:"0.29"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:8192,range:[0,16384]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-04-29"},{id:"deepinfra:qwen3-32b",name:"Qwen3-32B",provider:"deepinfra",providerHumanName:"DeepInfra",makerHumanName:"DeepInfra",modelApiName:"Qwen/Qwen3-32B",minBillingTier:"pro",new:!0,supportsStructuredOutput:!1,supportsToolCalling:!1,supportsVision:!0,info:{description:"Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support",website:"https://deepinfra.com",modelUrl:"https://deepinfra.com/Qwen/Qwen3-32B",contextWindow:40960,pricing:{pricingUrl:"https://deepinfra.com/pricing",inputCostPerMil:"0.1",outputCostPerMil:"0.3"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:8192,range:[0,16384]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-04-29"},{id:"deepinfra:qwen3-14b",name:"Qwen3-14B",provider:"deepinfra",providerHumanName:"DeepInfra",makerHumanName:"DeepInfra",modelApiName:"Qwen/Qwen3-14B",minBillingTier:"hobby",new:!0,supportsStructuredOutput:!1,supportsToolCalling:!1,supportsVision:!0,info:{description:"Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support",website:"https://deepinfra.com",modelUrl:"https://deepinfra.com/Qwen/Qwen3-14B",contextWindow:40960,pricing:{pricingUrl:"https://deepinfra.com/pricing",inputCostPerMil:"0.06",outputCostPerMil:"0.24"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:8192,range:[0,16384]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-04-29"},{id:"deepinfra:kimi-k2-instruct",name:"Kimi K2",provider:"deepinfra",providerHumanName:"DeepInfra",makerHumanName:"Moonshot AI",modelApiName:"moonshotai/Kimi-K2-Instruct",minBillingTier:"hobby",new:!0,supportsStructuredOutput:!1,supportsToolCalling:!1,supportsVision:!0,gateway:{chef:"moonshotai",model:{primary:"kimi-k2",secondary:["kimi-k2-instruct"]}},info:{description:"Kimi K2 is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active per forward pass. It is optimized for agentic capabilities, including advanced tool use, reasoning, and code synthesis. Kimi K2 excels across a broad range of benchmarks, particularly in coding (LiveCodeBench, SWE-bench), reasoning (ZebraLogic, GPQA), and tool-use (Tau2, AceBench) tasks.",website:"https://deepinfra.com",modelUrl:"https://deepinfra.com/moonshotai/Kimi-K2-Instruct",contextWindow:131072,pricing:{pricingUrl:"https://deepinfra.com/pricing",inputCostPerMil:"0.5",outputCostPerMil:"2"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:8192,range:[0,16384]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-07-17"},{id:"deepinfra:qwen3-coder-480b-a35b-instruct",name:"Qwen3 Coder",provider:"deepinfra",providerHumanName:"DeepInfra",makerHumanName:"Qwen",modelApiName:"Qwen/Qwen3-Coder-480B-A35B-Instruct",minBillingTier:"hobby",new:!0,supportsStructuredOutput:!1,supportsToolCalling:!1,supportsVision:!0,gateway:{chef:"alibaba",model:{primary:"qwen3-coder",secondary:["qwen3-coder-480b","qwen3-coder-480b-a35b-instruct"]}},info:{description:"Qwen3-Coder-480B-A35B-Instruct is Qwen's most agentic code model, featuring significant performance on Agentic Coding, Agentic Browser-Use and other foundational coding tasks, achieving results comparable to Claude Sonnet.",website:"https://deepinfra.com",modelUrl:"https://deepinfra.com/Qwen/Qwen3-Coder-480B-A35B-Instruct",contextWindow:262144,pricing:{pricingUrl:"https://deepinfra.com/pricing",inputCostPerMil:"0.4",outputCostPerMil:"1.6"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:8192,range:[0,66536]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-07-24"},{id:"deepinfra:devstral-small-2507",name:"Devstral Small",provider:"deepinfra",providerHumanName:"DeepInfra",makerHumanName:"Mistral",modelApiName:"mistralai/Devstral-Small-2507",minBillingTier:"hobby",new:!0,supportsStructuredOutput:!1,supportsToolCalling:!1,supportsVision:!0,gateway:{chef:"mistral",model:{primary:"devstral-small",secondary:["devstral-small-2507"]}},info:{description:"Devstral is an agentic LLM for software engineering tasks, making it a great choice for software engineering agents.",website:"https://deepinfra.com",modelUrl:"https://deepinfra.com/mistralai/Devstral-Small-2507",contextWindow:128e3,pricing:{pricingUrl:"https://deepinfra.com/pricing",inputCostPerMil:"0.07",outputCostPerMil:"0.28"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:8192,range:[0,128e3]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-07-24",disabled:!0},{id:"deepinfra:deepseek-v3.1",name:"DeepSeek V3.1",provider:"deepinfra",providerHumanName:"DeepInfra",makerHumanName:"DeepSeek",modelApiName:"deepseek-ai/DeepSeek-V3.1",minBillingTier:"hobby",new:!0,supportsStructuredOutput:!1,supportsToolCalling:!1,supportsVision:!0,gateway:{chef:"deepseek",model:{primary:"deepseek-v3.1"}},info:{description:"DeepSeek-V3.1 is post-trained on the top of DeepSeek-V3.1-Base, which is built upon the original V3 base checkpoint through a two-phase long context extension approach, following the methodology outlined in the original DeepSeek-V3 report. We have expanded our dataset by collecting additional long documents and substantially extending both training phases. The 32K extension phase has been increased 10-fold to 630B tokens, while the 128K extension phase has been extended by 3.3x to 209B tokens. Additionally, DeepSeek-V3.1 is trained using the UE8M0 FP8 scale data format to ensure compatibility with microscaling data formats.",website:"https://deepinfra.com",modelUrl:"https://deepinfra.com/deepseek-ai/DeepSeek-V3.1",contextWindow:163840,pricing:{pricingUrl:"https://deepinfra.com/pricing",inputCostPerMil:"0.3",outputCostPerMil:"1"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:8192,range:[0,128e3]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-08-22"},{id:"deepinfra:qwen3-next-80b-a3b-instruct",name:"Qwen3 Next 80B A3B Instruct",provider:"deepinfra",providerHumanName:"DeepInfra",makerHumanName:"Alibaba Cloud",modelApiName:"Qwen/Qwen3-Next-80B-A3B-Instruct",minBillingTier:"hobby",new:!0,supportsStructuredOutput:!1,supportsToolCalling:!1,supportsVision:!0,disabled:!0,gateway:{chef:"alibaba",model:{primary:"qwen3-next-80b-a3b-instruct"}},info:{description:"Over the past few months, we have observed increasingly clear trends toward scaling both total parameters and context lengths in the pursuit of more powerful and agentic artificial intelligence (AI). We are excited to share our latest advancements in addressing these demands, centered on improving scaling efficiency through innovative model architecture. We call this next-generation foundation models Qwen3-Next.",website:"https://deepinfra.com",modelUrl:"https://deepinfra.com/Qwen/Qwen3-Next-80B-A3B-Instruct",contextWindow:163840,pricing:{pricingUrl:"https://deepinfra.com/pricing",inputCostPerMil:"0.14",outputCostPerMil:"1.4"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:8192,range:[0,32768]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-09-12"},{id:"deepinfra:qwen3-next-80b-a3b-thinking",name:"Qwen3 Next 80B A3B Thinking",provider:"deepinfra",providerHumanName:"DeepInfra",makerHumanName:"Alibaba Cloud",modelApiName:"Qwen/Qwen3-Next-80B-A3B-Thinking",minBillingTier:"hobby",new:!0,supportsStructuredOutput:!1,supportsToolCalling:!1,supportsVision:!0,gateway:{chef:"alibaba",model:{primary:"qwen3-next-80b-a3b-thinking"}},info:{description:"Over the past few months, we have observed increasingly clear trends toward scaling both total parameters and context lengths in the pursuit of more powerful and agentic artificial intelligence (AI). We are excited to share our latest advancements in addressing these demands, centered on improving scaling efficiency through innovative model architecture. We call this next-generation foundation models Qwen3-Next.",website:"https://deepinfra.com",modelUrl:"https://deepinfra.com/Qwen/Qwen3-Next-80B-A3B-Thinking",contextWindow:163840,pricing:{pricingUrl:"https://deepinfra.com/pricing",inputCostPerMil:"0.14",outputCostPerMil:"1.4"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:8192,range:[0,32768]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-09-12"},{id:"deepseek:chat",name:"DeepSeek-V3",provider:"fireworks",providerHumanName:"DeepSeek hosted on Fireworks",makerHumanName:"DeepSeek",modelApiName:"accounts/fireworks/models/deepseek-v3",minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,disabled:!0,info:{description:"DeepSeek-V3 is an open-source large language model that builds upon LLaMA (Meta's foundational language model) to enable versatile functionalities such as text generation, code completion, and more, served by Fireworks AI.",website:"https://www.fireworks.ai/",modelUrl:"https://fireworks.ai/models/fireworks/deepseek-v3",contextWindow:128e3,pricing:{pricingUrl:"https://fireworks.ai/pricing#text",inputCostPerMil:"0.9",outputCostPerMil:"0.9"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:1024,range:[0,8192]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-02-19"},{id:"deepseek:deepseek-r1",name:"DeepSeek R1",modelApiName:"accounts/fireworks/models/deepseek-r1",provider:"fireworks",providerHumanName:"DeepSeek hosted on Fireworks",makerHumanName:"DeepSeek",supportsStructuredOutput:!1,supportsToolCalling:!1,minBillingTier:"pro",disabled:!0,info:{description:"DeepSeek Reasoner is a specialized model developed by DeepSeek that uses Chain of Thought (CoT) reasoning to improve response accuracy. Before providing a final answer, it generates detailed reasoning steps that are accessible through the API, allowing users to examine and leverage the model's thought process, served by Fireworks AI.",website:"https://www.fireworks.ai/",modelUrl:"https://fireworks.ai/models/fireworks/deepseek-r1",contextWindow:16e4,pricing:{pricingUrl:"https://fireworks.ai/pricing#text",inputCostPerMil:"3",outputCostPerMil:"8"}},parameters:{temperature:{value:.7,range:[.01,2]},maximumLength:{value:1024,range:[300,8192]},topP:{value:1,range:[.01,1]},topK:{value:1,range:[.01,2048]},presencePenalty:{value:0,range:[-2,2]},frequencyPenalty:{value:.1,range:[.01,1]}},lastModifiedDate:"2025-02-19"},{id:"deepseek:deepseek-r1-0528",name:"DeepSeek R1 0528",modelApiName:"deepseek-reasoner",provider:"deepseek",providerHumanName:"DeepSeek",makerHumanName:"DeepSeek",supportsStructuredOutput:!1,supportsToolCalling:!1,minBillingTier:"pro",new:!0,disabled:!0,info:{description:"The DeepSeek R1 model has undergone a minor version upgrade, with the current version being DeepSeek-R1-0528. In the latest update, DeepSeek R1 has significantly improved its depth of reasoning and inference capabilities by leveraging increased computational resources and introducing algorithmic optimization mechanisms during post-training. The model has demonstrated outstanding performance across various benchmark evaluations, including mathematics, programming, and general logic. Its overall performance is now approaching that of leading models, such as O3 and Gemini 2.5 Pro.",website:"https://www.deepseek.com",modelUrl:"https://huggingface.co/deepseek-ai/DeepSeek-R1-0528",contextWindow:64e3,pricing:{pricingUrl:"https://api-docs.deepseek.com/quick_start/pricing",inputCostPerMil:"0.56",outputCostPerMil:"1.68",cachedInputCostPerMil:"0.07"}},parameters:{temperature:{value:.7,range:[.01,2]},maximumLength:{value:4096,range:[300,8192]},topP:{value:1,range:[.01,1]},topK:{value:1,range:[.01,2048]},presencePenalty:{value:0,range:[-2,2]},frequencyPenalty:{value:.1,range:[.01,1]}},lastModifiedDate:"2025-05-29"},{id:"deepseek:deepseek-v3.1",name:"DeepSeek V3.1",modelApiName:"deepseek-chat",provider:"deepseek",providerHumanName:"DeepSeek",makerHumanName:"DeepSeek",supportsStructuredOutput:!1,supportsToolCalling:!1,minBillingTier:"pro",new:!0,disabled:!0,gateway:{chef:"deepseek",model:{primary:"deepseek-v3.1"}},info:{description:"DeepSeek-V3.1 marks DeepSeek's first step toward the agent era with revolutionary hybrid inference capabilities. Operates in two modes: Think and Non-Think. The Think variant delivers faster reasoning compared to DeepSeek-R1-0528, reaching answers more efficiently while maintaining high-quality outputs. Enhanced through specialized post-training, the model excels at tool usage and complex multi-step agent tasks.",website:"https://www.deepseek.com",modelUrl:"https://api-docs.deepseek.com/news/news250821",contextWindow:128e3,pricing:{pricingUrl:"https://api-docs.deepseek.com/quick_start/pricing",inputCostPerMil:"0.56",outputCostPerMil:"1.68",cachedInputCostPerMil:"0.07"}},parameters:{temperature:{value:.7,range:[.01,2]},maximumLength:{value:4096,range:[300,8192]},topP:{value:1,range:[.01,1]},topK:{value:1,range:[.01,2048]},presencePenalty:{value:0,range:[-2,2]},frequencyPenalty:{value:.1,range:[.01,1]}},lastModifiedDate:"2025-08-22"},{id:"deepseek:deepseek-v3.1-thinking",name:"DeepSeek V3.1 Thinking",modelApiName:"deepseek-reasoner",provider:"deepseek",providerHumanName:"DeepSeek",makerHumanName:"DeepSeek",supportsStructuredOutput:!1,supportsToolCalling:!1,minBillingTier:"pro",new:!0,disabled:!0,gateway:{chef:"deepseek",model:{primary:"deepseek-v3.1-thinking"}},info:{description:"DeepSeek-V3.1 marks DeepSeek's first step toward the agent era with revolutionary hybrid inference capabilities. Operates in two modes: Think and Non-Think. The Think variant delivers faster reasoning compared to DeepSeek-R1-0528, reaching answers more efficiently while maintaining high-quality outputs. Enhanced through specialized post-training, the model excels at tool usage and complex multi-step agent tasks.",website:"https://www.deepseek.com",modelUrl:"https://api-docs.deepseek.com/news/news250821",contextWindow:128e3,pricing:{pricingUrl:"https://api-docs.deepseek.com/quick_start/pricing",inputCostPerMil:"0.56",outputCostPerMil:"1.68",cachedInputCostPerMil:"0.07"}},parameters:{temperature:{value:.7,range:[.01,2]},maximumLength:{value:4096,range:[300,8192]},topP:{value:1,range:[.01,1]},topK:{value:1,range:[.01,2048]},presencePenalty:{value:0,range:[-2,2]},frequencyPenalty:{value:.1,range:[.01,1]}},lastModifiedDate:"2025-08-22"},{id:"deepseek:deepseek-v3.2-exp",name:"DeepSeek V3.2 Exp",modelApiName:"deepseek-chat",provider:"deepseek",providerHumanName:"DeepSeek",makerHumanName:"DeepSeek",supportsStructuredOutput:!1,supportsToolCalling:!1,minBillingTier:"pro",new:!0,gateway:{chef:"deepseek",model:{primary:"deepseek-v3.2-exp",secondary:["deepseek-chat"]}},info:{description:"DeepSeek-V3.2-Exp is an experimental model introducing the groundbreaking DeepSeek Sparse Attention (DSA) mechanism for enhanced long-context processing efficiency. Built on V3.1-Terminus, DSA achieves fine-grained sparse attention while maintaining identical output quality.",website:"https://www.deepseek.com",modelUrl:"https://api-docs.deepseek.com/news/news250929",contextWindow:163840,pricing:{pricingUrl:"https://api-docs.deepseek.com/quick_start/pricing",inputCostPerMil:"0.28",outputCostPerMil:"0.42"}},parameters:{temperature:{value:.7,range:[.01,2]},maximumLength:{value:4096,range:[300,8192]},topP:{value:1,range:[.01,1]},topK:{value:1,range:[.01,2048]},presencePenalty:{value:0,range:[-2,2]},frequencyPenalty:{value:.1,range:[.01,1]}},lastModifiedDate:"2025-09-29"},{id:"deepseek:deepseek-v3.2-exp-thinking",name:"DeepSeek V3.2 Exp Thinking",modelApiName:"deepseek-reasoner",provider:"deepseek",providerHumanName:"DeepSeek",makerHumanName:"DeepSeek",supportsStructuredOutput:!1,supportsToolCalling:!1,minBillingTier:"pro",new:!0,gateway:{chef:"deepseek",model:{primary:"deepseek-v3.2-exp-thinking",secondary:["deepseek-reasoner"]}},info:{description:"DeepSeek-V3.2-Exp is an experimental model introducing the groundbreaking DeepSeek Sparse Attention (DSA) mechanism for enhanced long-context processing efficiency. Built on V3.1-Terminus, DSA achieves fine-grained sparse attention while maintaining identical output quality.",website:"https://www.deepseek.com",modelUrl:"https://api-docs.deepseek.com/news/news250929",contextWindow:163840,pricing:{pricingUrl:"https://api-docs.deepseek.com/quick_start/pricing",inputCostPerMil:"0.28",outputCostPerMil:"0.42"}},parameters:{temperature:{value:.7,range:[.01,2]},maximumLength:{value:4096,range:[300,8192]},topP:{value:1,range:[.01,1]},topK:{value:1,range:[.01,2048]},presencePenalty:{value:0,range:[-2,2]},frequencyPenalty:{value:.1,range:[.01,1]}},lastModifiedDate:"2025-09-29"},{id:"fireworks:deepseek-v3",name:"DeepSeek-V3",provider:"fireworks",providerHumanName:"DeepSeek hosted on Fireworks",makerHumanName:"Fireworks",modelApiName:"accounts/fireworks/models/deepseek-v3",minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"DeepSeek-V3 is an open-source large language model that builds upon LLaMA (Meta's foundational language model) to enable versatile functionalities such as text generation, code completion, and more, served by Fireworks AI.",website:"https://www.fireworks.ai/",modelUrl:"https://fireworks.ai/models/fireworks/deepseek-v3",contextWindow:128e3,pricing:{pricingUrl:"https://fireworks.ai/pricing#text",inputCostPerMil:"0.9",outputCostPerMil:"0.9"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:1024,range:[0,8192]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-02-19"},{id:"fireworks:deepseek-v3.1",name:"DeepSeek-V3.1",provider:"fireworks",providerHumanName:"DeepSeek hosted on Fireworks",makerHumanName:"Fireworks",modelApiName:"accounts/fireworks/models/deepseek-v3.1",minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"deepseek",model:{primary:"deepseek-v3.1"}},info:{description:"DeepSeek-V3.1 is post-trained on the top of DeepSeek-V3.1-Base, which is built upon the original V3 base checkpoint through a two-phase long context extension approach, following the methodology outlined in the original DeepSeek-V3 report. DeepSeek has expanded their dataset by collecting additional long documents and substantially extending both training phases.",website:"https://www.fireworks.ai/",modelUrl:"https://fireworks.ai/models/fireworks/deepseek-v3p1",contextWindow:163840,pricing:{pricingUrl:"https://fireworks.ai/pricing#text",inputCostPerMil:"1.2",outputCostPerMil:"1.2"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:1024,range:[0,8192]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-08-21"},{id:"fireworks:deepseek-r1",name:"DeepSeek R1",modelApiName:"accounts/fireworks/models/deepseek-r1",provider:"fireworks",providerHumanName:"DeepSeek hosted on Fireworks",makerHumanName:"Fireworks",supportsStructuredOutput:!1,supportsToolCalling:!1,minBillingTier:"pro",info:{description:"DeepSeek Reasoner is a specialized model developed by DeepSeek that uses Chain of Thought (CoT) reasoning to improve response accuracy. Before providing a final answer, it generates detailed reasoning steps that are accessible through the API, allowing users to examine and leverage the model's thought process, served by Fireworks AI.",website:"https://www.fireworks.ai/",modelUrl:"https://fireworks.ai/models/fireworks/deepseek-r1",contextWindow:16e4,pricing:{pricingUrl:"https://fireworks.ai/pricing#text",inputCostPerMil:"3",outputCostPerMil:"8"}},parameters:{temperature:{value:.7,range:[.01,2]},maximumLength:{value:1024,range:[300,8192]},topP:{value:1,range:[.01,1]},topK:{value:1,range:[.01,2048]},presencePenalty:{value:0,range:[-2,2]},frequencyPenalty:{value:.1,range:[.01,1]}},lastModifiedDate:"2025-02-19"},{id:"fireworks:firefunction-v1",provider:"fireworks",providerHumanName:"Fireworks",modelApiName:"accounts/fireworks/models/firefunction-v1",makerHumanName:"Fireworks",supportsStructuredOutput:!0,supportsToolCalling:!0,disabled:!0,info:{description:"Fireworks' GPT-4-level function calling model - 4x faster than GPT-4 and open weights.",website:"https://fireworks.ai/blog/firefunction-v1-gpt-4-level-function-calling",modelUrl:"https://fireworks.ai/models/fireworks/firefunction-v1",contextWindow:32768,pricing:{pricingUrl:"https://fireworks.ai/models/fireworks/firefunction-v1",inputCostPerMil:"0.9",outputCostPerMil:"0.9"}},parameters:{temperature:{value:.6,range:[.01,5]},maximumLength:{value:4096,range:[0,32768]},topP:{value:1,range:[.01,1]},frequencyPenalty:{value:0,range:[-2,2]}},name:"FireFunction V1",lastModifiedDate:"2025-02-19"},{id:"fireworks:dbrx-instruct",provider:"fireworks",providerHumanName:"Fireworks",modelApiName:"accounts/fireworks/models/dbrx-instruct",makerHumanName:"Fireworks",disabled:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"DBRX Instruct is a mixture-of-experts (MoE) large language model trained from scratch by Databricks. DBRX Instruct specializes in few-turn interactions. Dbrx is hosted as an experimental model.",website:"https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm",modelUrl:"https://fireworks.ai/models/fireworks/dbrx-instruct",contextWindow:32768,pricing:{pricingUrl:"https://fireworks.ai/models/fireworks/dbrx-instruct",inputCostPerMil:"1.6",outputCostPerMil:"1.6"}},parameters:{temperature:{value:.6,range:[0,2]},maximumLength:{value:4096,range:[0,32768]},topP:{value:1,range:[0,1]},topK:{value:40,range:[1,100]},presencePenalty:{value:0,range:[-2,2]},frequencyPenalty:{value:0,range:[-2,2]}},name:"dbrx-instruct",lastModifiedDate:"2024-02-01"},{id:"fireworks:llama7b-v2-chat",provider:"fireworks",providerHumanName:"Fireworks",modelApiName:"accounts/fireworks/models/llama-v2-7b-chat",makerHumanName:"Meta",disabled:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"7 billion parameter open source model by Meta fine-tuned for chat purposes served by Fireworks. LLaMA v2 was trained on more data (~2 trillion tokens) compared to LLaMA v1 and supports context windows up to 4k tokens.",website:"https://ai.meta.com/llama/",modelUrl:"https://ai.meta.com/llama/",contextWindow:4096,pricing:{pricingUrl:"https://readme.fireworks.ai/page/pricing",inputCostPerMil:"0.07",outputCostPerMil:"0.28"}},parameters:{temperature:{value:.75,range:[.01,5]},maximumLength:{value:1e3,range:[300,3e3]},topP:{value:1,range:[.01,1]},frequencyPenalty:{value:1,range:[.01,1]}},version:"058333670f2a6e88cf1b29b8183405b17bb997767282f790b82137df8c090c1f",name:"llama-2-7b-chat",lastModifiedDate:"2023-07-18"},{id:"fireworks:llama13b-v2-chat",provider:"fireworks",providerHumanName:"Fireworks",makerHumanName:"Meta",modelApiName:"accounts/fireworks/models/llama-v2-13b-chat",disabled:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"13 billion parameter open source model by Meta fine-tuned for chat purposes served by Fireworks. LLaMA v2 was trained on more data (~2 trillion tokens) compared to LLaMA v1 and supports context windows up to 4k tokens.",website:"https://ai.meta.com/llama/",modelUrl:"https://ai.meta.com/llama/",contextWindow:4096,pricing:{pricingUrl:"https://readme.fireworks.ai/page/pricing",inputCostPerMil:"0.14",outputCostPerMil:"0.56"}},parameters:{temperature:{value:.75,range:[.01,5]},maximumLength:{value:1e3,range:[300,3e3]},topP:{value:1,range:[.01,1]},frequencyPenalty:{value:1,range:[.01,1]}},name:"llama-2-13b-chat",lastModifiedDate:"2023-07-18"},{id:"fireworks:llama-2-70b-chat",provider:"fireworks",providerHumanName:"Fireworks",makerHumanName:"Meta",modelApiName:"accounts/fireworks/models/llama-v2-70b-chat",disabled:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"70 billion parameter open source model by Meta fine-tuned for chat purposes served by Fireworks. LLaMA v2 was trained on more data (~2 trillion tokens) compared to LLaMA v1 and supports context windows up to 4k tokens.",website:"https://ai.meta.com/llama/",modelUrl:"https://ai.meta.com/llama/",contextWindow:4096,pricing:{pricingUrl:"https://readme.fireworks.ai/page/pricing",inputCostPerMil:"0.7",outputCostPerMil:"2.8"}},parameters:{temperature:{value:.75,range:[.01,5]},maximumLength:{value:1e3,range:[300,3e3]},topP:{value:1,range:[.01,1]},frequencyPenalty:{value:1,range:[.01,1]}},name:"llama-2-70b-chat",lastModifiedDate:"2023-07-18"},{id:"fireworks:mixtral-8x22b-instruct",provider:"fireworks",providerHumanName:"Fireworks",modelApiName:"accounts/fireworks/models/mixtral-8x22b-instruct",makerHumanName:"Mistral",supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"8x22b Instruct model. 8x22b is mixture-of-experts open source model by Mistral served by Fireworks.",website:"https://x.com/FireworksAI_HQ/status/1778617118583586852",modelUrl:"https://fireworks.ai/models/fireworks/mixtral-8x22b-instruct",contextWindow:2048,pricing:{pricingUrl:"https://fireworks.ai/models/fireworks/mixtral-8x22b-instruct",inputCostPerMil:"1.2",outputCostPerMil:"1.2"}},parameters:{temperature:{value:.6,range:[0,2]},maximumLength:{value:256,range:[0,2048]},topP:{value:1,range:[0,1]},topK:{value:40,range:[1,100]},presencePenalty:{value:0,range:[-2,2]},frequencyPenalty:{value:0,range:[-2,2]}},name:"Mixtral MoE 8x22B Instruct",lastModifiedDate:"2025-02-19"},{id:"fireworks:mixtral-8x22b-instruct-preview",provider:"fireworks",providerHumanName:"Fireworks",modelApiName:"accounts/fireworks/models/mixtral-8x22b-instruct-preview",makerHumanName:"Mistral",disabled:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"8x22b Instruct model. 8x22b is mixture-of-experts open source model by Mistral served by Fireworks.",website:"https://x.com/FireworksAI_HQ/status/1778617118583586852",modelUrl:"https://fireworks.ai/models/fireworks/mixtral-8x22b-instruct-preview",contextWindow:2048,pricing:{pricingUrl:"https://fireworks.ai/models/fireworks/mixtral-8x22b-instruct-preview",inputCostPerMil:"0.9",outputCostPerMil:"0.9"}},parameters:{temperature:{value:.6,range:[0,2]},maximumLength:{value:256,range:[0,2048]},topP:{value:1,range:[0,1]},topK:{value:40,range:[1,100]},presencePenalty:{value:0,range:[-2,2]},frequencyPenalty:{value:0,range:[-2,2]}},name:"mixtral-8x22b-instruct-preview",lastModifiedDate:"2023-07-18"},{id:"fireworks:mixtral-8x22b",provider:"fireworks",providerHumanName:"Fireworks",makerHumanName:"Mistral",disabled:!0,modelApiName:"accounts/fireworks/models/mixtral-8x22b",supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"8x22b mixture-of-experts open source model by Mistral served by Fireworks.",website:"https://mistral.ai/",modelUrl:"https://twitter.com/MistralAI/status/1777872671709057307/",contextWindow:65536,pricing:{pricingUrl:"https://docs.mistral.ai/platform/pricing/",inputCostPerMil:"1.2",outputCostPerMil:"1.2"}},parameters:{temperature:{value:.75,range:[.01,2]},maximumLength:{value:1e3,range:[300,3e3]},topP:{value:1,range:[.01,1]},frequencyPenalty:{value:1,range:[.01,1]}},name:"mixtral-8x22b",lastModifiedDate:"2023-07-18"},{id:"fireworks:mistral-7b-instruct-4k",modelApiName:"accounts/fireworks/models/mistral-7b-instruct-4k",provider:"fireworks",providerHumanName:"Fireworks",makerHumanName:"Mistral",disabled:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"The Mistral-7B-Instruct-v0.1 Large Language Model (LLM) is a instruct fine-tuned version of the Mistral-7B-v0.1 served by Fireworks.",website:"https://mistral.ai/",modelUrl:"https://mistral.ai/news/announcing-mistral-7b/",contextWindow:4096,pricing:{pricingUrl:"https://readme.fireworks.ai/page/pricing",inputCostPerMil:"0.2",outputCostPerMil:"0.8"}},parameters:{temperature:{value:.75,range:[.01,5]},maximumLength:{value:1e3,range:[300,3e3]},topP:{value:1,range:[.01,1]},frequencyPenalty:{value:1,range:[.01,1]}},name:"mistral-7b-instruct-4k",lastModifiedDate:"2023-07-18"},{id:"fireworks:mixtral-8x7b",provider:"fireworks",providerHumanName:"Fireworks",modelApiName:"accounts/fireworks/models/mixtral-8x7b",makerHumanName:"Mistral",disabled:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"Mistral MoE LLM model with 8 experts, each 7B. Warning: unofficial implementation + served by Fireworks.",website:"https://mistral.ai/",modelUrl:"https://x.com/MistralAI/status/1733150512395038967?s=20",contextWindow:4096,pricing:{pricingUrl:"https://readme.fireworks.ai/page/pricing",inputCostPerMil:"0.2",outputCostPerMil:"0.8"}},parameters:{temperature:{value:.5,range:[.01,5]},maximumLength:{value:400,range:[300,4e3]},topP:{value:1,range:[.01,1]},frequencyPenalty:{value:1,range:[.01,1]}},name:"mixtral-8x7b",lastModifiedDate:"2023-07-18"},{id:"fireworks:mixtral-8x7b-instruct",provider:"fireworks",providerHumanName:"Fireworks",modelApiName:"accounts/fireworks/models/mixtral-8x7b-instruct",makerHumanName:"Mistral",supportsStructuredOutput:!0,supportsToolCalling:!0,disabled:!0,info:{description:"Mistral MoE 8x7B Instruct v0.1 model with Sparse Mixture of Experts. Fine tuned for instruction following.Warning: unofficial implementation + served by Fireworks.",website:"https://mistral.ai/",modelUrl:"https://fireworks.ai/models/fireworks/mixtral-8x7b-instruct",contextWindow:4096,pricing:{pricingUrl:"https://readme.fireworks.ai/page/pricing",inputCostPerMil:"0.5",outputCostPerMil:"0.5"}},parameters:{temperature:{value:.5,range:[.01,5]},maximumLength:{value:400,range:[300,4e3]},topP:{value:1,range:[.01,1]},frequencyPenalty:{value:1,range:[.01,1]}},name:"Mixtral MoE 8x7B Instruct",lastModifiedDate:"2025-02-19"},{id:"fireworks:qwen2.5-coder-32b-instruct",name:"Qwen2.5-Coder 32B Instruct",provider:"fireworks",providerHumanName:"Fireworks",modelApiName:"accounts/fireworks/models/qwen2p5-coder-32b-instruct",makerHumanName:"Fireworks",supportsStructuredOutput:!0,supportsToolCalling:!0,disabled:!0,info:{description:"Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen).",website:"https://x.com/FireworksAI_HQ/status/1856146229690019948",modelUrl:"https://fireworks.ai/models/fireworks/qwen2p5-coder-32b-instruct",contextWindow:128e3,pricing:{pricingUrl:"https://fireworks.ai/pricing",inputCostPerMil:"0.9",outputCostPerMil:"0.9"}},parameters:{temperature:{value:.6,range:[0,2]},maximumLength:{value:4096,range:[0,32768]},topP:{value:1,range:[0,1]},topK:{value:40,range:[1,100]},presencePenalty:{value:0,range:[-2,2]},frequencyPenalty:{value:0,range:[-2,2]}},lastModifiedDate:"2025-04-18"},{id:"fireworks:qwq-32b",name:"QwQ-32B",provider:"fireworks",providerHumanName:"Fireworks",modelApiName:"accounts/fireworks/models/qwq-32b",makerHumanName:"Fireworks",minBillingTier:"pro",supportsStructuredOutput:!0,info:{description:"QwQ-32B is a 32B parameter open-source model that uses Reinforcement Learning to enhance reasoning capabilities for coding and mathematical tasks, achieving performance comparable to much larger models.",website:"https://fireworks.ai",modelUrl:"https://fireworks.ai/models/fireworks/qwq-32b",contextWindow:131072,pricing:{pricingUrl:"https://fireworks.ai/pricing",inputCostPerMil:"0.9",outputCostPerMil:"0.9"}},parameters:{temperature:{value:.6,range:[0,2]},maximumLength:{value:8192,range:[0,16384]},topP:{value:1,range:[0,1]},topK:{value:40,range:[1,100]},presencePenalty:{value:0,range:[-2,2]},frequencyPenalty:{value:0,range:[-2,2]}},lastModifiedDate:"2025-03-05"},{id:"fireworks:qwen3-235b-a22b",name:"Qwen3-235B-A22B",provider:"fireworks",providerHumanName:"Fireworks",modelApiName:"accounts/fireworks/models/qwen3-235b-a22b",makerHumanName:"Fireworks",minBillingTier:"pro",supportsStructuredOutput:!0,info:{description:"Qwen3 235B with 22B active parameter model.",website:"https://fireworks.ai",modelUrl:"https://fireworks.ai/models/fireworks/qwen3-235b-a22b",contextWindow:32768,pricing:{pricingUrl:"https://fireworks.ai/pricing",inputCostPerMil:"0.9",outputCostPerMil:"0.9"}},parameters:{temperature:{value:.6,range:[0,2]},maximumLength:{value:8192,range:[0,16384]},topP:{value:1,range:[0,1]},topK:{value:40,range:[1,100]},presencePenalty:{value:0,range:[-2,2]},frequencyPenalty:{value:0,range:[-2,2]}},lastModifiedDate:"2025-04-29"},{id:"fireworks:kimi-k2-instruct",name:"Kimi K2",provider:"fireworks",providerHumanName:"Fireworks",modelApiName:"accounts/fireworks/models/kimi-k2-instruct",makerHumanName:"Moonshot AI",minBillingTier:"hobby",supportsStructuredOutput:!1,supportsToolCalling:!1,new:!0,gateway:{chef:"moonshotai",model:{primary:"kimi-k2",secondary:["kimi-k2-instruct"]}},info:{description:"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the Muon optimizer, Kimi K2 achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities.",website:"https://fireworks.ai",modelUrl:"https://app.fireworks.ai/models/fireworks/kimi-k2-instruct",contextWindow:131072,pricing:{pricingUrl:"https://fireworks.ai/pricing",inputCostPerMil:"0.6",outputCostPerMil:"2.5"}},parameters:{temperature:{value:.6,range:[0,2]},maximumLength:{value:8192,range:[0,16384]},topP:{value:1,range:[0,1]},topK:{value:40,range:[1,100]},presencePenalty:{value:0,range:[-2,2]},frequencyPenalty:{value:0,range:[-2,2]}},lastModifiedDate:"2025-07-17"},{id:"fireworks:gpt-oss-20b",name:"gpt-oss-20b",provider:"fireworks",providerHumanName:"Fireworks",modelApiName:"accounts/fireworks/models/gpt-oss-20b",makerHumanName:"OpenAI",minBillingTier:"hobby",supportsStructuredOutput:!1,supportsToolCalling:!0,supportsVision:!0,gateway:{chef:"openai",model:{primary:"gpt-oss-20b"}},info:{description:"A compact, open-weight language model optimized for low-latency and resource-constrained environments, including local and edge deployments",website:"https://fireworks.ai",modelUrl:"https://fireworks.ai/models/fireworks/gpt-oss-20b",contextWindow:128e3,pricing:{pricingUrl:"https://fireworks.ai/models/fireworks/gpt-oss-20b",inputCostPerMil:"0.07",outputCostPerMil:"0.3"}},parameters:{maximumLength:{value:4096,range:[0,128e3]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-08-05"},{id:"fireworks:gpt-oss-120b",name:"gpt-oss-120b",provider:"fireworks",providerHumanName:"Fireworks",modelApiName:"accounts/fireworks/models/gpt-oss-120b",makerHumanName:"OpenAI",minBillingTier:"hobby",supportsStructuredOutput:!1,supportsToolCalling:!0,supportsVision:!0,gateway:{chef:"openai",model:{primary:"gpt-oss-120b"}},info:{description:"A high-performance, open-weight language model designed for production-grade, general-purpose use cases.",website:"https://fireworks.ai",modelUrl:"https://fireworks.ai/models/fireworks/gpt-oss-120b",contextWindow:128e3,pricing:{pricingUrl:"https://fireworks.ai/models/fireworks/gpt-oss-120b",inputCostPerMil:"0.15",outputCostPerMil:"0.6"}},parameters:{maximumLength:{value:4096,range:[0,128e3]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-08-05"},{id:"fireworks:kimi-k2-instruct-0905",name:"Kimi K2 0905",provider:"fireworks",providerHumanName:"Fireworks",modelApiName:"accounts/fireworks/models/kimi-k2-instruct-0905",makerHumanName:"Moonshot AI",minBillingTier:"hobby",supportsStructuredOutput:!1,supportsToolCalling:!0,gateway:{chef:"moonshotai",model:{primary:"kimi-k2-0905",secondary:["kimi-k2-instruct-0905"]}},info:{description:"Kimi K2 0905 is an updated version of Kimi K2, a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Kimi K2 0905 has improved coding abilities, a longer context window, and agentic tool use, and a longer (262K) context window.",website:"https://fireworks.ai",modelUrl:"https://app.fireworks.ai/models/fireworks/kimi-k2-instruct-0905",contextWindow:256e3,pricing:{pricingUrl:"https://app.fireworks.ai/models/fireworks/kimi-k2-instruct-0905",inputCostPerMil:"0.6",outputCostPerMil:"1.2"}},parameters:{maximumLength:{value:4096,range:[0,128e3]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-09-04"},{id:"google:gemma-3-27b-it",lastModifiedDate:"2025-03-12",makerHumanName:"Google",supportsVision:!0,supportsToolCalling:!1,supportsStructuredOutput:!1,modelApiName:"gemma-3-27b-it",provider:"google",providerHumanName:"Google",name:"Gemma 3 27B",disableSystemPrompt:!0,disableInGateway:!0,parameters:{maximumLength:{value:32768,range:[1,64e3]},temperature:{value:.7,range:[0,2]},stopSequences:{value:[],range:[]},topP:{value:.4,range:[0,1]}},info:{contextWindow:128e3,description:"Gemma 3 is Google's most advanced open model family that supports multimodal inputs with a 128k token context window and comes in four sizes (1B, 4B, 12B, and 27B parameters). It excels at math, reasoning, coding, and multilingual tasks across 140+ languages.",modelUrl:"https://ai.google.dev/gemma/docs/core",website:"https://developers.googleblog.com/en/introducing-gemma3/",pricing:{pricingUrl:"https://ai.google.dev/pricing"}}},{id:"google:gemini-2.0-flash-001",lastModifiedDate:"2025-10-14",makerHumanName:"Google",supportsVision:!0,supportsToolCalling:!0,supportsStructuredOutput:!0,modelApiName:"gemini-2.0-flash-001",provider:"google",providerHumanName:"Google",name:"Gemini 2.0 Flash",parameters:{maximumLength:{value:8192,range:[1,8192]},temperature:{value:.7,range:[0,2]},stopSequences:{value:[],range:[]},topP:{value:.4,range:[0,1]}},info:{contextWindow:1e6,description:"Gemini 2.0 Flash delivers next-gen features and improved capabilities, including superior speed, native tool use, multimodal generation, and a 1M token context window.",modelUrl:"https://ai.google.dev/gemini-api/docs/models/gemini-v2",website:"https://developers.googleblog.com/en/gemini-2-family-expands",pricing:{pricingUrl:"https://ai.google.dev/pricing",inputCostPerMil:"0.1",outputCostPerMil:"0.4"}}},{id:"google:gemini-2.0-flash-lite-preview-02-05",lastModifiedDate:"2025-02-19",makerHumanName:"Google",supportsVision:!0,supportsToolCalling:!1,supportsStructuredOutput:!1,modelApiName:"gemini-2.0-flash-lite-preview-02-05",provider:"google",providerHumanName:"Google",name:"Gemini 2.0 Flash Lite Preview",disableInGateway:!0,parameters:{maximumLength:{value:8192,range:[1,8192]},temperature:{value:.7,range:[0,2]},stopSequences:{value:[],range:[]},topP:{value:.4,range:[0,1]}},info:{contextWindow:1e6,description:"A Gemini 2.0 Flash model optimized for cost efficiency and low latency.",modelUrl:"https://ai.google.dev/gemini-api/docs/models/gemini-v2",website:"https://developers.googleblog.com/en/gemini-2-family-expands",pricing:{pricingUrl:"https://ai.google.dev/pricing",inputCostPerMil:"0.075",outputCostPerMil:"0.3"}}},{id:"google:gemini-2.0-pro-exp-02-05",lastModifiedDate:"2025-02-05",makerHumanName:"Google",supportsVision:!0,supportsToolCalling:!0,supportsStructuredOutput:!0,modelApiName:"gemini-2.0-pro-exp-02-05",provider:"google",providerHumanName:"Google",name:"gemini-2.0-pro-exp-02-05",new:!1,disabled:!0,parameters:{maximumLength:{value:8192,range:[1,8192]},temperature:{value:.7,range:[0,2]},stopSequences:{value:[],range:[]},topP:{value:.4,range:[0,1]}},info:{contextWindow:2e6,description:"Improved quality, especially for world knowledge, code, and long context",modelUrl:"https://ai.google.dev/gemini-api/docs/models/gemini-v2",website:"https://developers.googleblog.com/en/gemini-2-family-expands",pricing:{pricingUrl:"https://ai.google.dev/pricing"}}},{id:"google:gemini-2.5-pro-exp-03-25",lastModifiedDate:"2025-02-05",makerHumanName:"Google",supportsVision:!0,supportsToolCalling:!0,supportsStructuredOutput:!0,modelApiName:"gemini-2.5-pro-exp-03-25",provider:"google",providerHumanName:"Google",name:"gemini-2.5-pro-exp-03-25",new:!0,disabled:!0,parameters:{maximumLength:{value:8192,range:[1,8192]},temperature:{value:.7,range:[0,2]},stopSequences:{value:[],range:[]},topP:{value:.4,range:[0,1]}},info:{contextWindow:1e6,description:"Gemini 2.5 Pro Experimental is our state-of-the-art thinking model, capable of reasoning over complex problems in code, math, and STEM, as well as analyzing large datasets, codebases, and documents using long context.",modelUrl:"https://ai.google.dev/gemini-api/docs/models",website:"https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/",pricing:{pricingUrl:"https://ai.google.dev/pricing"}}},{id:"google:gemini-2.0-flash-thinking-exp-01-21",modelApiName:"gemini-2.0-flash-thinking-exp-01-21",makerHumanName:"Google",provider:"google",providerHumanName:"Google",name:"gemini-2.0-flash-thinking-exp-01-21",minBillingTier:"pro",supportsVision:!0,supportsToolCalling:!1,supportsStructuredOutput:!1,disabled:!0,parameters:{maximumLength:{value:1024,range:[1,65536]},temperature:{value:.7,range:[0,2]},topP:{value:.95,range:[0,1]},topK:{value:32,range:[1,64]},stopSequences:{value:[],range:[]}},info:{contextWindow:1e6,description:"Gemini 2.0 Flash Thinking is an experimental model trained to expose its reasoning process in responses. By making its thinking process explicit, this model demonstrates enhanced reasoning capabilities compared to other Gemini 2.0 Flash models.",modelUrl:"https://ai.google.dev/gemini-api/docs/thinking",website:"https://deepmind.google/technologies/gemini/flash-thinking",pricing:{pricingUrl:"https://ai.google.dev/pricing"}},lastModifiedDate:"2025-01-22"},{id:"google:gemini-1.5-pro",lastModifiedDate:"2025-02-19",makerHumanName:"Google",modelApiName:"gemini-1.5-pro-001",provider:"google",supportsVision:!0,supportsToolCalling:!0,supportsStructuredOutput:!0,providerHumanName:"Google",name:"Gemini 1.5 Pro 001",minBillingTier:"pro",disabled:!0,parameters:{maximumLength:{value:4096,range:[1,8192]},temperature:{value:.7,range:[0,2]},stopSequences:{value:[],range:[]},topP:{value:.4,range:[0,1]},topK:{value:32,range:[1,64]}},info:{contextWindow:1e6,description:"Gemini 1.5 Pro is the latest model of the Gemini family. It's a mid-size multimodal model that supports up to 1 million tokens and excels at long-context tasks.",modelUrl:"https://deepmind.google/technologies/gemini/#gemini-1.5",website:"https://deepmind.google/technologies/gemini/",pricing:{pricingUrl:"https://ai.google.dev/pricing",inputCostPerMil:"1.25",outputCostPerMil:"5.0"}}},{id:"google:gemini-1.5-flash",lastModifiedDate:"2025-02-19",makerHumanName:"Google",supportsVision:!0,supportsToolCalling:!0,supportsStructuredOutput:!0,modelApiName:"gemini-1.5-flash-001",provider:"google",providerHumanName:"Google",name:"Gemini 1.5 Flash 001",disabled:!0,parameters:{maximumLength:{value:4096,range:[1,8192]},temperature:{value:.7,range:[0,2]},stopSequences:{value:[],range:[]},topP:{value:.4,range:[0,1]},topK:{value:32,range:[1,40]}},info:{contextWindow:1e6,description:"Gemini 1.5 Flash is the latest model of the Gemini family. It's a multimodal model that supports up to 1 million tokens. It is optimized for speed and efficiency.",modelUrl:"https://deepmind.google/technologies/gemini/flash/",website:"https://deepmind.google/technologies/gemini/",pricing:{pricingUrl:"https://ai.google.dev/pricing",inputCostPerMil:"0.075",outputCostPerMil:"0.3"}}},{id:"google:gemini-1.5-pro-002",lastModifiedDate:"2025-02-19",makerHumanName:"Google",modelApiName:"gemini-1.5-pro-002",provider:"google",supportsVision:!0,supportsToolCalling:!0,supportsStructuredOutput:!0,providerHumanName:"Google",name:"Gemini 1.5 Pro 002",new:!1,minBillingTier:"pro",disableInGateway:!0,parameters:{maximumLength:{value:4096,range:[1,8192]},temperature:{value:.7,range:[0,2]},stopSequences:{value:[],range:[]},topP:{value:.4,range:[0,1]},topK:{value:32,range:[1,64]}},info:{contextWindow:1e6,description:"Gemini 1.5 Pro is the latest model of the Gemini family. It's a mid-size multimodal model that supports up to 1 million tokens and excels at long-context tasks.",modelUrl:"https://deepmind.google/technologies/gemini/#gemini-1.5",website:"https://deepmind.google/technologies/gemini/",pricing:{pricingUrl:"https://ai.google.dev/pricing",inputCostPerMil:"1.25",outputCostPerMil:"5.0"}}},{id:"google:gemini-1.5-flash-002",lastModifiedDate:"2025-02-19",makerHumanName:"Google",supportsVision:!0,supportsToolCalling:!0,supportsStructuredOutput:!0,modelApiName:"gemini-1.5-flash-002",provider:"google",providerHumanName:"Google",name:"Gemini 1.5 Flash 002",new:!1,disableInGateway:!0,parameters:{maximumLength:{value:4096,range:[1,8192]},temperature:{value:.7,range:[0,2]},stopSequences:{value:[],range:[]},topP:{value:.4,range:[0,1]},topK:{value:32,range:[1,40]}},info:{contextWindow:1e6,description:"Gemini 1.5 Flash is the latest model of the Gemini family. It's a multimodal model that supports up to 1 million tokens. It is optimized for speed and efficiency.",modelUrl:"https://deepmind.google/technologies/gemini/flash/",website:"https://deepmind.google/technologies/gemini/",pricing:{pricingUrl:"https://ai.google.dev/pricing",inputCostPerMil:"0.075",outputCostPerMil:"0.3"}}},{id:"google:gemini-1.5-flash-8b",lastModifiedDate:"2025-02-19",makerHumanName:"Google",supportsVision:!0,supportsToolCalling:!1,supportsStructuredOutput:!1,modelApiName:"gemini-1.5-flash-8b",provider:"google",providerHumanName:"Google",name:"Gemini 1.5 Flash 8b",new:!1,disableInGateway:!0,parameters:{maximumLength:{value:4096,range:[1,8192]},temperature:{value:.7,range:[0,2]},stopSequences:{value:[],range:[]},topP:{value:.4,range:[0,1]}},info:{contextWindow:1e6,description:"Gemini 1.5 Flash 8b is the latest model of the Gemini family. It's a multimodal model that supports up to 1 million tokens. It is optimized for speed and cost-efficiency.",modelUrl:"https://deepmind.google/technologies/gemini/flash/",website:"https://deepmind.google/technologies/gemini/",pricing:{pricingUrl:"https://ai.google.dev/pricing",inputCostPerMil:"0.0375",outputCostPerMil:"0.15"}}},{id:"google:gemini-exp-1206",lastModifiedDate:"2025-02-19",makerHumanName:"Google",supportsVision:!0,supportsToolCalling:!0,supportsStructuredOutput:!0,modelApiName:"gemini-exp-1206",provider:"google",providerHumanName:"Google",name:"gemini-exp-1206",new:!1,disabled:!0,parameters:{maximumLength:{value:4096,range:[1,8192]},temperature:{value:.7,range:[0,2]},stopSequences:{value:[],range:[]},topP:{value:.4,range:[0,1]}},info:{contextWindow:1e6,description:"In addition to the base models, the Gemini API offers experimental models available in Preview. This model has qualitative improvements and celebrates 1 year of Gemini's launch.",modelUrl:"https://ai.google.dev/gemini-api/docs/models/experimental-models",website:"https://deepmind.google/technologies/gemini",pricing:{pricingUrl:"https://ai.google.dev/pricing"}}},{id:"google:gemini-exp-1121",lastModifiedDate:"2025-02-19",makerHumanName:"Google",supportsVision:!0,supportsToolCalling:!0,supportsStructuredOutput:!0,modelApiName:"gemini-exp-1121",provider:"google",providerHumanName:"Google",name:"gemini-exp-1121",new:!1,disabled:!0,parameters:{maximumLength:{value:4096,range:[1,8192]},temperature:{value:.7,range:[0,2]},stopSequences:{value:[],range:[]},topP:{value:.4,range:[0,1]}},info:{contextWindow:1e6,description:"In addition to the base models, the Gemini API offers experimental models available in Preview. This model has improved coding, reasoning, and vision capabilities.",modelUrl:"https://ai.google.dev/gemini-api/docs/models/experimental-models",website:"https://deepmind.google/technologies/gemini",pricing:{pricingUrl:"https://ai.google.dev/pricing"}}},{id:"google:gemini-2.0-flash-exp",lastModifiedDate:"2025-02-19",makerHumanName:"Google",supportsVision:!0,supportsToolCalling:!0,supportsStructuredOutput:!0,modelApiName:"gemini-2.0-flash-exp",provider:"google",providerHumanName:"Google",name:"gemini-2.0-flash-exp",new:!1,disabled:!0,parameters:{maximumLength:{value:8192,range:[1,8192]},temperature:{value:.7,range:[0,2]},stopSequences:{value:[],range:[]},topP:{value:.4,range:[0,1]}},info:{contextWindow:1e6,description:"Gemini 2.0 Flash Experimental delivers next-gen features and improved capabilities, including superior speed, native tool use, multimodal generation, and a 1M token context window.",modelUrl:"https://ai.google.dev/gemini-api/docs/models/gemini-v2",website:"https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024",pricing:{pricingUrl:"https://ai.google.dev/pricing"}}},{id:"google:gemini-2.5-pro-preview-03-25",lastModifiedDate:"2025-04-14",makerHumanName:"Google",supportsVision:!0,supportsToolCalling:!0,supportsStructuredOutput:!0,modelApiName:"gemini-2.5-pro-preview-03-25",provider:"google",providerHumanName:"Google",name:"Gemini 2.5 Pro Preview",new:!0,disableInGateway:!0,parameters:{maximumLength:{value:8192,range:[1,8192]},temperature:{value:.7,range:[0,2]},stopSequences:{value:[],range:[]},topP:{value:.4,range:[0,1]}},info:{contextWindow:1e6,description:"Gemini 2.5 Pro Experimental is our state-of-the-art thinking model, capable of reasoning over complex problems in code, math, and STEM, as well as analyzing large datasets, codebases, and documents using long context.",modelUrl:"https://ai.google.dev/gemini-api/docs/models",website:"https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/",pricing:{pricingUrl:"https://ai.google.dev/pricing"}}},{id:"google:gemini-2.5-flash-preview-04-17",lastModifiedDate:"2025-04-17",makerHumanName:"Google",supportsVision:!0,supportsToolCalling:!0,supportsStructuredOutput:!0,modelApiName:"gemini-2.5-flash-preview-04-17",provider:"google",providerHumanName:"Google",name:"Gemini 2.5 Flash Preview",new:!0,disableInGateway:!0,parameters:{maximumLength:{value:32768,range:[1,64e3]},temperature:{value:.7,range:[0,2]},stopSequences:{value:[],range:[]},topP:{value:.4,range:[0,1]}},info:{contextWindow:1e6,description:"Gemini 2.5 Flash is our first fully hybrid reasoning model, giving developers the ability to turn thinking on or off. The model also allows developers to set thinking budgets to find the right tradeoff between quality, cost, and latency.",modelUrl:"https://ai.google.dev/gemini-api/docs/models",website:"https://developers.googleblog.com/en/start-building-with-gemini-25-flash",pricing:{pricingUrl:"https://ai.google.dev/pricing",inputCostPerMil:"0.15",outputCostPerMil:"0.6"}}},{id:"google:gemini-2.5-flash-preview-09-2025",provider:"google",providerHumanName:"Google",modelApiName:"gemini-2.5-flash-preview-09-2025",makerHumanName:"Google",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,minBillingTier:"hobby",new:!0,gateway:{chef:"google",model:{primary:"gemini-2.5-flash-preview-09-2025"}},info:{description:"Gemini 2.5 Flash is a thinking model that offers great, well-rounded capabilities. It is designed to offer a balance between price and performance with multimodal support and a 1M token context window.",website:"https://developers.googleblog.com/en/continuing-to-bring-you-our-latest-models-with-an-improved-gemini-2-5-flash-and-flash-lite-release/",modelUrl:"https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview",contextWindow:1e6,pricing:{pricingUrl:"https://ai.google.dev/gemini-api/docs/pricing",inputCostPerMil:"0.3",outputCostPerMil:"2.5"}},parameters:{temperature:{value:1,range:[0,2]},maximumLength:{value:4096,range:[0,65536]},topP:{value:.95,range:[0,1]}},name:"Gemini 2.5 Flash Preview 09-2025",lastModifiedDate:"2025-09-25"},{id:"google:gemini-2.5-flash-lite-preview-09-2025",provider:"google",providerHumanName:"Google",modelApiName:"gemini-2.5-flash-lite-preview-09-2025",makerHumanName:"Google",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,minBillingTier:"hobby",new:!0,gateway:{chef:"google",model:{primary:"gemini-2.5-flash-lite-preview-09-2025"}},info:{description:"Gemini 2.5 Flash-Lite is a balanced, low-latency model with configurable thinking budgets and tool connectivity (e.g., Google Search grounding and code execution). It supports multimodal input and offers a 1M-token context window.",website:"https://developers.googleblog.com/en/continuing-to-bring-you-our-latest-models-with-an-improved-gemini-2-5-flash-and-flash-lite-release/",modelUrl:"https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-lite-preview",contextWindow:1048576,pricing:{pricingUrl:"https://ai.google.dev/gemini-api/docs/pricing",inputCostPerMil:"0.1",outputCostPerMil:"0.4"}},parameters:{temperature:{value:1,range:[0,2]},maximumLength:{value:4096,range:[0,65536]},topP:{value:.95,range:[0,1]}},name:"Gemini 2.5 Flash Lite Preview 09-2025",lastModifiedDate:"2025-09-25"},{id:"google:gemini-2.0-flash-lite-001",lastModifiedDate:"2025-10-14",makerHumanName:"Google",supportsVision:!0,supportsToolCalling:!0,supportsStructuredOutput:!0,modelApiName:"gemini-2.0-flash-lite-001",provider:"google",providerHumanName:"Google",name:"Gemini 2.0 Flash Lite",gateway:{chef:"google",model:{primary:"gemini-2.0-flash-lite",secondary:["gemini-2.0-flash-lite-001"]}},parameters:{maximumLength:{value:8192,range:[1,8192]},temperature:{value:.7,range:[0,2]},stopSequences:{value:[],range:[]},topP:{value:.4,range:[0,1]}},info:{contextWindow:1048576,description:"A Gemini 2.0 Flash model optimized for cost efficiency and low latency.",modelUrl:"https://ai.google.dev/gemini-api/docs/models#gemini-2.0-flash-lite",website:"https://developers.googleblog.com/en/gemini-2-family-expands",pricing:{pricingUrl:"https://ai.google.dev/pricing",inputCostPerMil:"0.075",outputCostPerMil:"0.3"}}},{id:"google:gemini-2.5-pro",lastModifiedDate:"2025-10-14",makerHumanName:"Google",supportsVision:!0,supportsToolCalling:!0,supportsStructuredOutput:!0,modelApiName:"gemini-2.5-pro",provider:"google",providerHumanName:"Google",name:"Gemini 2.5 Pro",new:!0,gateway:{chef:"google",model:{primary:"gemini-2.5-pro"}},parameters:{maximumLength:{value:8192,range:[1,8192]},temperature:{value:.7,range:[0,2]},stopSequences:{value:[],range:[]},topP:{value:.4,range:[0,1]}},info:{contextWindow:1048576,description:"Gemini 2.5 Pro is our most advanced reasoning Gemini model, capable of solving complex problems. Gemini 2.5 Pro can comprehend vast datasets and challenging problems from different information sources, including text, audio, images, video, and even entire code repositories.",modelUrl:"https://ai.google.dev/gemini-api/docs/models#gemini-2.5-pro",website:"https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/",pricing:{pricingUrl:"https://ai.google.dev/pricing",inputCostPerMil:"1.25",outputCostPerMil:"10.0"}}},{id:"google:gemini-2.5-flash",lastModifiedDate:"2025-10-14",makerHumanName:"Google",supportsVision:!0,supportsToolCalling:!0,supportsStructuredOutput:!0,modelApiName:"gemini-2.5-flash",provider:"google",providerHumanName:"Google",name:"Gemini 2.5 Flash",new:!0,gateway:{chef:"google",model:{primary:"gemini-2.5-flash"}},parameters:{maximumLength:{value:32768,range:[1,64e3]},temperature:{value:.7,range:[0,2]},stopSequences:{value:[],range:[]},topP:{value:.4,range:[0,1]}},info:{contextWindow:1e6,description:"Gemini 2.5 Flash is a thinking model that offers great, well-rounded capabilities. It is designed to offer a balance between price and performance with multimodal support and a 1M token context window.",modelUrl:"https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash",website:"https://developers.googleblog.com/en/start-building-with-gemini-25-flash",pricing:{pricingUrl:"https://ai.google.dev/pricing",inputCostPerMil:"0.3",outputCostPerMil:"2.5"}}},{id:"google:gemini-2.5-flash-lite",provider:"google",providerHumanName:"Google",modelApiName:"gemini-2.5-flash-lite",makerHumanName:"Google",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,minBillingTier:"hobby",new:!0,gateway:{chef:"google",model:{primary:"gemini-2.5-flash-lite"}},info:{description:"Gemini 2.5 Flash-Lite is a balanced, low-latency model with configurable thinking budgets and tool connectivity (e.g., Google Search grounding and code execution). It supports multimodal input and offers a 1M-token context window.",website:"https://developers.googleblog.com/en/continuing-to-bring-you-our-latest-models-with-an-improved-gemini-2-5-flash-and-flash-lite-release/",modelUrl:"https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-lite",contextWindow:1048576,pricing:{pricingUrl:"https://ai.google.dev/gemini-api/docs/pricing",inputCostPerMil:"0.1",outputCostPerMil:"0.4"}},parameters:{temperature:{value:1,range:[0,2]},maximumLength:{value:4096,range:[0,65536]},topP:{value:.95,range:[0,1]}},name:"Gemini 2.5 Flash Lite",lastModifiedDate:"2025-10-14"},{id:"groq:llama2-70b-4096",lastModifiedDate:"2024-01-01",disabled:!0,provider:"groq",providerHumanName:"Groq",modelApiName:"llama2-70b-4096",makerHumanName:"Meta",info:{description:"70 billion parameter open source model by Meta fine-tuned for chat purposes served by Groq. Groq uses custom Language Processing Units (LPUs) hardware to provide fast and efficient inference.",website:"https://ai.meta.com/llama/",modelUrl:"https://x.com/MistralAI/status/1733150512395038967?s=20",contextWindow:4096,pricing:{pricingUrl:"https://wow.groq.com/",inputCostPerMil:"0.7",outputCostPerMil:"0.8"}},parameters:{temperature:{value:.2,range:[.01,5]},maximumLength:{value:400,range:[300,4e3]},topP:{value:.8,range:[.01,1]},topK:{value:40,range:[1,500]}},name:"llama-2-70b-chat-groq"},{id:"groq:llama-3.2-1b",lastModifiedDate:"2025-02-19",name:"Llama 3.2 1B",modelApiName:"llama-3.2-1b-preview",provider:"groq",providerHumanName:"Groq",minBillingTier:"pro",makerHumanName:"Groq",supportsStructuredOutput:!1,supportsToolCalling:!1,disabled:!0,info:{description:"The Llama 3.2, 1 billion parameter multi-lingual text only model is made by Meta. It is lightweight and can be run everywhere on mobile and on edge devices.  Served by Groq with their custom Language Processing Units (LPUs) hardware to provide fast and efficient inference.",website:"https://groq.com",modelUrl:"https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2",contextWindow:128e3,pricing:{pricingUrl:"https://wow.groq.com/",inputCostPerMil:"0.04",outputCostPerMil:"0.04"}},parameters:{temperature:{value:.5,range:[.01,5]},maximumLength:{value:1e3,range:[300,8192]},topP:{value:1,range:[.01,1]},frequencyPenalty:{value:1,range:[.01,1]}}},{id:"groq:llama-3.2-3b",lastModifiedDate:"2025-02-19",name:"Llama 3.2 3B",modelApiName:"llama-3.2-3b-preview",provider:"groq",providerHumanName:"Groq",minBillingTier:"pro",makerHumanName:"Groq",supportsStructuredOutput:!1,supportsToolCalling:!1,disabled:!0,info:{description:"The Llama 3.2, 3 billion parameter multi-lingual text only model is made by Meta. It is lightweight and can be run everywhere on mobile and on edge devices. Served by Groq with their custom Language Processing Units (LPUs) hardware to provide fast and efficient inference.",website:"https://groq.com",modelUrl:"https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2",contextWindow:128e3,pricing:{pricingUrl:"https://wow.groq.com/",inputCostPerMil:"0.06",outputCostPerMil:"0.06"}},parameters:{temperature:{value:.5,range:[.01,5]},maximumLength:{value:1e3,range:[300,8192]},topP:{value:1,range:[.01,1]},frequencyPenalty:{value:1,range:[.01,1]}}},{id:"groq:llama-3.2-11b",lastModifiedDate:"2024-03-21",name:"llama-3.2-11b-groq",modelApiName:"llama-3.2-11b-text-preview",provider:"groq",providerHumanName:"Groq",minBillingTier:"pro",makerHumanName:"Groq",disabled:!0,supportsStructuredOutput:!1,supportsToolCalling:!1,info:{description:"The Llama 3.2, 11 billion parameter multi-modal model is made by Meta and served by Groq on their LPU hardware. It is flexible and can reason on high resolution images.",website:"https://groq.com",modelUrl:"https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2",contextWindow:128e3,pricing:{pricingUrl:"https://wow.groq.com/",inputCostPerMil:"0.7",outputCostPerMil:"0.8"}},parameters:{temperature:{value:.5,range:[.01,5]},maximumLength:{value:1e3,range:[300,8192]},topP:{value:1,range:[.01,1]},frequencyPenalty:{value:1,range:[.01,1]}}},{id:"groq:llama-3.2-11b-vision-preview",lastModifiedDate:"2025-02-19",name:"Llama 3.2 11B",modelApiName:"llama-3.2-11b-vision-preview",provider:"groq",providerHumanName:"Groq",minBillingTier:"pro",makerHumanName:"Groq",supportsVision:!0,supportsStructuredOutput:!1,supportsToolCalling:!1,disableInGateway:!0,info:{description:"The Llama 3.2, 11 billion parameter multi-modal model is made by Meta and served by Groq on their LPU hardware. It is flexible and can reason on high resolution images.",website:"https://groq.com",modelUrl:"https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2",contextWindow:128e3,pricing:{pricingUrl:"https://wow.groq.com/"}},parameters:{temperature:{value:.5,range:[.01,5]},maximumLength:{value:1e3,range:[300,8192]},topP:{value:1,range:[.01,1]},frequencyPenalty:{value:1,range:[.01,1]}}},{id:"groq:llama-3.2-90b",lastModifiedDate:"2024-03-21",name:"llama-3.2-90b-groq",modelApiName:"llama-3.2-90b-text-preview",provider:"groq",providerHumanName:"Groq",minBillingTier:"pro",makerHumanName:"Groq",disabled:!0,supportsStructuredOutput:!1,supportsToolCalling:!1,info:{description:"The Llama 3.2, 90 billion parameter multi-modal model is made by Meta and served by Groq on their LPU hardware. It is flexible and can reason on high resolution images.",website:"https://groq.com",modelUrl:"https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2",contextWindow:128e3,pricing:{pricingUrl:"https://wow.groq.com/",inputCostPerMil:"0.7",outputCostPerMil:"0.8"}},parameters:{temperature:{value:.5,range:[.01,5]},maximumLength:{value:1e3,range:[300,8192]},topP:{value:1,range:[.01,1]},frequencyPenalty:{value:1,range:[.01,1]}}},{id:"groq:llama-3.2-90b-vision-preview",lastModifiedDate:"2025-02-19",name:"Llama 3.2 90B",modelApiName:"llama-3.2-90b-vision-preview",provider:"groq",providerHumanName:"Groq",minBillingTier:"pro",makerHumanName:"Groq",supportsVision:!0,supportsStructuredOutput:!1,supportsToolCalling:!1,disabled:!0,info:{description:"The Llama 3.2, 90 billion parameter multi-modal model is made by Meta and served by Groq on their LPU hardware. It is flexible and can reason on high resolution images.",website:"https://groq.com",modelUrl:"https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2",contextWindow:128e3,pricing:{pricingUrl:"https://wow.groq.com/"}},parameters:{temperature:{value:.5,range:[.01,5]},maximumLength:{value:1e3,range:[300,8192]},topP:{value:1,range:[.01,1]},frequencyPenalty:{value:1,range:[.01,1]}}},{id:"groq:llama-3.3-70b-versatile",lastModifiedDate:"2025-03-18",name:"Llama 3.3 70B Versatile",modelApiName:"llama-3.3-70b-versatile",provider:"groq",providerHumanName:"Groq",minBillingTier:"pro",makerHumanName:"Groq",supportsStructuredOutput:!1,supportsToolCalling:!1,info:{description:"The Meta Llama 3.3 multilingual model is a pretrained and instruction tuned generative model with 70B parameters. Optimized for multilingual dialogue use cases, it  outperforms many of the available open source and closed chat models on common industry benchmarks. Served by Groq with their custom Language Processing Units (LPUs) hardware to provide fast and efficient inference.",website:"https://groq.com",modelUrl:"https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md",contextWindow:128e3,pricing:{pricingUrl:"https://wow.groq.com/",inputCostPerMil:"0.59",outputCostPerMil:"0.79"}},parameters:{temperature:{value:.5,range:[.01,5]},maximumLength:{value:8192,range:[0,16384]},topP:{value:1,range:[.01,1]},frequencyPenalty:{value:1,range:[.01,1]}}},{id:"groq:llama-3.1-405b",lastModifiedDate:"2024-03-21",modelApiName:"llama-3.1-405b-reasoning",provider:"groq",providerHumanName:"Groq",minBillingTier:"pro",makerHumanName:"Meta",disabled:!0,supportsStructuredOutput:!1,supportsToolCalling:!1,info:{description:"Llama is a 405 billion parameter open source model by Meta fine-tuned for instruction following purposes served by Groq on their LPU hardware.",website:"https://groq.com",modelUrl:"https://console.groq.com/playground?model=llama-3.1-405b-reasoning",contextWindow:16e3,pricing:{pricingUrl:"https://wow.groq.com/",inputCostPerMil:"0.7",outputCostPerMil:"0.8"}},parameters:{temperature:{value:.5,range:[.01,5]},maximumLength:{value:1e3,range:[300,8192]},topP:{value:1,range:[.01,1]},frequencyPenalty:{value:1,range:[.01,1]}},name:"llama-3.1-405b"},{id:"groq:llama-3.1-70b",lastModifiedDate:"2025-02-19",modelApiName:"llama-3.1-70b-versatile",provider:"groq",providerHumanName:"Groq",minBillingTier:"hobby",makerHumanName:"Meta",supportsStructuredOutput:!1,supportsToolCalling:!1,disabled:!0,info:{description:"Llama is a 70 billion parameter open source model by Meta fine-tuned for instruction following purposes served by Groq on their LPU hardware.",website:"https://groq.com",modelUrl:"https://console.groq.com/playground?model=llama-3.1-70b-reasoning",contextWindow:16e3,pricing:{pricingUrl:"https://wow.groq.com/",inputCostPerMil:"0.7",outputCostPerMil:"0.8"}},parameters:{temperature:{value:.5,range:[.01,5]},maximumLength:{value:1e3,range:[300,8192]},topP:{value:1,range:[.01,1]},frequencyPenalty:{value:1,range:[.01,1]}},name:"llama-3.1-70b"},{id:"groq:llama-3.1-8b",lastModifiedDate:"2025-02-19",modelApiName:"llama-3.1-8b-instant",provider:"groq",providerHumanName:"Groq",makerHumanName:"Groq",supportsStructuredOutput:!1,supportsToolCalling:!1,info:{description:"Llama 3.1 8B with 128K context window support, making it ideal for real-time conversational interfaces and data analysis while offering significant cost savings compared to larger models. Served by Groq with their custom Language Processing Units (LPUs) hardware to provide fast and efficient inference.",website:"https://groq.com",modelUrl:"https://console.groq.com/playground?model=llama-3.1-8b-reasoning",contextWindow:128e3,pricing:{pricingUrl:"https://wow.groq.com/",inputCostPerMil:"0.05",outputCostPerMil:"0.08"}},parameters:{temperature:{value:.5,range:[.01,5]},maximumLength:{value:4e3,range:[0,8e3]},topP:{value:1,range:[.01,1]},frequencyPenalty:{value:1,range:[.01,1]}},name:"Llama 3.1 8B Instant"},{id:"groq:llama-3-8b-instruct",lastModifiedDate:"2025-02-19",modelApiName:"llama3-8b-8192",provider:"groq",providerHumanName:"Groq",makerHumanName:"Groq",supportsStructuredOutput:!1,supportsToolCalling:!1,info:{description:"Llama is a 8 billion parameter open source model by Meta fine-tuned for instruction following purposes. Served by Groq with their custom Language Processing Units (LPUs) hardware to provide fast and efficient inference.",website:"https://groq.com",modelUrl:"https://console.groq.com/playground?model=llama3-8b-8192",contextWindow:8192,pricing:{pricingUrl:"https://wow.groq.com/",inputCostPerMil:"0.05",outputCostPerMil:"0.08"}},parameters:{temperature:{value:.5,range:[.01,5]},maximumLength:{value:1e3,range:[300,8192]},topP:{value:1,range:[.01,1]},frequencyPenalty:{value:1,range:[.01,1]}},name:"Llama 3 8B Instruct"},{id:"groq:llama-3-70b-instruct",lastModifiedDate:"2025-02-19",modelApiName:"llama3-70b-8192",provider:"groq",providerHumanName:"Groq",makerHumanName:"Groq",supportsStructuredOutput:!1,supportsToolCalling:!1,info:{description:"Llama is a 70 billion parameter open source model by Meta fine-tuned for instruction following purposes. Served by Groq with their custom Language Processing Units (LPUs) hardware to provide fast and efficient inference.",website:"https://groq.com",modelUrl:"https://console.groq.com/playground?model=llama3-70b-8192",contextWindow:8192,pricing:{pricingUrl:"https://wow.groq.com/",inputCostPerMil:"0.59",outputCostPerMil:"0.79"}},parameters:{temperature:{value:.5,range:[.01,5]},maximumLength:{value:1e3,range:[300,8192]},topP:{value:1,range:[.01,1]},frequencyPenalty:{value:1,range:[.01,1]}},name:"Llama 3 70B Instruct"},{id:"groq:mixtral-8x7b-32768",lastModifiedDate:"2025-03-21",provider:"groq",providerHumanName:"Groq",makerHumanName:"Groq",modelApiName:"mixtral-8x7b-32768",supportsStructuredOutput:!1,supportsToolCalling:!1,disabled:!0,info:{description:"Mistral MoE LLM model with 8 experts, each 7B. Warning: unofficial implementation. Served by Groq with their custom Language Processing Units (LPUs) hardware to provide fast and efficient inference.",website:"https://mistral.ai/",modelUrl:"https://x.com/MistralAI/status/1733150512395038967?s=20",contextWindow:21845,pricing:{pricingUrl:"https://wow.groq.com/",inputCostPerMil:"0.24",outputCostPerMil:"0.24"}},parameters:{temperature:{value:.2,range:[.01,5]},maximumLength:{value:400,range:[300,4e3]},topP:{value:.8,range:[.01,1]},topK:{value:40,range:[1,500]}},name:"mixtral-8x7b-groq"},{id:"groq:gemma-7b-it",lastModifiedDate:"2025-02-24",provider:"groq",providerHumanName:"Groq",modelApiName:"gemma-7b-it",makerHumanName:"Google",supportsStructuredOutput:!1,supportsToolCalling:!1,disabled:!0,info:{description:"7 billion parameter open source model by Google fine-tuned for chat purposes served by Groq. Groq uses custom Language Processing Units (LPUs) hardware to provide fast and efficient inference.",website:"https://blog.google/technology/developers/google-gemma-2/",modelUrl:"https://blog.google/technology/developers/google-gemma-2/",contextWindow:8192,pricing:{pricingUrl:"https://wow.groq.com/"}},parameters:{temperature:{value:.2,range:[.01,5]},maximumLength:{value:1e3,range:[300,4e3]},topP:{value:.8,range:[.01,1]},topK:{value:40,range:[1,500]}},name:"gemma-7b-it"},{id:"groq:gemma2-9b-it",lastModifiedDate:"2025-02-19",provider:"groq",providerHumanName:"Groq",modelApiName:"gemma2-9b-it",makerHumanName:"Groq",supportsStructuredOutput:!1,supportsToolCalling:!1,info:{description:"9 billion parameter open source model by Google fine-tuned for chat purposes. Served by Groq with their custom Language Processing Units (LPUs) hardware to provide fast and efficient inference.",website:"https://blog.google/technology/developers/google-gemma-2/",modelUrl:"https://blog.google/technology/developers/google-gemma-2/",contextWindow:8192,pricing:{pricingUrl:"https://wow.groq.com/",inputCostPerMil:"0.2",outputCostPerMil:"0.2"}},parameters:{temperature:{value:.2,range:[.01,5]},maximumLength:{value:1e3,range:[300,4e3]},topP:{value:.8,range:[.01,1]},topK:{value:40,range:[1,500]}},name:"Gemma 2 9B IT"},{id:"groq:deepseek-r1-distill-llama-70b",lastModifiedDate:"2025-02-19",provider:"groq",providerHumanName:"Groq",modelApiName:"deepseek-r1-distill-llama-70b",makerHumanName:"Groq",minBillingTier:"pro",info:{description:"DeepSeek-R1-Distill-Llama-70B is a distilled, more efficient variant of the 70B Llama model. It preserves strong performance across text-generation tasks, reducing computational overhead for easier deployment and research. Served by Groq with their custom Language Processing Units (LPUs) hardware to provide fast and efficient inference.",website:"https://groq.com",modelUrl:"https://console.groq.com/playground?model=deepseek-r1-distill-llama-70b",contextWindow:131072,pricing:{pricingUrl:"https://wow.groq.com/",inputCostPerMil:"0.75",outputCostPerMil:"0.99"}},parameters:{temperature:{value:.6,range:[.01,2]},maximumLength:{value:4096,range:[0,131072]},topP:{value:.95,range:[.01,1]},topK:{value:40,range:[1,500]}},name:"DeepSeek R1 Distill Llama 70B"},{id:"groq:mistral-saba-24b",lastModifiedDate:"2025-03-18",provider:"groq",providerHumanName:"Groq",modelApiName:"mistral-saba-24b",makerHumanName:"Groq",disabled:!0,info:{description:"Mistral Saba 24B is a 24 billion parameter open source model by Mistral.ai. Saba is a specialized model trained to excel in Arabic, Farsi, Urdu, Hebrew, and Indic languages. Served by Groq with their custom Language Processing Units (LPUs) hardware to provide fast and efficient inference.",website:"https://groq.com",modelUrl:"https://console.groq.com/docs/model/mistral-saba-24b",contextWindow:32768,pricing:{pricingUrl:"https://wow.groq.com/",inputCostPerMil:"0.79",outputCostPerMil:"0.79"}},parameters:{temperature:{value:.6,range:[.01,2]},maximumLength:{value:16384,range:[0,32768]},topP:{value:.95,range:[.01,1]},topK:{value:40,range:[1,500]}},name:"Mistral Saba 24B"},{id:"groq:qwen-qwq-32b",lastModifiedDate:"2025-03-18",provider:"groq",providerHumanName:"Groq",modelApiName:"qwen-qwq-32b",makerHumanName:"Groq",info:{description:"Qwen QWQ-32B is a powerful large language model with strong reasoning capabilities and versatile applications across various tasks. Served by Groq with their custom Language Processing Units (LPUs) hardware to provide fast and efficient inference.",website:"https://groq.com",modelUrl:"https://console.groq.com/docs/model/qwen-qwq-32b",contextWindow:32768,pricing:{pricingUrl:"https://wow.groq.com/",inputCostPerMil:"0.29",outputCostPerMil:"0.39"}},parameters:{temperature:{value:.6,range:[.01,2]},maximumLength:{value:16384,range:[0,32768]},topP:{value:.95,range:[.01,1]},topK:{value:40,range:[1,500]}},name:"QWQ-32B"},{id:"groq:llama-4-scout-17b-16e-instruct",lastModifiedDate:"2025-04-05",name:"Llama 4 Scout 17B 16E Instruct",modelApiName:"meta-llama/llama-4-scout-17b-16e-instruct",provider:"groq",new:!0,providerHumanName:"Groq",minBillingTier:"pro",makerHumanName:"Groq",supportsStructuredOutput:!1,supportsToolCalling:!0,supportsVision:!0,info:{description:"Llama 4 Scout is Meta's natively multimodal model with a 17B parameter mixture-of-experts architecture (16 experts), offering exceptional performance across text and image understanding with support for 12 languages, optimized for assistant-like chat, image recognition, and coding tasks. Served by Groq with their custom Language Processing Units (LPUs) hardware to provide fast and efficient inference.",website:"https://groq.com",modelUrl:"https://console.groq.com/docs/model/llama-4-scout-17b-16e-instruct",contextWindow:131072,pricing:{pricingUrl:"https://wow.groq.com/",inputCostPerMil:"0.11",outputCostPerMil:"0.34"}},parameters:{temperature:{value:.5,range:[.01,5]},maximumLength:{value:4096,range:[0,8192]},topP:{value:1,range:[.01,1]},frequencyPenalty:{value:1,range:[.01,1]}}},{id:"groq:kimi-k2-instruct",lastModifiedDate:"2025-07-14",name:"Kimi K2",modelApiName:"moonshotai/kimi-k2-instruct",provider:"groq",new:!0,providerHumanName:"Groq",minBillingTier:"hobby",makerHumanName:"Moonshot AI",supportsStructuredOutput:!1,supportsToolCalling:!0,supportsVision:!0,gateway:{chef:"moonshotai",model:{primary:"kimi-k2",secondary:["kimi-k2-instruct"]}},info:{description:"Kimi K2 is a model with a context length of 128k, featuring powerful code and Agent capabilities based on MoE architecture. It has 1T total parameters with 32B activated parameters. In benchmark performance tests across major categories including general knowledge reasoning, programming, mathematics, and Agent capabilities, the K2 model outperforms other mainstream open-source models.",website:"https://groq.com",modelUrl:"https://console.groq.com/docs/model/moonshotai/kimi-k2-instruct",contextWindow:131072,pricing:{pricingUrl:"https://wow.groq.com/",inputCostPerMil:"1",outputCostPerMil:"3"}},parameters:{temperature:{value:.5,range:[.01,5]},maximumLength:{value:4096,range:[0,131072]},topP:{value:1,range:[.01,1]},frequencyPenalty:{value:1,range:[.01,1]}}},{id:"groq:gpt-oss-120b",lastModifiedDate:"2025-08-04",name:"gpt-oss-120b",modelApiName:"openai/gpt-oss-120b",provider:"groq",new:!0,providerHumanName:"Groq",minBillingTier:"hobby",makerHumanName:"OpenAI",supportsStructuredOutput:!1,supportsToolCalling:!0,supportsVision:!0,gateway:{chef:"openai",model:{primary:"gpt-oss-120b"}},info:{description:"For production, general purpose, high reasoning use cases that fits into a single H100 GPU (117B parameters with 5.1B active parameters).",website:"https://groq.com",modelUrl:"https://console.groq.com/docs/models",contextWindow:131072,pricing:{pricingUrl:"https://wow.groq.com/",inputCostPerMil:"0.15",outputCostPerMil:"0.75"}},parameters:{temperature:{value:.5,range:[.01,5]},maximumLength:{value:4096,range:[0,32768]},topP:{value:1,range:[.01,1]},frequencyPenalty:{value:1,range:[.01,1]}}},{id:"groq:gpt-oss-20b",lastModifiedDate:"2025-08-04",name:"gpt-oss-20b",modelApiName:"openai/gpt-oss-20b",provider:"groq",new:!0,providerHumanName:"Groq",minBillingTier:"hobby",makerHumanName:"OpenAI",supportsStructuredOutput:!1,supportsToolCalling:!0,supportsVision:!0,gateway:{chef:"openai",model:{primary:"gpt-oss-20b"}},info:{description:"For lower latency, and local or specialized use cases (21B parameters with 3.6B active parameters).",website:"https://groq.com",modelUrl:"https://console.groq.com/docs/models",contextWindow:131072,pricing:{pricingUrl:"https://wow.groq.com/",inputCostPerMil:"0.1",outputCostPerMil:"0.5"}},parameters:{temperature:{value:.5,range:[.01,5]},maximumLength:{value:4096,range:[0,32768]},topP:{value:1,range:[.01,1]},frequencyPenalty:{value:1,range:[.01,1]}}},{id:"groq:kimi-k2-instruct-0905",lastModifiedDate:"2025-09-04",name:"Kimi K2 0905",modelApiName:"moonshotai/kimi-k2-instruct-0905",provider:"groq",new:!0,providerHumanName:"Groq",minBillingTier:"pro",makerHumanName:"Moonshot AI",supportsStructuredOutput:!1,supportsToolCalling:!0,gateway:{chef:"moonshotai",model:{primary:"kimi-k2-0905",secondary:["kimi-k2-instruct-0905"]}},info:{description:"Kimi K2 0905 is Moonshot AI's improved version of the Kimi K2 model, featuring enhanced coding capabilities with superior frontend development and tool calling performance. This Mixture-of-Experts (MoE) model with 1 trillion total parameters and 32 billion activated parameters offers improved integration with various agent scaffolds, making it ideal for building sophisticated AI agents and autonomous systems.",website:"https://groq.com",modelUrl:"https://console.groq.com/docs/model/moonshotai/kimi-k2-instruct-0905",contextWindow:262144,pricing:{pricingUrl:"https://wow.groq.com/pricing",inputCostPerMil:"1",outputCostPerMil:"3"}},parameters:{temperature:{value:.5,range:[.01,5]},maximumLength:{value:4096,range:[0,16384]},topP:{value:1,range:[.01,1]},frequencyPenalty:{value:1,range:[.01,1]}}},{id:"inception:mercury-coder-small",modelApiName:"mercury-coder-small",provider:"inception",providerHumanName:"Inception",makerHumanName:"Inception",name:"Mercury Coder Small Beta",supportsVision:!1,minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,disabled:!1,info:{description:"Mercury Coder Small is ideal for code generation, debugging, and refactoring tasks with minimal latency.",modelUrl:"https://platform.inceptionlabs.ai/docs#models",website:"https://platform.inceptionlabs.ai",contextWindow:32e3,pricing:{inputCostPerMil:"0.25",outputCostPerMil:"1",pricingUrl:"https://platform.inceptionlabs.ai/docs#models"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,16384]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-05-02"},{id:"meituan:longcat-flash-chat",name:"LongCat Flash Chat",provider:"meituan",providerHumanName:"Meituan LongCat",makerHumanName:"Meituan",modelApiName:"LongCat-Flash-Chat",minBillingTier:"hobby",supportsStructuredOutput:!1,supportsToolCalling:!0,gateway:{chef:"meituan",model:{primary:"longcat-flash-chat",secondary:["LongCat-Flash-Chat"]}},info:{description:"LongCat-Flash-Chat is a high-throughput MoE chat model (128k context) optimized for agentic tasks.",website:"https://longcat.ai",modelUrl:"https://huggingface.co/meituan-longcat/LongCat-Flash-Chat",contextWindow:128e3,pricing:{pricingUrl:"https://longcat.ai"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:1024,range:[0,1e5]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-09-09"},{id:"mistral:mistral-large",lastModifiedDate:"2025-02-19",provider:"mistral",providerHumanName:"Mistral",makerHumanName:"Mistral",modelApiName:"mistral-large-latest",minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,new:!1,info:{description:"Mistral Large is ideal for complex tasks that require large reasoning capabilities or are highly specialized - like Synthetic Text Generation, Code Generation, RAG, or Agents.",website:"https://mistral.ai/",modelUrl:"https://mistral.ai/news/mistral-large/",contextWindow:32e3,pricing:{pricingUrl:"https://docs.mistral.ai/platform/pricing/",inputCostPerMil:"2",outputCostPerMil:"6"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:1024,range:[0,4e3]},topP:{value:1,range:[0,1]}},name:"Mistral Large"},{id:"mistral:mistral-medium",lastModifiedDate:"2024-01-15",provider:"mistral",providerHumanName:"Mistral",makerHumanName:"Mistral",modelApiName:"mistral-medium-latest",minBillingTier:"pro",disabled:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"Mistral Medium is the ideal for intermediate tasks that require moderate reasoning - like Data extraction, Summarizing a Document, Writing a Job Description, or Writing Product Descriptions. Mistral Medium strikes a balance between performance and capability, making it suitable for a wide range of tasks that only require language transformation.",website:"https://mistral.ai/",modelUrl:"https://mistral.ai/technology/",contextWindow:32e3,pricing:{pricingUrl:"https://docs.mistral.ai/platform/pricing/",inputCostPerMil:"2.7",outputCostPerMil:"8.1"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:1024,range:[0,4e3]},topP:{value:1,range:[0,1]}},name:"Mistral Medium"},{id:"mistral:mistral-small",lastModifiedDate:"2025-02-19",provider:"mistral",providerHumanName:"Mistral",makerHumanName:"Mistral",modelApiName:"mistral-small-latest",name:"Mistral Small",minBillingTier:"hobby",new:!1,supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"Mistral Small is the ideal choice for simple tasks that one can do in bulk - like Classification, Customer Support, or Text Generation. It offers excellent performance at an affordable price point.",website:"https://mistral.ai/",modelUrl:"https://mistral.ai/technology/",contextWindow:32e3,pricing:{pricingUrl:"https://docs.mistral.ai/platform/pricing/",inputCostPerMil:"0.1",outputCostPerMil:"0.3"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:1024,range:[0,4e3]},topP:{value:1,range:[0,1]}}},{id:"mistral:codestral-2501",lastModifiedDate:"2025-02-19",name:"Mistral Codestral 25.01",provider:"mistral",providerHumanName:"Mistral",makerHumanName:"Mistral",modelApiName:"codestral-2501",minBillingTier:"pro",disabled:!1,supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"Mistral Codestral 25.01 is a state-of-the-art coding model optimized for low-latency, high-frequency use cases. Proficient in over 80 programming languages, it excels at tasks like fill-in-the-middle (FIM), code correction, and test generation.",website:"https://mistral.ai/",modelUrl:"https://mistral.ai/technology/",contextWindow:256e3,pricing:{pricingUrl:"https://docs.mistral.ai/platform/pricing/",inputCostPerMil:"0.3",outputCostPerMil:"0.9"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:1024,range:[0,4e3]},topP:{value:1,range:[0,1]}}},{id:"mistral:codestral-latest",lastModifiedDate:"2024-02-01",name:"mistral-codestral",provider:"mistral",providerHumanName:"Mistral",makerHumanName:"Mistral",modelApiName:"codestral-latest",disabled:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"Mistral Codestral 22B is an open-weight generative AI model explicitly designed for code generation tasks. It helps developers write and interact with code through a shared instruction and completion API endpoint. As it masters code and English, it can be used to design advanced AI applications for software developers.",website:"https://mistral.ai/",modelUrl:"https://mistral.ai/technology/",contextWindow:32e3,pricing:{pricingUrl:"https://docs.mistral.ai/platform/pricing/",inputCostPerMil:"1",outputCostPerMil:"3"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:1024,range:[0,4e3]},topP:{value:1,range:[0,1]}}},{id:"mistral:codestral-mamba-latest",lastModifiedDate:"2024-02-15",name:"mistral-codestral-mamba",provider:"mistral",providerHumanName:"Mistral",makerHumanName:"Mistral",modelApiName:"codestral-mamba-latest",disabled:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"Mistral Codestral Mamba is an open-weight Mamba 2 language model specialized in code generation. It helps developers write and interact with code through a shared instruction and completion API endpoint. As it masters code and English, it can be used to design advanced AI applications for software developers.",website:"https://mistral.ai/",modelUrl:"https://mistral.ai/news/codestral-mamba/",contextWindow:256e3,pricing:{pricingUrl:"https://docs.mistral.ai/platform/pricing/",inputCostPerMil:"1",outputCostPerMil:"3"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:1024,range:[0,4e3]},topP:{value:1,range:[0,1]}}},{id:"mistral:mistral-large-2",lastModifiedDate:"2024-02-24",name:"mistral-large-2",new:!1,provider:"mistral",providerHumanName:"Mistral",makerHumanName:"Mistral",modelApiName:"mistral-large-2407",minBillingTier:"pro",disabled:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"Mistral Large 2 is significantly more capable in code generation, mathematics, and reasoning. It also provides a much stronger multilingual support, and advanced function calling capabilities.",website:"https://mistral.ai/",modelUrl:"https://mistral.ai/news/mistral-large-2407/",contextWindow:128e3,pricing:{pricingUrl:"https://docs.mistral.ai/platform/pricing/",inputCostPerMil:"3",outputCostPerMil:"9"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:1024,range:[0,4e3]},topP:{value:1,range:[0,1]}}},{id:"mistral:pixtral-12b-2409",lastModifiedDate:"2025-02-19",name:"Pixtral 12B 2409",new:!1,provider:"mistral",providerHumanName:"Mistral",makerHumanName:"Mistral",modelApiName:"pixtral-12b-2409",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,minBillingTier:"pro",info:{description:"A 12B model with image understanding capabilities in addition to text.",website:"https://mistral.ai/",modelUrl:"https://mistral.ai/news/pixtral-12b/",contextWindow:128e3,pricing:{pricingUrl:"https://docs.mistral.ai/platform/pricing/",inputCostPerMil:"0.15",outputCostPerMil:"0.15"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:1024,range:[0,4e3]},topP:{value:1,range:[0,1]}}},{id:"mistral:ministral-3b-latest",lastModifiedDate:"2025-02-19",name:"Ministral 3B",new:!1,provider:"mistral",providerHumanName:"Mistral",makerHumanName:"Mistral",modelApiName:"ministral-3b-latest",supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"A compact, efficient model for on-device tasks like smart assistants and local analytics, offering low-latency performance.",website:"https://mistral.ai/",modelUrl:"https://mistral.ai/news/ministraux/",contextWindow:128e3,pricing:{pricingUrl:"https://docs.mistral.ai/platform/pricing/",inputCostPerMil:"0.04",outputCostPerMil:"0.04"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:1024,range:[0,4e3]},topP:{value:1,range:[0,1]}}},{id:"mistral:ministral-8b-latest",lastModifiedDate:"2025-02-19",name:"Ministral 8B",new:!1,provider:"mistral",providerHumanName:"Mistral",makerHumanName:"Mistral",modelApiName:"ministral-8b-latest",supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"A more powerful model with faster, memory-efficient inference, ideal for complex workflows and demanding edge applications.",website:"https://mistral.ai/",modelUrl:"https://mistral.ai/news/ministraux/",contextWindow:128e3,pricing:{pricingUrl:"https://docs.mistral.ai/platform/pricing/",inputCostPerMil:"0.1",outputCostPerMil:"0.1"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:1024,range:[0,4e3]},topP:{value:1,range:[0,1]}}},{id:"mistral:pixtral-large-latest",lastModifiedDate:"2025-02-19",name:"Pixtral Large",new:!1,provider:"mistral",providerHumanName:"Mistral",makerHumanName:"Mistral",modelApiName:"pixtral-large-latest",supportsVision:!0,minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"Pixtral Large is the second model in our multimodal family and demonstrates frontier-level image understanding. Particularly, the model is able to understand documents, charts and natural images, while maintaining the leading text-only understanding of Mistral Large 2.",website:"https://mistral.ai/",modelUrl:"https://mistral.ai/news/pixtral-large/",contextWindow:128e3,pricing:{pricingUrl:"https://docs.mistral.ai/platform/pricing/",inputCostPerMil:"2",outputCostPerMil:"6"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:1024,range:[0,4e3]},topP:{value:1,range:[0,1]}}},{id:"mistral:mistral-small-2501",lastModifiedDate:"2025-03-17",name:"mistral-small-2501",disabled:!0,minBillingTier:"hobby",provider:"mistral",providerHumanName:"Mistral",makerHumanName:"Mistral",modelApiName:"mistral-small-2501",supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"Mistral Small 3 is a latency-optimized 24B-parameter model that excels in fast-response conversational assistance, low-latency function calling, and can be fine-tuned to create subject matter experts. With over 81% accuracy on MMLU and 150 tokens/s latency, it's competitive with larger models while being more than 3x faster.",website:"https://mistral.ai/",modelUrl:"https://mistral.ai/news/mistral-small-3/",contextWindow:32e3,pricing:{pricingUrl:"https://docs.mistral.ai/platform/pricing/",inputCostPerMil:"0.1",outputCostPerMil:"0.3"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:1024,range:[0,4e3]},topP:{value:1,range:[0,1]}}},{id:"mistral:mistral-small-2503",lastModifiedDate:"2025-03-17",name:"Mistral Small 2503",minBillingTier:"hobby",provider:"mistral",providerHumanName:"Mistral",makerHumanName:"Mistral",modelApiName:"mistral-small-2503",supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"Mistral Small 3.1 is a state-of-the-art multimodal and multilingual model with excellent benchmark performance while delivering 150 tokens per second inference speeds and supporting up to 128k context window.",website:"https://mistral.ai/",modelUrl:"https://mistral.ai/news/mistral-small-3-1/",contextWindow:128e3,pricing:{pricingUrl:"https://docs.mistral.ai/platform/pricing/",inputCostPerMil:"0.1",outputCostPerMil:"0.3"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:32768,range:[0,64e3]},topP:{value:1,range:[0,1]}}},{id:"mistral:magistral-small-2506",lastModifiedDate:"2025-06-11",name:"Magistral Small 2506",minBillingTier:"hobby",provider:"mistral",providerHumanName:"Mistral",makerHumanName:"Mistral",modelApiName:"magistral-small-2506",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"mistral",model:{primary:"magistral-small-2506"}},info:{description:"Complex thinking, backed by deep understanding, with transparent reasoning you can follow and verify. The model excels in maintaining high-fidelity reasoning across numerous languages, even when switching between languages mid-task.",website:"https://mistral.ai/",modelUrl:"https://mistral.ai/news/magistral",contextWindow:128e3,pricing:{pricingUrl:"https://docs.mistral.ai/platform/pricing/",inputCostPerMil:"0.5",outputCostPerMil:"1.5"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:32768,range:[0,64e3]},topP:{value:1,range:[0,1]}}},{id:"mistral:magistral-medium-2506",lastModifiedDate:"2025-06-11",name:"Magistral Medium 2506",minBillingTier:"hobby",provider:"mistral",providerHumanName:"Mistral",makerHumanName:"Mistral",modelApiName:"magistral-medium-2506",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"mistral",model:{primary:"magistral-medium-2506"}},info:{description:"Complex thinking, backed by deep understanding, with transparent reasoning you can follow and verify. The model excels in maintaining high-fidelity reasoning across numerous languages, even when switching between languages mid-task.",website:"https://mistral.ai/",modelUrl:"https://mistral.ai/news/mistral-small-3-1/",contextWindow:128e3,pricing:{pricingUrl:"https://docs.mistral.ai/platform/pricing/",inputCostPerMil:"2",outputCostPerMil:"5"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:32768,range:[0,64e3]},topP:{value:1,range:[0,1]}}},{id:"mistral:devstral-small-2507",lastModifiedDate:"2025-07-24",name:"Devstral Small",minBillingTier:"hobby",provider:"mistral",providerHumanName:"Mistral",makerHumanName:"Mistral",modelApiName:"devstral-small-2507",supportsStructuredOutput:!0,supportsToolCalling:!0,new:!0,gateway:{chef:"mistral",model:{primary:"devstral-small",secondary:["devstral-small-2507"]}},info:{description:"Devstral is an agentic LLM for software engineering tasks built under a collaboration between Mistral AI and All Hands AI \uD83D\uDE4C. Devstral excels at using tools to explore codebases, editing multiple files and power software engineering agents.",website:"https://mistral.ai/",modelUrl:"https://mistral.ai/news/devstral-2507",contextWindow:128e3,pricing:{pricingUrl:"https://docs.mistral.ai/platform/pricing/",inputCostPerMil:"0.1",outputCostPerMil:"0.3"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:32768,range:[0,64e3]},topP:{value:1,range:[0,1]}}},{id:"mistral:mistral-medium-2508",lastModifiedDate:"2025-08-23",name:"Mistral Medium 3.1",minBillingTier:"hobby",provider:"mistral",providerHumanName:"Mistral",makerHumanName:"Mistral",modelApiName:"mistral-medium-2508",supportsStructuredOutput:!0,supportsToolCalling:!0,new:!0,gateway:{chef:"mistral",model:{primary:"mistral-medium",secondary:["mistral-medium-2508"]}},info:{description:"Mistral Medium 3 delivers frontier performance while being an order of magnitude less expensive. For instance, the model performs at or above 90% of Claude Sonnet 3.7 on benchmarks across the board at a significantly lower cost.",website:"https://mistral.ai/",modelUrl:"https://mistral.ai/news/mistral-medium-3",contextWindow:128e3,pricing:{pricingUrl:"https://docs.mistral.ai/platform/pricing/",inputCostPerMil:"0.4",outputCostPerMil:"2"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:32768,range:[0,64e3]},topP:{value:1,range:[0,1]}}},{id:"mistral:magistral-small-2509",lastModifiedDate:"2025-06-11",name:"Magistral Small 2509",minBillingTier:"hobby",provider:"mistral",providerHumanName:"Mistral",makerHumanName:"Mistral",modelApiName:"magistral-small-2509",supportsStructuredOutput:!0,supportsToolCalling:!0,supportsVision:!0,new:!0,gateway:{chef:"mistral",model:{primary:"magistral-small",secondary:["magistral-small-2509"]}},info:{description:"Complex thinking, backed by deep understanding, with transparent reasoning you can follow and verify. The model excels in maintaining high-fidelity reasoning across numerous languages, even when switching between languages mid-task.",website:"https://mistral.ai/",modelUrl:"https://mistral.ai/news/magistral",contextWindow:128e3,pricing:{pricingUrl:"https://docs.mistral.ai/platform/pricing/",inputCostPerMil:"0.5",outputCostPerMil:"1.5"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:32768,range:[0,64e3]},topP:{value:1,range:[0,1]}}},{id:"mistral:magistral-medium-2509",lastModifiedDate:"2025-09-18",name:"Magistral Medium 2509",minBillingTier:"hobby",provider:"mistral",providerHumanName:"Mistral",makerHumanName:"Mistral",modelApiName:"magistral-medium-2509",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,new:!0,gateway:{chef:"mistral",model:{primary:"magistral-medium",secondary:["magistral-medium-2509"]}},info:{description:"Complex thinking, backed by deep understanding, with transparent reasoning you can follow and verify. The model excels in maintaining high-fidelity reasoning across numerous languages, even when switching between languages mid-task.",website:"https://mistral.ai/",modelUrl:"https://mistral.ai/news/mistral-small-3-1/",contextWindow:128e3,pricing:{pricingUrl:"https://docs.mistral.ai/platform/pricing/",inputCostPerMil:"2",outputCostPerMil:"5"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:32768,range:[0,64e3]},topP:{value:1,range:[0,1]}}},{id:"moonshotai:kimi-k2-0905-preview",name:"Kimi K2 0905",provider:"moonshotai",providerHumanName:"Moonshot AI",makerHumanName:"Moonshot AI",modelApiName:"kimi-k2-0905-preview",minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"moonshotai",model:{primary:"kimi-k2-0905",secondary:["kimi-k2-0905-preview"]}},info:{description:"Kimi K2 is a model with a context length of 256k, featuring stronger Agentic Coding capabilities, more prominent front-end code aesthetics and practicality, and better context understanding capabilities based on the capabilities of kimi-k2-0711-preview.",website:"https://www.moonshot.ai",modelUrl:"https://moonshotai.github.io/Kimi-K2/",contextWindow:256e3,pricing:{pricingUrl:"https://platform.moonshot.ai/docs/pricing/chat",inputCostPerMil:"0.6",outputCostPerMil:"2.5"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:8192,range:[0,16384]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-09-04"},{id:"moonshotai:kimi-k2-turbo-preview",name:"Kimi K2 Turbo",provider:"moonshotai",providerHumanName:"Moonshot AI",makerHumanName:"Moonshot AI",modelApiName:"kimi-k2-turbo-preview",minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"moonshotai",model:{primary:"kimi-k2-turbo",secondary:["kimi-k2-turbo-preview"]}},info:{description:"Kimi K2 Turbo is the high-speed version of kimi-k2, with the same model parameters as kimi-k2, but the output speed is increased to 60 tokens per second, with a maximum of 100 tokens per second, the context length is 256k",website:"https://www.moonshot.ai",modelUrl:"https://moonshotai.github.io/Kimi-K2/",contextWindow:256e3,pricing:{pricingUrl:"https://platform.moonshot.ai/docs/pricing/chat",inputCostPerMil:"2.4",outputCostPerMil:"10"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:8192,range:[0,16384]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-09-04"},{id:"moonshotai:kimi-k2-0711-preview",name:"Kimi K2",provider:"moonshotai",providerHumanName:"Moonshot AI",makerHumanName:"Moonshot AI",modelApiName:"kimi-k2",minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,new:!0,gateway:{chef:"moonshotai",model:{primary:"kimi-k2",secondary:["kimi-k2-0711-preview"]}},info:{description:"Kimi K2 is a model with a context length of 128k, featuring powerful code and Agent capabilities based on MoE architecture. It has 1T total parameters with 32B activated parameters. In benchmark performance tests across major categories including general knowledge reasoning, programming, mathematics, and Agent capabilities, the K2 model outperforms other mainstream open-source models.",website:"https://www.moonshot.ai",modelUrl:"https://moonshotai.github.io/Kimi-K2/",contextWindow:131072,pricing:{pricingUrl:"https://platform.moonshot.ai/docs/pricing/chat",inputCostPerMil:"0.6",outputCostPerMil:"2.5"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,16384]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-07-12"},{id:"morph:morph-v2",name:"Morph V2",provider:"morph",providerHumanName:"Morph",makerHumanName:"Morph",modelApiName:"morph-v2",minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,disabled:!0,info:{description:"Morph offers a specialized AI model that applies code changes suggested by frontier models (like Claude or GPT-4o) to your existing code files FAST - 1600 tokens/second. It is designed to seamlessly apply code changes suggested by frontier AI models (like Claude or GPT-4o) to your existing code files. It acts as the final step in the AI coding workflow.",website:"https://morphllm.com/",modelUrl:"https://morphllm.com/blog/what-is-morph-for",contextWindow:5e4,pricing:{pricingUrl:"https://morphllm.com/dashboard",inputCostPerMil:"0.9",outputCostPerMil:"1.9"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,16384]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-06-01"},{id:"morph:morph-v3-fast",name:"Morph V3 Fast",provider:"morph",providerHumanName:"Morph",makerHumanName:"Morph",modelApiName:"morph-v3-fast",minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"morph",model:{primary:"morph-v3-fast"}},info:{description:"Morph offers a specialized AI model that applies code changes suggested by frontier models (like Claude or GPT-4o) to your existing code files FAST - 4500+ tokens/second. It acts as the final step in the AI coding workflow. Supports 16k input tokens and 16k output tokens.",website:"https://morphllm.com/",modelUrl:"https://morphllm.com/blog/what-is-morph-for",contextWindow:81920,pricing:{pricingUrl:"https://morphllm.com/dashboard",inputCostPerMil:"0.8",outputCostPerMil:"1.2"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,16384]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-08-8"},{id:"morph:morph-v3-large",name:"Morph V3 Large",provider:"morph",providerHumanName:"Morph",makerHumanName:"Morph",modelApiName:"morph-v3-large",minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"morph",model:{primary:"morph-v3-large"}},info:{description:"Morph offers a specialized AI model that applies code changes suggested by frontier models (like Claude or GPT-4o) to your existing code files FAST - 2500+ tokens/second. It acts as the final step in the AI coding workflow. Supports 16k input tokens and 16k output tokens.",website:"https://morphllm.com/",modelUrl:"https://morphllm.com/blog/what-is-morph-for",contextWindow:81920,pricing:{pricingUrl:"https://morphllm.com/dashboard",inputCostPerMil:"0.9",outputCostPerMil:"1.9"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,16384]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-08-08"},{id:"novita:deepseek-v3-0324",modelApiName:"deepseek/deepseek-v3-0324",provider:"novita",providerHumanName:"Novita AI",makerHumanName:"DeepSeek",name:"DeepSeek V3 0324",supportsVision:!1,minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"deepseek",model:{primary:"deepseek-v3",secondary:["deepseek-v3-0324"]}},info:{description:"DeepSeek V3, a 685B-parameter, mixture-of-experts model, is the latest iteration of the flagship chat model family from the DeepSeek team.",modelUrl:"https://novita.ai/models",website:"https://novita.ai",contextWindow:163840,pricing:{inputCostPerMil:"0.28",outputCostPerMil:"1.14",pricingUrl:"https://novita.ai/pricing"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,163840]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-08-03"},{id:"novita:deepseek-v3.1",modelApiName:"deepseek/deepseek-v3-0324",provider:"novita",providerHumanName:"Novita AI",makerHumanName:"DeepSeek",name:"DeepSeek V3.1",supportsVision:!1,minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"deepseek",model:{primary:"deepseek-v3.1"}},info:{description:"DeepSeek-V3.1 is post-trained on the top of DeepSeek-V3.1-Base, which is built upon the original V3 base checkpoint through a two-phase long context extension approach, following the methodology outlined in the original DeepSeek-V3 report. DeepSeek has expanded their dataset by collecting additional long documents and substantially extending both training phases.",modelUrl:"https://novita.ai/models",website:"https://novita.ai",contextWindow:163840,pricing:{inputCostPerMil:"0.55",outputCostPerMil:"1.66",pricingUrl:"https://novita.ai/pricing"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,163840]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-08-21"},{id:"novita:qwen3-235b-a22b-thinking-2507",modelApiName:"qwen/qwen3-235b-a22b-thinking-2507",provider:"novita",providerHumanName:"Novita AI",makerHumanName:"Alibaba",name:"Qwen3 235B A22b Thinking 2507",supportsVision:!1,minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"alibaba",model:{primary:"qwen-3-235b",secondary:["qwen-3-235b-a22b","qwen-3-235b-a22b-thinking-2507"]}},info:{description:"The Qwen3-235B-A22B-Thinking-2507 represents the newest thinking-enabled model in the Qwen3 series, delivering groundbreaking improvements in reasoning capabilities. This advanced AI demonstrates significantly enhanced performance across logical reasoning, mathematics, scientific analysis, coding tasks, and academic benchmarks.",modelUrl:"https://novita.ai/models",website:"https://novita.ai",contextWindow:131072,pricing:{inputCostPerMil:"0.3",outputCostPerMil:"3",pricingUrl:"https://novita.ai/pricing"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,131072]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-08-03"},{id:"novita:kimi-k2-instruct",modelApiName:"moonshotai/kimi-k2-instruct",provider:"novita",providerHumanName:"Novita AI",makerHumanName:"Moonshot AI",name:"Kimi K2 Instruct",supportsVision:!1,minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"moonshotai",model:{primary:"kimi-k2",secondary:["kimi-k2-instruct"]}},info:{description:"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the Muon optimizer, Kimi K2 achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities.",modelUrl:"https://novita.ai/models",website:"https://novita.ai",contextWindow:131072,pricing:{inputCostPerMil:"0.57",outputCostPerMil:"2.3",pricingUrl:"https://novita.ai/pricing"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,131072]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-08-03"},{id:"novita:qwen3-coder-480b-a35b-instruct",modelApiName:"qwen/qwen3-coder-480b-a35b-instruct",provider:"novita",providerHumanName:"Novita AI",makerHumanName:"Alibaba",name:"Qwen3 Coder 480B A35B Instruct",supportsVision:!1,minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"alibaba",model:{primary:"qwen3-coder",secondary:["qwen3-coder-480b","qwen3-coder-480b-a35b-instruct"]}},info:{description:"Qwen3-Coder-480B-A35B-Instruct is a cutting-edge open coding model from Qwen, matching Claude Sonnet’s performance in agentic programming, browser automation, and core development tasks.",modelUrl:"https://novita.ai/models",website:"https://novita.ai",contextWindow:262144,pricing:{inputCostPerMil:"0.64",outputCostPerMil:"2.5",pricingUrl:"https://novita.ai/pricing"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,65536]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-08-03"},{id:"novita:glm-4.5",modelApiName:"zai-org/glm-4.5",provider:"novita",providerHumanName:"Novita AI",makerHumanName:"Z.ai",name:"GLM-4.5",supportsVision:!1,minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"zai",model:{primary:"glm-4.5"}},info:{description:"GLM-4.5 Series Models are foundation models specifically engineered for intelligent agents. The flagship GLM-4.5 integrates 355 billion total parameters (32 billion active), unifying reasoning, coding, and agent capabilities to address complex application demands. As a hybrid reasoning system, it offers dual operational modes.",modelUrl:"https://novita.ai/models",website:"https://novita.ai",contextWindow:131072,pricing:{inputCostPerMil:"0.6",outputCostPerMil:"2.2",pricingUrl:"https://novita.ai/pricing"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,131072]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-08-03"},{id:"novita:glm-4.5v",modelApiName:"zai-org/glm-4.5v",provider:"novita",providerHumanName:"Novita AI",makerHumanName:"Z.ai",name:"GLM 4.5V",supportsVision:!0,minBillingTier:"hobby",supportsStructuredOutput:!1,supportsToolCalling:!0,gateway:{chef:"zai",model:{primary:"glm-4.5v"}},info:{description:"Built on the GLM-4.5-Air base model, GLM-4.5V inherits proven techniques from GLM-4.1V-Thinking while achieving effective scaling through a powerful 106B-parameter MoE architecture.",modelUrl:"https://novita.ai/models",website:"https://novita.ai",contextWindow:65536,pricing:{inputCostPerMil:"0.6",outputCostPerMil:"1.8",pricingUrl:"https://novita.ai/pricing"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,66e3]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-08-11"},{id:"novita:qwen3-next-80b-a3b-instruct",modelApiName:"qwen/qwen3-next-80b-a3b-instruct",provider:"novita",providerHumanName:"Novita AI",makerHumanName:"Alibaba Cloud",name:"Qwen3 Next 80B A3B Instruct",supportsVision:!0,minBillingTier:"hobby",supportsStructuredOutput:!1,supportsToolCalling:!0,gateway:{chef:"alibaba",model:{primary:"qwen3-next-80b-a3b-instruct"}},info:{description:"Qwen3-Next uses a highly sparse MoE design: 80B total parameters, but only ~3B activated per inference step. Experiments show that, with global load balancing, increasing total expert parameters while keeping activated experts fixed steadily reduces training loss.Compared to Qwen3’s MoE (128 total experts, 8 routed), Qwen3-Next expands to 512 total experts, combining 10 routed experts + 1 shared expert — maximizing resource usage without hurting performance. The Qwen3-Next-80B-A3B-Instruct performs comparably to our flagship model Qwen3-235B-A22B-Instruct-2507, and shows clear advantages in tasks requiring ultra-long context (up to 256K tokens).",modelUrl:"https://novita.ai/models-console/model-detail/qwen-qwen3-next-80b-a3b-instruct",website:"https://novita.ai",contextWindow:65536,pricing:{inputCostPerMil:"0.15",outputCostPerMil:"1.5",pricingUrl:"https://novita.ai/pricing"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,65536]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-09-12"},{id:"novita:qwen3-next-80b-a3b-thinking",modelApiName:"qwen/qwen3-next-80b-a3b-thinking",provider:"novita",providerHumanName:"Novita AI",makerHumanName:"Alibaba Cloud",name:"Qwen3 Next 80B A3B Thinking",supportsVision:!0,minBillingTier:"hobby",supportsStructuredOutput:!1,supportsToolCalling:!0,gateway:{chef:"alibaba",model:{primary:"qwen3-next-80b-a3b-thinking"}},info:{description:"Qwen3-Next uses a highly sparse MoE design: 80B total parameters, but only ~3B activated per inference step. Experiments show that, with global load balancing, increasing total expert parameters while keeping activated experts fixed steadily reduces training loss.Compared to Qwen3’s MoE (128 total experts, 8 routed), Qwen3-Next expands to 512 total experts, combining 10 routed experts + 1 shared expert — maximizing resource usage without hurting performance. The Qwen3-Next-80B-A3B-Thinking excels at complex reasoning tasks — outperforming higher-cost models like Qwen3-30B-A3B-Thinking-2507 and Qwen3-32B-Thinking, outpeforming the closed-source Gemini-2.5-Flash-Thinking on multiple benchmarks, and approaching the performance of our top-tier model Qwen3-235B-A22B-Thinking-2507.",modelUrl:"https://novita.ai/models-console/model-detail/qwen-qwen3-next-80b-a3b-thinking",website:"https://novita.ai",contextWindow:65536,pricing:{inputCostPerMil:"0.15",outputCostPerMil:"1.5",pricingUrl:"https://novita.ai/pricing"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,65536]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-09-12"},{id:"novita:deepseek-v3.1-terminus",modelApiName:"deepseek/deepseek-v3.1-terminus",provider:"novita",providerHumanName:"Novita AI",makerHumanName:"DeepSeek",name:"DeepSeek V3.1 Terminus",supportsVision:!1,minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"deepseek",model:{primary:"deepseek-v3.1-terminus"}},info:{description:"DeepSeek-V3.1-Terminus delivers more stable & reliable outputs across benchmarks compared to the previous version and addresses user feedback (i.e. language consistency and agent upgrades).",modelUrl:"https://novita.ai/models/model-detail/deepseek-deepseek-v3.1-terminus",website:"https://novita.ai",contextWindow:131072,pricing:{inputCostPerMil:"0.27",outputCostPerMil:"1",pricingUrl:"https://novita.ai/pricing"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,65536]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-09-22"},{id:"novita:deepseek-v3.2-exp",modelApiName:"deepseek/deepseek-v3.2-exp",provider:"novita",providerHumanName:"Novita AI",makerHumanName:"DeepSeek",name:"DeepSeek V3.2 Exp",supportsVision:!1,minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"deepseek",model:{primary:"deepseek-v3.2-exp"}},info:{description:"DeepSeek-V3.2-Exp is an experimental model introducing the groundbreaking DeepSeek Sparse Attention (DSA) mechanism for enhanced long-context processing efficiency. Built on V3.1-Terminus, DSA achieves fine-grained sparse attention while maintaining identical output quality.",modelUrl:"https://novita.ai/models/model-detail/deepseek-deepseek-v3.2-exp",website:"https://novita.ai",contextWindow:163840,pricing:{inputCostPerMil:"0.27",outputCostPerMil:"0.41",pricingUrl:"https://novita.ai/pricing"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,65536]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-09-29"},{id:"openai:o1-preview",modelApiName:"o1-preview",provider:"openai",providerHumanName:"OpenAI",makerHumanName:"OpenAI",name:"o1 Preview",supportsVision:!1,minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,disabled:!0,info:{description:"Reasoning model designed to solve hard problems across domains – the o1 series of large language models are trained with reinforcement learning to perform complex reasoning. o1 models think before they answer, producing a long internal chain of thought before responding to the user.",modelUrl:"https://platform.openai.com/docs/models/o1",website:"https://openai.com",contextWindow:128e3,pricing:{inputCostPerMil:"15",outputCostPerMil:"60",pricingUrl:"https://platform.openai.com/docs/pricing"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:1024,range:[50,2048]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2024-03-14"},{id:"openai:o1-mini",modelApiName:"o1-mini",provider:"openai",providerHumanName:"OpenAI",makerHumanName:"OpenAI",name:"o1-mini",supportsVision:!1,minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,disabled:!0,info:{description:"Faster and cheaper reasoning model particularly good at coding, math, and science – the o1 series of large language models are trained with reinforcement learning to perform complex reasoning. o1 models think before they answer, producing a long internal chain of thought before responding to the user.",modelUrl:"https://platform.openai.com/docs/models/o1",website:"https://openai.com",contextWindow:128e3,pricing:{inputCostPerMil:"3",outputCostPerMil:"12",pricingUrl:"https://platform.openai.com/docs/pricing"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:1024,range:[50,2048]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2024-03-14"},{id:"openai:o3-mini",modelApiName:"o3-mini",provider:"openai",providerHumanName:"OpenAI",makerHumanName:"OpenAI",name:"o3-mini",supportsVision:!1,minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"o3-mini is OpenAI's most recent small reasoning model, providing high intelligence at the same cost and latency targets of o1-mini.",modelUrl:"https://platform.openai.com/docs/models/o3-mini",website:"https://openai.com",contextWindow:2e5,pricing:{inputCostPerMil:"1.1",outputCostPerMil:"4.4",pricingUrl:"https://platform.openai.com/docs/pricing",cachedInputCostPerMil:"0.55",cacheCreationInputCostPerMil:"0"}},parameters:{maximumLength:{value:4096,range:[50,8192]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-02-19"},{id:"openai:o3-mini-low",modelApiName:"o3-mini",provider:"openai",providerHumanName:"OpenAI",makerHumanName:"OpenAI",name:"o3-mini (Low)",supportsVision:!1,minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,disableInGateway:!0,info:{description:"o3-mini with low reasoning effort - optimized for speed while maintaining solid reasoning capabilities.",modelUrl:"https://platform.openai.com/docs/models/o3-mini",website:"https://openai.com",contextWindow:2e5,pricing:{inputCostPerMil:"1.1",outputCostPerMil:"4.4",pricingUrl:"https://platform.openai.com/docs/pricing",cachedInputCostPerMil:"0.55",cacheCreationInputCostPerMil:"0"}},parameters:{maximumLength:{value:4096,range:[50,8192]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-02-03"},{id:"openai:o3-mini-medium",modelApiName:"o3-mini",provider:"openai",providerHumanName:"OpenAI",makerHumanName:"OpenAI",name:"o3-mini (Medium)",supportsVision:!1,minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,disableInGateway:!0,info:{description:"o3-mini with medium reasoning effort - balanced approach matching o1's performance levels.",modelUrl:"https://platform.openai.com/docs/models/o3-mini",website:"https://openai.com",contextWindow:2e5,pricing:{inputCostPerMil:"1.1",outputCostPerMil:"4.4",pricingUrl:"https://platform.openai.com/docs/pricing",cachedInputCostPerMil:"0.55",cacheCreationInputCostPerMil:"0"}},parameters:{maximumLength:{value:4096,range:[50,8192]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-02-03"},{id:"openai:o3-mini-high",modelApiName:"o3-mini",provider:"openai",providerHumanName:"OpenAI",makerHumanName:"OpenAI",name:"o3-mini (High)",supportsVision:!1,minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,disableInGateway:!0,info:{description:"o3-mini with high reasoning effort - enhanced reasoning power exceeding o1 in many STEM domains.",modelUrl:"https://platform.openai.com/docs/models/o3-mini",website:"https://openai.com",contextWindow:2e5,pricing:{inputCostPerMil:"1.1",outputCostPerMil:"4.4",pricingUrl:"https://platform.openai.com/docs/pricing",cachedInputCostPerMil:"0.55",cacheCreationInputCostPerMil:"0"}},parameters:{maximumLength:{value:4096,range:[50,8192]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-02-03"},{id:"openai:o3",modelApiName:"o3",provider:"openai",providerHumanName:"OpenAI",makerHumanName:"OpenAI",name:"o3",supportsVision:!0,minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,new:!0,info:{description:"OpenAI's o3 is their most powerful reasoning model, setting new state-of-the-art benchmarks in coding, math, science, and visual perception. It excels at complex queries requiring multi-faceted analysis, with particular strength in analyzing images, charts, and graphics.",modelUrl:"https://platform.openai.com/docs/models/o3",website:"https://openai.com",contextWindow:2e5,pricing:{inputCostPerMil:"2",outputCostPerMil:"8",pricingUrl:"https://platform.openai.com/docs/pricing",cachedInputCostPerMil:"0.5",cacheCreationInputCostPerMil:"0"}},parameters:{maximumLength:{value:16384,range:[50,32768]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-04-16"},{id:"openai:o4-mini",modelApiName:"o4-mini",provider:"openai",providerHumanName:"OpenAI",makerHumanName:"OpenAI",name:"o4-mini",supportsVision:!0,minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,new:!0,info:{description:"OpenAI's o4-mini delivers fast, cost-efficient reasoning with exceptional performance for its size, particularly excelling in math (best-performing on AIME benchmarks), coding, and visual tasks.",modelUrl:"https://platform.openai.com/docs/models/o4-mini",website:"https://openai.com",contextWindow:2e5,pricing:{inputCostPerMil:"1.1",outputCostPerMil:"4.4",pricingUrl:"https://platform.openai.com/docs/pricing",cachedInputCostPerMil:"0.275",cacheCreationInputCostPerMil:"0"}},parameters:{maximumLength:{value:16384,range:[50,32768]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-04-16"},{id:"openai:gpt-4.1",modelApiName:"gpt-4.1",provider:"openai",providerHumanName:"OpenAI",makerHumanName:"OpenAI",name:"GPT-4.1",supportsVision:!0,minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,new:!0,info:{description:"GPT 4.1 is OpenAI's flagship model for complex tasks. It is well suited for problem solving across domains.",modelUrl:"https://platform.openai.com/docs/models/gpt-4.1",website:"https://openai.com",contextWindow:1047576,pricing:{inputCostPerMil:"2",outputCostPerMil:"8",pricingUrl:"https://platform.openai.com/docs/pricing",cachedInputCostPerMil:"0.5",cacheCreationInputCostPerMil:"0"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,16384]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-04-14"},{id:"openai:gpt-4.1-mini",modelApiName:"gpt-4.1-mini",provider:"openai",providerHumanName:"OpenAI",makerHumanName:"OpenAI",name:"GPT-4.1 mini",supportsVision:!0,minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,new:!0,info:{description:"GPT 4.1 mini provides a balance between intelligence, speed, and cost that makes it an attractive model for many use cases.",modelUrl:"https://platform.openai.com/docs/models/gpt-4.1-mini",website:"https://openai.com",contextWindow:1047576,pricing:{inputCostPerMil:"0.4",outputCostPerMil:"1.6",pricingUrl:"https://platform.openai.com/docs/pricing",cachedInputCostPerMil:"0.1",cacheCreationInputCostPerMil:"0"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,16384]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-04-14"},{id:"openai:gpt-4.1-nano",modelApiName:"gpt-4.1-nano",provider:"openai",providerHumanName:"OpenAI",makerHumanName:"OpenAI",name:"GPT-4.1 nano",supportsVision:!0,minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,new:!0,info:{description:"GPT-4.1 nano is the fastest, most cost-effective GPT 4.1 model.",modelUrl:"https://platform.openai.com/docs/models/gpt-4.1-nano",website:"https://openai.com",contextWindow:1047576,pricing:{inputCostPerMil:"0.1",outputCostPerMil:"0.4",pricingUrl:"https://platform.openai.com/docs/pricing",cachedInputCostPerMil:"0.025",cacheCreationInputCostPerMil:"0"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,16384]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-04-14"},{id:"openai:gpt-4o",modelApiName:"gpt-4o",provider:"openai",providerHumanName:"OpenAI",makerHumanName:"OpenAI",name:"GPT-4o",supportsVision:!0,minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"GPT-4o from OpenAI has broad general knowledge and domain expertise allowing it to follow complex instructions in natural language and solve difficult problems accurately. It matches GPT-4 Turbo performance with a faster and cheaper API.",modelUrl:"https://platform.openai.com/docs/models/gpt-4o",website:"https://openai.com",contextWindow:128e3,pricing:{inputCostPerMil:"2.5",outputCostPerMil:"10",pricingUrl:"https://platform.openai.com/docs/pricing",cachedInputCostPerMil:"1.25",cacheCreationInputCostPerMil:"0"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:1024,range:[50,2048]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-02-19"},{id:"openai:gpt-4o-mini",modelApiName:"gpt-4o-mini",provider:"openai",providerHumanName:"OpenAI",makerHumanName:"OpenAI",name:"GPT-4o mini",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"GPT-4o mini from OpenAI is their most advanced and cost-efficient small model. It is multi-modal (accepting text or image inputs and outputting text) and has higher intelligence than gpt-3.5-turbo but is just as fast.",modelUrl:"https://platform.openai.com/docs/models/gpt-4o-mini",website:"https://openai.com",contextWindow:128e3,pricing:{inputCostPerMil:"0.15",outputCostPerMil:"0.6",pricingUrl:"https://platform.openai.com/docs/pricing",cachedInputCostPerMil:"0.075",cacheCreationInputCostPerMil:"0"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:1024,range:[50,2048]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-02-19"},{id:"openai:gpt-4.5-preview",modelApiName:"gpt-4.5-preview",provider:"openai",providerHumanName:"OpenAI",makerHumanName:"OpenAI",name:"GPT-4.5 Preview",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,new:!0,minBillingTier:"pro",disableInGateway:!0,info:{description:"GPT-4.5 from OpenAI is a research preview of their largest and most knowledgeable model yet, building on GPT-4o with expanded pre-training and broader general-purpose capabilities. It demonstrates more natural interactions, stronger alignment with user intent, and improved emotional intelligence, making it well-suited for writing, programming, and practical problem-solving with reduced hallucinations.",modelUrl:"https://platform.openai.com/docs/models#gpt-4-5",website:"https://openai.com",contextWindow:128e3,pricing:{inputCostPerMil:"75",outputCostPerMil:"150",pricingUrl:"https://platform.openai.com/docs/pricing"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,16384]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-03-01"},{id:"openai:gpt-4-turbo",modelApiName:"gpt-4-turbo",provider:"openai",providerHumanName:"OpenAI",makerHumanName:"OpenAI",supportsVision:!0,name:"GPT-4 Turbo",minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"gpt-4-turbo from OpenAI has broad general knowledge and domain expertise allowing it to follow complex instructions in natural language and solve difficult problems accurately. It has a knowledge cutoff of April 2023 and a 128,000 token context window.",modelUrl:"https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4",website:"https://openai.com",contextWindow:128e3,pricing:{inputCostPerMil:"10",outputCostPerMil:"30",pricingUrl:"https://platform.openai.com/docs/pricing"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:1024,range:[1,4095]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-02-19"},{id:"openai:gpt-4",modelApiName:"gpt-4",provider:"openai",providerHumanName:"OpenAI",makerHumanName:"OpenAI",name:"GPT-4",minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,disabled:!0,info:{description:"GPT-4 from OpenAI has broad general knowledge and domain expertise allowing it to follow complex instructions in natural language and solve difficult problems accurately.",modelUrl:"https://platform.openai.com/docs/models/gpt-4",website:"https://openai.com",contextWindow:8192,pricing:{inputCostPerMil:"30",outputCostPerMil:"60",pricingUrl:"https://platform.openai.com/docs/pricing"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:1024,range:[1,4095]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-02-19"},{id:"openai:gpt-4-0613",disabled:!0,provider:"openai",providerHumanName:"OpenAI",makerHumanName:"OpenAI",modelApiName:"gpt-4-0613",name:"gpt-4-0613",lastModifiedDate:"2023-06-13",minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"Snapshot of gpt-4 from June 13th 2023 with function calling data. Unlike gpt-4, this model does not receive updates, and is deprecated 3 months after a new version is released.",modelUrl:"https://platform.openai.com/docs/models/gpt-4",website:"https://openai.com",contextWindow:8192,pricing:{inputCostPerMil:"30",outputCostPerMil:"60",pricingUrl:"https://platform.openai.com/docs/pricing"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:500,range:[1,4095]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}}},{id:"openai:gpt-4-1106-preview",disabled:!0,provider:"openai",name:"gpt-4-1106-preview",modelApiName:"gpt-4-1106-preview",providerHumanName:"OpenAI",makerHumanName:"OpenAI",minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"The latest GPT-4 model with improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more. Returns a maximum of 4,096 output tokens. This preview model is not yet suited for production traffic.",modelUrl:"https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo",website:"https://openai.com",contextWindow:128e3,pricing:{inputCostPerMil:"10",outputCostPerMil:"30",pricingUrl:"https://platform.openai.com/docs/pricing"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:500,range:[1,4095]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2024-03-14"},{id:"openai:gpt-3.5-turbo",modelApiName:"gpt-3.5-turbo",provider:"openai",providerHumanName:"OpenAI",makerHumanName:"OpenAI",supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"OpenAI's most capable and cost effective model in the GPT-3.5 family optimized for chat purposes, but also works well for traditional completions tasks.",modelUrl:"https://platform.openai.com/docs/models/gpt-3-5",contextWindow:4096,website:"https://openai.com",pricing:{pricingUrl:"https://platform.openai.com/docs/pricing",inputCostPerMil:"0.5",outputCostPerMil:"1.5"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:500,range:[1,4095]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},name:"GPT-3.5 Turbo",lastModifiedDate:"2025-02-19"},{id:"openai:gpt-3.5-turbo-1106",disabled:!0,provider:"openai",name:"gpt-3.5-turbo-1106",modelApiName:"gpt-3.5-turbo-1106",providerHumanName:"OpenAI",makerHumanName:"OpenAI",supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"The latest GPT-3.5 Turbo model with improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more. Returns a maximum of 4,096 output tokens.",modelUrl:"https://platform.openai.com/docs/models/gpt-3-5",contextWindow:16385,website:"https://openai.com",pricing:{pricingUrl:"https://platform.openai.com/docs/pricing",inputCostPerMil:"1",outputCostPerMil:"2"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:500,range:[50,4096]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2024-03-14"},{id:"openai:gpt-3.5-turbo-16k",disabled:!0,modelApiName:"gpt-3.5-turbo-16k",provider:"openai",providerHumanName:"OpenAI",makerHumanName:"OpenAI",supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"Same capabilities as the standard gpt-3.5-turbo model but with 4 times the context.",modelUrl:"https://platform.openai.com/docs/models/gpt-3-5",contextWindow:16384,website:"https://openai.com",pricing:{pricingUrl:"https://platform.openai.com/docs/pricing",inputCostPerMil:"1.5",outputCostPerMil:"2"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:500,range:[50,16280]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},name:"gpt-3.5-turbo-16k",lastModifiedDate:"2024-03-14"},{id:"openai:gpt-3.5-turbo-16k-0613",modelApiName:"gpt-3.5-turbo-16k-0613",disabled:!0,provider:"openai",providerHumanName:"OpenAI",makerHumanName:"OpenAI",supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"Snapshot of gpt-3.5-turbo-16k from June 13th 2023. Unlike gpt-3.5-turbo-16k, this model does not receive updates, and will be deprecated 3 months after a new version is released.",modelUrl:"https://platform.openai.com/docs/models/gpt-3-5",contextWindow:16384,website:"https://openai.com",pricing:{pricingUrl:"https://platform.openai.com/docs/pricing",inputCostPerMil:"1.5",outputCostPerMil:"2"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:500,range:[50,16280]},topP:{value:1,range:[.1,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},name:"gpt-3.5-turbo-16k-0613",lastModifiedDate:"2024-03-14"},{id:"openai:gpt-3.5-turbo-instruct",modelApiName:"gpt-3.5-turbo-instruct",provider:"openai",providerHumanName:"OpenAI",makerHumanName:"OpenAI",supportsStructuredOutput:!0,supportsToolCalling:!1,name:"GPT-3.5 Turbo Instruct",info:{description:"Similar capabilities as GPT-3 era models. Compatible with legacy Completions endpoint and not Chat Completions.",modelUrl:"https://platform.openai.com/docs/models/gpt-3-5",contextWindow:4096,website:"https://openai.com",pricing:{pricingUrl:"https://platform.openai.com/docs/pricing",inputCostPerMil:"1.5",outputCostPerMil:"2"}},parameters:{temperature:{value:.5,range:[.1,1]},maximumLength:{value:200,range:[50,4097]},topP:{value:1,range:[.1,1]},presencePenalty:{value:0,range:[0,1]},frequencyPenalty:{value:0,range:[0,1]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-02-19"},{id:"openai:gpt-5-2025-08-07",modelApiName:"gpt-5-2025-08-07",provider:"openai",providerHumanName:"OpenAI",makerHumanName:"OpenAI",name:"GPT-5",supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"GPT-5 is OpenAI's flagship language model that excels at complex reasoning, broad real-world knowledge, code-intensive, and multi-step agentic tasks.",modelUrl:"https://platform.openai.com/docs/models",website:"https://openai.com",contextWindow:4e5,pricing:{inputCostPerMil:"1.25",outputCostPerMil:"10",pricingUrl:"https://platform.openai.com/docs/pricing",cachedInputCostPerMil:"0.125",cacheCreationInputCostPerMil:"0"}},parameters:{maximumLength:{value:64e3,range:[50,128e3]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-08-07"},{id:"openai:gpt-5-mini-2025-08-07",modelApiName:"gpt-5-mini-2025-08-07",provider:"openai",providerHumanName:"OpenAI",makerHumanName:"OpenAI",name:"GPT-5 mini",supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"GPT-5 mini is a cost optimized model that excels at reasoning/chat tasks. It offers an optimal balance between speed, cost, and capability.",modelUrl:"https://platform.openai.com/docs/models",website:"https://openai.com",contextWindow:4e5,pricing:{inputCostPerMil:"0.25",outputCostPerMil:"2",pricingUrl:"https://platform.openai.com/docs/pricing",cachedInputCostPerMil:"0.025",cacheCreationInputCostPerMil:"0"}},parameters:{maximumLength:{value:64e3,range:[50,128e3]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-08-07"},{id:"openai:gpt-5-nano-2025-08-07",modelApiName:"gpt-5-nano-2025-08-07",provider:"openai",providerHumanName:"OpenAI",makerHumanName:"OpenAI",name:"GPT-5 nano",supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"GPT-5 nano is a high throughput model that excels at simple instruction or classification tasks.",modelUrl:"https://platform.openai.com/docs/models",website:"https://openai.com",contextWindow:4e5,pricing:{inputCostPerMil:"0.05",outputCostPerMil:"0.40",pricingUrl:"https://platform.openai.com/docs/pricing",cachedInputCostPerMil:"0.005",cacheCreationInputCostPerMil:"0"}},parameters:{maximumLength:{value:64e3,range:[50,128e3]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-08-07"},{id:"openai:gpt-5-codex",modelApiName:"gpt-5-codex",provider:"openai",providerHumanName:"OpenAI",makerHumanName:"OpenAI",name:"GPT-5-Codex",supportsVision:!1,minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"openai",model:{primary:"gpt-5-codex"}},info:{description:"GPT-5-Codex is a version of GPT-5 optimized for agentic coding tasks in Codex or similar environments.",modelUrl:"https://platform.openai.com/docs/models/gpt-5-codex",website:"https://openai.com",contextWindow:4e5,pricing:{inputCostPerMil:"1.25",outputCostPerMil:"10",cachedInputCostPerMil:"0.125",cacheCreationInputCostPerMil:"0",pricingUrl:"https://platform.openai.com/docs/pricing"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:1024,range:[50,128e3]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-09-23"},{id:"openai:gpt-5-pro",modelApiName:"gpt-5-pro",provider:"openai",providerHumanName:"OpenAI",makerHumanName:"OpenAI",name:"GPT-5 pro",supportsVision:!1,minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"openai",model:{primary:"gpt-5-pro",secondary:["gpt-5-pro-2025-10-06"]}},info:{description:"GPT-5 pro uses more compute to think harder and provide consistently better answers. Since GPT-5 pro is designed to tackle tough problems, some requests may take several minutes to finish.",modelUrl:"https://platform.openai.com/docs/models/gpt-5-pro",website:"https://openai.com",contextWindow:4e5,pricing:{inputCostPerMil:"15",outputCostPerMil:"120",pricingUrl:"https://platform.openai.com/docs/pricing"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:1024,range:[50,272e3]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-10-06"},{id:"parasail:parasail-kimi-k2-instruct",modelApiName:"parasail-kimi-k2-instruct",provider:"parasail",providerHumanName:"Parasail",makerHumanName:"Moonshot AI",name:"Kimi K2",supportsVision:!1,minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,new:!0,gateway:{chef:"moonshotai",model:{primary:"kimi-k2",secondary:["kimi-k2-instruct"]}},info:{description:"Kimi K2 is a model with a context length of 128k, featuring powerful code and Agent capabilities based on MoE architecture. It has 1T total parameters with 32B activated parameters. In benchmark performance tests across major categories including general knowledge reasoning, programming, mathematics, and Agent capabilities, the K2 model outperforms other mainstream open-source models.",modelUrl:"https://moonshotai.github.io/Kimi-K2/",website:"https://www.parasail.io/",contextWindow:32768,pricing:{inputCostPerMil:".99",outputCostPerMil:"2.99",pricingUrl:"https://www.saas.parasail.io/pricing"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,16384]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-07-12"},{id:"parasail:parasail-qwen3-coder-480b-a35b-instruct",modelApiName:"parasail-qwen3-coder-480b-a35b-instruct",provider:"parasail",providerHumanName:"Parasail",makerHumanName:"Qwen",name:"Qwen3 Coder",supportsVision:!1,minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"alibaba",model:{primary:"qwen3-coder",secondary:["qwen3-coder-480b","qwen3-coder-480b-a35b-instruct"]}},info:{description:"Qwen3-Coder is an advanced agentic code model delivering performance comparable to Claude Sonnet across coding tasks, browser automation, and repository-scale understanding. With native 256K token context (extendable to 1M via Yarn) and specialized function calling for platforms like Qwen Code and CLINE, it excels at complex agentic coding workflows.",modelUrl:"https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8",website:"https://www.parasail.io/",contextWindow:262144,pricing:{inputCostPerMil:"2",outputCostPerMil:"3.5",pricingUrl:"https://www.saas.parasail.io/pricing"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,66536]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-07-22"},{id:"parasail:parasail-gpt-oss-120b",modelApiName:"parasail-gpt-oss-120b",provider:"parasail",providerHumanName:"Parasail",makerHumanName:"OpenAI",name:"gpt-oss-120b",supportsVision:!1,minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"openai",model:{primary:"gpt-oss-120b"}},info:{description:"This model excels at efficient reasoning across science, math, and coding applications. It’s ideal for real-time coding assistance, processing large documents for Q&A and summarization, agentic research workflows, and regulated on-premises workloads.",modelUrl:"https://www.parasail.io/",website:"https://www.parasail.io/",contextWindow:131072,pricing:{inputCostPerMil:"0.15",outputCostPerMil:"0.60",pricingUrl:"https://www.saas.parasail.io/pricing"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,131e3]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-08-07"},{id:"parasail:parasail-deepseek-v3-0324",modelApiName:"parasail-deepseek-v3-0324",provider:"parasail",providerHumanName:"Parasail",makerHumanName:"DeepSeek",name:"DeepSeek V3 0324",supportsVision:!1,minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"deepseek",model:{primary:"deepseek-v3",secondary:["deepseek-v3-0324"]}},info:{description:"Fast general-purpose LLM with enhanced reasoning capabilities",modelUrl:"https://www.parasail.io/",website:"https://www.parasail.io/",contextWindow:163840,pricing:{inputCostPerMil:"0.79",outputCostPerMil:"1.15",pricingUrl:"https://www.saas.parasail.io/pricing"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,16384]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-08-07"},{id:"parasail:parasail-deepseek-r1-0528",modelApiName:"parasail-deepseek-r1-0528",provider:"parasail",providerHumanName:"Parasail",makerHumanName:"DeepSeek",name:"DeepSeek R1 0528",supportsVision:!1,minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"deepseek",model:{primary:"deepseek-r1",secondary:["deepseek-r1-0528"]}},info:{description:"The latest revision of DeepSeek's first-generation reasoning model",modelUrl:"https://www.parasail.io/",website:"https://www.parasail.io/",contextWindow:163840,pricing:{inputCostPerMil:"0.79",outputCostPerMil:"4.00",pricingUrl:"https://www.saas.parasail.io/pricing"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,16384]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-08-08"},{id:"parasail:parasail-qwen3-32b",modelApiName:"parasail-qwen3-32b",provider:"parasail",providerHumanName:"Parasail",makerHumanName:"Alibaba",name:"Qwen 3.32B",supportsVision:!1,minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"alibaba",model:{primary:"qwen-3-32b",secondary:["qwen3-32b","qwen3-32b-fp8"]}},info:{description:"Qwen3-32B is a world-class model with comparable quality to DeepSeek R1 while outperforming GPT-4.1 and Claude Sonnet 3.7. It excels in code-gen, tool-calling, and advanced reasoning, making it an exceptional model for a wide range of production use cases.",modelUrl:"https://www.parasail.io/",website:"https://www.parasail.io/",contextWindow:40960,pricing:{inputCostPerMil:"0.1",outputCostPerMil:"0.5",pricingUrl:"https://www.saas.parasail.io/pricing"}},parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:8192,range:[50,16384]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},lastModifiedDate:"2025-08-08"},{id:"perplexity:sonar",modelApiName:"sonar",provider:"perplexity",providerHumanName:"Perplexity",makerHumanName:"Perplexity",minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"Perplexity's lightweight offering with search grounding, quicker and cheaper than Sonar Pro.",website:"https://perplexity.ai",modelUrl:"https://sonar.perplexity.ai",contextWindow:127e3,pricing:{pricingUrl:"https://docs.perplexity.ai/docs/pricing",inputCostPerMil:"1",outputCostPerMil:"1"}},parameters:{temperature:{value:.2,range:[0,2]},maximumLength:{value:1024,range:[1,8e3]},topP:{value:.9,range:[0,1]},topK:{value:1,range:[0,2048]},presencePenalty:{value:0,range:[-2,2]},frequencyPenalty:{value:1,range:[0,1]},stopSequences:{value:[],range:[]}},name:"Sonar",lastModifiedDate:"2025-02-19"},{id:"perplexity:sonar-pro",modelApiName:"sonar-pro",provider:"perplexity",providerHumanName:"Perplexity",makerHumanName:"Perplexity",minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"Perplexity's premier offering with search grounding, supporting advanced queries and follow-ups.",website:"https://perplexity.ai",modelUrl:"https://sonar.perplexity.ai",contextWindow:2e5,pricing:{pricingUrl:"https://docs.perplexity.ai/docs/pricing",inputCostPerMil:"3",outputCostPerMil:"15"}},parameters:{temperature:{value:.2,range:[0,2]},maximumLength:{value:1024,range:[1,8e3]},topP:{value:.9,range:[0,1]},topK:{value:1,range:[0,2048]},presencePenalty:{value:0,range:[-2,2]},frequencyPenalty:{value:1,range:[0,1]}},name:"Sonar Pro",lastModifiedDate:"2025-02-19"},{id:"perplexity:sonar-reasoning",modelApiName:"sonar-reasoning",provider:"perplexity",providerHumanName:"Perplexity",makerHumanName:"Perplexity",minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"A reasoning-focused model that outputs Chain of Thought (CoT) in responses, providing detailed explanations with search grounding.",website:"https://perplexity.ai",modelUrl:"https://sonar.perplexity.ai",contextWindow:127e3,pricing:{pricingUrl:"https://docs.perplexity.ai/guides/pricing",inputCostPerMil:"1",outputCostPerMil:"5"}},parameters:{temperature:{value:.2,range:[0,2]},maximumLength:{value:1024,range:[1,8e3]},topP:{value:.9,range:[0,1]},topK:{value:1,range:[0,2048]},presencePenalty:{value:0,range:[-2,2]},frequencyPenalty:{value:1,range:[0,1]}},name:"Sonar Reasoning",lastModifiedDate:"2025-02-19"},{id:"perplexity:sonar-reasoning-pro",modelApiName:"sonar-reasoning-pro",provider:"perplexity",providerHumanName:"Perplexity",makerHumanName:"Perplexity",minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"A premium reasoning-focused model that outputs Chain of Thought (CoT) in responses, providing comprehensive explanations with enhanced search capabilities and multiple search queries per request.",website:"https://perplexity.ai",modelUrl:"https://sonar.perplexity.ai",contextWindow:127e3,pricing:{pricingUrl:"https://docs.perplexity.ai/guides/pricing",inputCostPerMil:"2",outputCostPerMil:"8"}},parameters:{temperature:{value:.2,range:[0,2]},maximumLength:{value:1024,range:[1,8e3]},topP:{value:.9,range:[0,1]},topK:{value:1,range:[0,2048]},presencePenalty:{value:0,range:[-2,2]},frequencyPenalty:{value:1,range:[0,1]}},name:"Sonar Reasoning Pro",lastModifiedDate:"2025-02-19"},{id:"perplexity:llama-3-sonar-small-32k-chat",modelApiName:"llama-3-sonar-small-32k-chat",provider:"perplexity",providerHumanName:"Perplexity",makerHumanName:"Meta",supportsStructuredOutput:!0,supportsToolCalling:!0,disabled:!0,info:{description:"Llama 3 Sonar is a 7 Billion parameter model based on Meta's Llama 3 model. It is fine-tuned for chat completions and served by Perplexity.",website:"https://perplexity.ai/",modelUrl:"https://www.perplexity.ai/search/Llama-3-Overview-Mz3Cw09KTdq9gavmibDBeA",contextWindow:32768,pricing:{pricingUrl:"https://docs.perplexity.ai/docs/pricing",inputCostPerMil:"0.2",outputCostPerMil:"0.2"}},parameters:{temperature:{value:.5,range:[.01,2]},maximumLength:{value:1e3,range:[300,32768]},topP:{value:1,range:[.01,1]},topK:{value:1,range:[.01,2048]},presencePenalty:{value:0,range:[-2,2]},frequencyPenalty:{value:.1,range:[.01,1]}},name:"llama-3-sonar-small-32k-chat",lastModifiedDate:"2024-03-01"},{id:"perplexity:llama-3.1-sonar-small-128k-chat",modelApiName:"llama-3.1-sonar-small-128k-chat",provider:"perplexity",providerHumanName:"Perplexity",makerHumanName:"Meta",supportsStructuredOutput:!1,supportsToolCalling:!0,disabled:!0,info:{description:"Llama 3.1 Sonar is a 8 Billion parameter model based on Meta's Llama 3 model. It is fine-tuned for chat completions and served by Perplexity.",website:"https://perplexity.ai/",modelUrl:"https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md",contextWindow:32768,pricing:{pricingUrl:"https://docs.perplexity.ai/docs/pricing",inputCostPerMil:"0.2",outputCostPerMil:"0.2"}},parameters:{temperature:{value:.5,range:[.01,2]},maximumLength:{value:1e3,range:[300,32768]},topP:{value:1,range:[.01,1]},topK:{value:1,range:[.01,2048]},presencePenalty:{value:0,range:[-2,2]},frequencyPenalty:{value:.1,range:[.01,1]}},name:"llama-3.1-sonar-small-128k-chat",lastModifiedDate:"2024-03-21"},{id:"perplexity:llama-3-sonar-large-32k-chat",modelApiName:"llama-3-sonar-large-32k-chat",provider:"perplexity",providerHumanName:"Perplexity",makerHumanName:"Meta",supportsStructuredOutput:!0,supportsToolCalling:!0,disabled:!0,info:{description:"Llama 3 Sonar is an 8x7B parameter model based on Meta's Llama 3 model. It is fine-tuned for chat completions and served by Perplexity.",website:"https://perplexity.ai/",modelUrl:"https://www.perplexity.ai/search/Llama-3-Overview-Mz3Cw09KTdq9gavmibDBeA",contextWindow:32768,pricing:{pricingUrl:"https://docs.perplexity.ai/docs/pricing",inputCostPerMil:"0.6",outputCostPerMil:"0.6"}},parameters:{temperature:{value:.5,range:[.01,2]},maximumLength:{value:1e3,range:[300,32768]},topP:{value:1,range:[.01,1]},topK:{value:1,range:[.01,2048]},presencePenalty:{value:0,range:[-2,2]},frequencyPenalty:{value:.1,range:[.01,1]}},name:"llama-3-sonar-large-32k-chat",lastModifiedDate:"2024-03-14"},{id:"perplexity:llama-3.1-sonar-large-128k-chat",modelApiName:"llama-3.1-sonar-large-128k-chat",provider:"perplexity",providerHumanName:"Perplexity",makerHumanName:"Meta",supportsStructuredOutput:!1,supportsToolCalling:!0,disabled:!0,info:{description:"Llama 3.1 Sonar is a 70B parameter model based on Meta's Llama 3.1 model. It is fine-tuned for chat completions and served by Perplexity.",website:"https://perplexity.ai/",modelUrl:"https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md",contextWindow:32768,pricing:{pricingUrl:"https://docs.perplexity.ai/docs/pricing",inputCostPerMil:"0.6",outputCostPerMil:"0.6"}},parameters:{temperature:{value:.5,range:[.01,2]},maximumLength:{value:1e3,range:[300,32768]},topP:{value:1,range:[.01,1]},topK:{value:1,range:[.01,2048]},presencePenalty:{value:0,range:[-2,2]},frequencyPenalty:{value:.1,range:[.01,1]}},name:"llama-3.1-sonar-large-128k-chat",lastModifiedDate:"2024-03-21"},{id:"perplexity:llama-3-sonar-small-32k-online",modelApiName:"llama-3-sonar-small-32k-online",provider:"perplexity",providerHumanName:"Perplexity",makerHumanName:"Meta",minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,disabled:!0,info:{description:"Llama 3 Sonar Online is a 7 Billion parameter model based on Meta's Llama 3 model. It is fine-tuned for chat completions and served by Perplexity. The model has access to recent internet knowledge when forming responses.",website:"https://perplexity.ai/",modelUrl:"https://www.perplexity.ai/search/Llama-3-Overview-Mz3Cw09KTdq9gavmibDBeA",contextWindow:28e3,pricing:{pricingUrl:"https://docs.perplexity.ai/docs/pricing",inputCostPerMil:"0.2",outputCostPerMil:"0.2"}},parameters:{temperature:{value:.5,range:[.01,2]},maximumLength:{value:1e3,range:[300,28e3]},topP:{value:1,range:[.01,1]},topK:{value:1,range:[.01,2048]},presencePenalty:{value:0,range:[-2,2]},frequencyPenalty:{value:.1,range:[.01,1]}},name:"llama-3-sonar-small-32k-online",lastModifiedDate:"2024-03-21"},{id:"perplexity:llama-3.1-sonar-small-128k-online",modelApiName:"llama-3.1-sonar-small-128k-online",provider:"perplexity",providerHumanName:"Perplexity",makerHumanName:"Meta",minBillingTier:"pro",supportsStructuredOutput:!1,supportsToolCalling:!0,disabled:!0,info:{description:"Llama 3 Sonar Online is a 8B parameter model based on Meta's Llama 3.1 model. It is fine-tuned for chat completions and served by Perplexity. The model has access to recent internet knowledge when forming responses.",website:"https://perplexity.ai/",modelUrl:"https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md",contextWindow:28e3,pricing:{pricingUrl:"https://docs.perplexity.ai/docs/pricing",inputCostPerMil:"0.2",outputCostPerMil:"0.2"}},parameters:{temperature:{value:.5,range:[.01,2]},maximumLength:{value:1e3,range:[300,28e3]},topP:{value:1,range:[.01,1]},topK:{value:1,range:[.01,2048]},presencePenalty:{value:0,range:[-2,2]},frequencyPenalty:{value:.1,range:[.01,1]}},name:"llama-3.1-sonar-small-128k-online",lastModifiedDate:"2024-03-21"},{id:"perplexity:llama-3-sonar-large-32k-online",modelApiName:"llama-3-sonar-large-32k-online",provider:"perplexity",providerHumanName:"Perplexity",makerHumanName:"Meta",minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,disabled:!0,info:{description:"Llama 3 Sonar Online is an 70b parameter model based on Meta's Llama 3 model. It is fine-tuned for chat completions and served by Perplexity. The model has access to recent internet knowledge when forming responses.",website:"https://perplexity.ai/",modelUrl:"https://www.perplexity.ai/search/Llama-3-Overview-Mz3Cw09KTdq9gavmibDBeA",contextWindow:28e3,pricing:{pricingUrl:"https://docs.perplexity.ai/docs/pricing",inputCostPerMil:"0.6",outputCostPerMil:"0.6"}},parameters:{temperature:{value:.5,range:[.01,2]},maximumLength:{value:1e3,range:[300,28e3]},topP:{value:1,range:[.01,1]},topK:{value:1,range:[.01,2048]},presencePenalty:{value:0,range:[-2,2]},frequencyPenalty:{value:.1,range:[.01,1]}},name:"llama-3-sonar-large-32k-online",lastModifiedDate:"2024-03-14"},{id:"perplexity:llama-3.1-sonar-large-128k-online",modelApiName:"llama-3.1-sonar-large-128k-online",provider:"perplexity",providerHumanName:"Perplexity",makerHumanName:"Meta",minBillingTier:"pro",supportsStructuredOutput:!1,supportsToolCalling:!0,disabled:!0,info:{description:"Llama 3 Sonar Online is an 8x7B parameter model based on Meta's Llama 3.1 model. It is fine-tuned for chat completions and served by Perplexity. The model has access to recent internet knowledge when forming responses.",website:"https://perplexity.ai/",modelUrl:"https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md",contextWindow:28e3,pricing:{pricingUrl:"https://docs.perplexity.ai/docs/pricing",inputCostPerMil:"0.6",outputCostPerMil:"0.6"}},parameters:{temperature:{value:.5,range:[.01,2]},maximumLength:{value:1e3,range:[300,28e3]},topP:{value:1,range:[.01,1]},topK:{value:1,range:[.01,2048]},presencePenalty:{value:0,range:[-2,2]},frequencyPenalty:{value:.1,range:[.01,1]}},name:"llama-3.1-sonar-large-128k-online",lastModifiedDate:"2024-03-14"},{id:"perplexity:llama-3.1-sonar-huge-128k-online",modelApiName:"llama-3.1-sonar-huge-128k-online",provider:"perplexity",providerHumanName:"Perplexity",makerHumanName:"Meta",minBillingTier:"pro",supportsStructuredOutput:!1,supportsToolCalling:!0,disabled:!0,info:{description:"Llama 3 Sonar Online is an 405B parameter model based on Meta's Llama 3.1 model. It is fine-tuned for chat completions and served by Perplexity. The model has access to recent internet knowledge when forming responses.",website:"https://perplexity.ai/",modelUrl:"https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md",contextWindow:28e3,pricing:{pricingUrl:"https://docs.perplexity.ai/docs/pricing",inputCostPerMil:"0.6",outputCostPerMil:"0.6"}},parameters:{temperature:{value:.5,range:[.01,2]},maximumLength:{value:1e3,range:[300,28e3]},topP:{value:1,range:[.01,1]},topK:{value:1,range:[.01,2048]},presencePenalty:{value:0,range:[-2,2]},frequencyPenalty:{value:.1,range:[.01,1]}},name:"llama-3.1-sonar-huge-128k-online",lastModifiedDate:"2024-03-14"},{id:"perplexity:llama-3-8b-instruct",modelApiName:"llama-3-8b-instruct",provider:"perplexity",providerHumanName:"Perplexity",makerHumanName:"Meta",supportsStructuredOutput:!1,supportsToolCalling:!0,disabled:!0,info:{description:"Llama is a 8 billion parameter open source model by Meta fine-tuned for instruction following purposes served by Perplexity.",website:"https://perplexity.ai/",modelUrl:"https://blog.perplexity.ai/blog/introducing-pplx-online-llms?utm_source=labs&utm_medium=labs&utm_campaign=online-llms",contextWindow:8192,pricing:{pricingUrl:"https://docs.perplexity.ai/docs/pricing",inputCostPerMil:"0.2",outputCostPerMil:"0.2"}},parameters:{temperature:{value:.5,range:[.01,5]},maximumLength:{value:1e3,range:[300,8192]},topP:{value:1,range:[.01,1]},frequencyPenalty:{value:1,range:[.01,1]}},name:"llama-3-8b-instruct",lastModifiedDate:"2024-03-14"},{id:"perplexity:llama-3-70b-instruct",modelApiName:"llama-3-70b-instruct",provider:"perplexity",providerHumanName:"Perplexity",minBillingTier:"pro",makerHumanName:"Meta",supportsStructuredOutput:!1,supportsToolCalling:!0,disabled:!0,info:{description:"Llama is a 70 billion parameter open source model by Meta fine-tuned for instruction following purposes served by Perplexity.",website:"https://perplexity.ai/",modelUrl:"https://blog.perplexity.ai/blog/introducing-pplx-online-llms?utm_source=labs&utm_medium=labs&utm_campaign=online-llms",contextWindow:8192,pricing:{pricingUrl:"https://docs.perplexity.ai/docs/pricing",inputCostPerMil:"1.0",outputCostPerMil:"1.0"}},parameters:{temperature:{value:.5,range:[.01,5]},maximumLength:{value:1e3,range:[300,8192]},topP:{value:1,range:[.01,1]},frequencyPenalty:{value:1,range:[.01,1]}},name:"llama-3-70b-instruct",lastModifiedDate:"2024-03-14"},{id:"perplexity:codellama-34b-instruct",modelApiName:"codellama-34b-instruct",disabled:!0,provider:"perplexity",providerHumanName:"Perplexity",minBillingTier:"pro",makerHumanName:"Meta",supportsStructuredOutput:!1,supportsToolCalling:!0,info:{description:"Code Llama is a 34 billion parameter open source model by Meta fine-tuned for instruction following purposes served by Perplexity.",website:"https://perplexity.ai/",modelUrl:"https://blog.perplexity.ai/blog/introducing-pplx-online-llms?utm_source=labs&utm_medium=labs&utm_campaign=online-llms",contextWindow:16384,pricing:{pricingUrl:"https://docs.perplexity.ai/docs/pricing",inputCostPerMil:"0.35",outputCostPerMil:"1.4"}},parameters:{temperature:{value:.75,range:[.01,5]},maximumLength:{value:1e3,range:[300,16384]},topP:{value:1,range:[.01,1]},frequencyPenalty:{value:1,range:[.01,1]}},name:"codellama-34b-instruct",lastModifiedDate:"2024-03-14"},{id:"perplexity:codellama-70b-instruct",modelApiName:"codellama-70b-instruct",provider:"perplexity",disabled:!0,providerHumanName:"Perplexity",minBillingTier:"pro",makerHumanName:"Meta",supportsStructuredOutput:!1,supportsToolCalling:!0,info:{description:"Code Llama is a 70 billion parameter open source model by Meta fine-tuned for instruction following purposes served by Perplexity.",website:"https://perplexity.ai/",modelUrl:"https://blog.perplexity.ai/blog/introducing-pplx-online-llms?utm_source=labs&utm_medium=labs&utm_campaign=online-llms",contextWindow:16384,pricing:{pricingUrl:"https://docs.perplexity.ai/docs/pricing",inputCostPerMil:"0.7",outputCostPerMil:"2.8"}},parameters:{temperature:{value:.75,range:[.01,5]},maximumLength:{value:1e3,range:[300,16384]},topP:{value:1,range:[.01,1]},frequencyPenalty:{value:1,range:[.01,1]}},name:"codellama-70b-instruct",lastModifiedDate:"2024-03-14"},{id:"stealth:sonoma-dusk-alpha",name:"Sonoma Dusk Alpha",provider:"stealth",providerHumanName:"Stealth",makerHumanName:"Stealth",modelApiName:"grok-4-fast-non-reasoning",supportsStructuredOutput:!0,supportsToolCalling:!0,minBillingTier:"hobby",disabled:!0,gateway:{chef:"stealth",model:{primary:"sonoma-dusk-alpha"}},info:{description:"A fast and intelligent general-purpose frontier model with a 2 million token context window. Supports image inputs and parallel tool calling. Note: prompts and responses may be retained and used for training by the provider during the stealth period.",website:"",modelUrl:"",contextWindow:2e6,pricing:{pricingUrl:"",inputCostPerMil:"0",outputCostPerMil:"0"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:8192,range:[0,131072]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-09-18"},{id:"stealth:sonoma-sky-alpha",name:"Sonoma Sky Alpha",provider:"stealth",providerHumanName:"Stealth",makerHumanName:"Stealth",modelApiName:"grok-4-fast-reasoning",supportsStructuredOutput:!0,supportsToolCalling:!0,minBillingTier:"hobby",disabled:!0,gateway:{chef:"stealth",model:{primary:"sonoma-sky-alpha"}},info:{description:"A maximally intelligent general-purpose frontier model with a 2 million token context window. Supports image inputs and parallel tool calling. Note: prompts and responses may be retained and used for training by the provider during the stealth period.",website:"",modelUrl:"",contextWindow:2e6,pricing:{pricingUrl:"",inputCostPerMil:"0",outputCostPerMil:"0"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:8192,range:[0,131072]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-09-18"},{id:"vercel:v0-chat",makerHumanName:"Vercel",modelApiName:"v0-engineer-a",provider:"vercel",providerHumanName:"Vercel",name:"v0-chat",disableInGateway:!0,parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:1024,range:[50,2048]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},info:{contextWindow:5e4,description:"Vercel v0-chat model",modelUrl:"https://vercel.com",website:"https://vercel.com",pricing:{pricingUrl:"https://vercel.com/pricing"}},lastModifiedDate:"2024-03-14",disabled:!0},{id:"vercel:v0-mini",makerHumanName:"Vercel",modelApiName:"v0-mini",provider:"vercel",providerHumanName:"Vercel",name:"v0-mini",disableInGateway:!0,parameters:{temperature:{value:.7,range:[0,2]},maximumLength:{value:1024,range:[50,2048]},topP:{value:1,range:[0,1]},presencePenalty:{value:0,range:[0,2]},frequencyPenalty:{value:0,range:[0,2]},stopSequences:{value:[],range:[]}},info:{contextWindow:5e4,description:"Vercel v0-mini model",modelUrl:"https://vercel.com",website:"https://vercel.com",pricing:{pricingUrl:"https://vercel.com/pricing"}},lastModifiedDate:"2024-03-14",disabled:!0},{id:"vercel:dev-1",makerHumanName:"Vercel",modelApiName:"dev-1",provider:"vercel",providerHumanName:"Vercel",name:"v0 DEV-1",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,disableInGateway:!0,parameters:{maximumLength:{value:1024,range:[1,32e3]}},new:!0,info:{contextWindow:128e3,description:"DEV-1 is the model behind v0, built by Vercel to generate, fix, and optimize modern web apps with framework-specific reasoning and up-to-date knowledge.",modelUrl:"https://v0.dev/api",website:"https://v0.dev",pricing:{pricingUrl:"https://v0.dev/pricing",inputCostPerMil:"1.50",outputCostPerMil:"7.50"}},lastModifiedDate:"2025-05-13",disabled:!0},{id:"vercel:v0-1.0-md",makerHumanName:"Vercel",modelApiName:"v0-1.0-md",provider:"vercel",providerHumanName:"Vercel",minBillingTier:"pro",name:"v0-1.0-md",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,parameters:{maximumLength:{value:1024,range:[1,32e3]}},info:{contextWindow:128e3,description:"Access the model behind v0 to generate, fix, and optimize modern web apps with framework-specific reasoning and up-to-date knowledge.",modelUrl:"https://vercel.com/docs/v0/model-api",website:"https://v0.dev",pricing:{pricingUrl:"https://v0.app/pricing",inputCostPerMil:"3.00",outputCostPerMil:"15.00"},termsOfServiceUrl:"https://vercel.com/legal/terms",privacyPolicyUrl:"https://vercel.com/legal/privacy-policy"},lastModifiedDate:"2025-05-21"},{id:"vercel:v0-1.5-md",makerHumanName:"Vercel",modelApiName:"v0-1.5-md",provider:"vercel",providerHumanName:"Vercel",minBillingTier:"pro",name:"v0-1.5-md",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,parameters:{maximumLength:{value:8192,range:[1,32768]}},new:!0,info:{contextWindow:128e3,description:"Access the model behind v0 to generate, fix, and optimize modern web apps with framework-specific reasoning and up-to-date knowledge.",modelUrl:"https://vercel.com/docs/v0/model-api",website:"https://v0.dev",pricing:{pricingUrl:"https://v0.app/pricing",inputCostPerMil:"3",outputCostPerMil:"15"},termsOfServiceUrl:"https://vercel.com/legal/terms",privacyPolicyUrl:"https://vercel.com/legal/privacy-policy"},lastModifiedDate:"2025-06-06"},{id:"vercel:v0-1.5-lg",makerHumanName:"Vercel",modelApiName:"v0-1.5-lg",provider:"vercel",providerHumanName:"Vercel",minBillingTier:"pro",name:"v0-1.5-lg",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,disabled:!0,parameters:{maximumLength:{value:1024,range:[1,32e3]}},new:!0,info:{contextWindow:512e3,description:"Access the model behind v0 to generate, fix, and optimize modern web apps with framework-specific reasoning and up-to-date knowledge.",modelUrl:"https://vercel.com/docs/v0/api",website:"https://v0.dev",pricing:{pricingUrl:"https://v0.dev/pricing",inputCostPerMil:"15.00",outputCostPerMil:"75.00"},termsOfServiceUrl:"https://vercel.com/legal/terms",privacyPolicyUrl:"https://vercel.com/legal/privacy-policy"},lastModifiedDate:"2025-06-06"},{id:"vertex:claude-3-7-sonnet-20250219",provider:"vertexAnthropic",providerHumanName:"Google Vertex AI",modelApiName:"claude-3-7-sonnet@20250219",makerHumanName:"Vertex",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!1,minBillingTier:"pro",info:{description:"A highly capable multimodal model with the best combination of accuracy, speed, and cost for a wide range of tasks.",website:"https://cloud.google.com/blog/products/ai-machine-learning/anthropics-claude-3-7-sonnet-is-available-on-vertex-ai",modelUrl:"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude#claude-3-7-sonnet",contextWindow:2e5,pricing:{pricingUrl:"https://cloud.google.com/vertex-ai/generative-ai/pricing#claude-models",inputCostPerMil:"3",outputCostPerMil:"15",cachedInputCostPerMil:"0.3",cacheCreationInputCostPerMil:"3.75"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,5e3]},topP:{value:.9,range:[0,1]}},name:"Claude 3.7 Sonnet (Vertex)",lastModifiedDate:"2025-03-28"},{id:"vertex:claude-3-5-sonnet-v2-20241022",provider:"vertexAnthropic",providerHumanName:"Google Vertex AI",modelApiName:"claude-3-5-sonnet-v2@20241022",makerHumanName:"Vertex",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!1,minBillingTier:"pro",info:{description:"The upgraded Claude 3.5 Sonnet is now state-of-the-art for a variety of tasks including real-world software engineering, enhanced agentic capabilities, and computer use.",website:"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude",modelUrl:"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude#claude-3-5-v2-sonnet",contextWindow:2e5,pricing:{pricingUrl:"https://cloud.google.com/vertex-ai/generative-ai/pricing#claude-models",inputCostPerMil:"3",outputCostPerMil:"15",cachedInputCostPerMil:"0.3",cacheCreationInputCostPerMil:"3.75"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"Claude 3.5 Sonnet (Vertex)",lastModifiedDate:"2025-05-18"},{id:"vertex:claude-3-5-haiku-20241022",provider:"vertexAnthropic",providerHumanName:"Google Vertex AI",modelApiName:"claude-3-5-haiku@20241022",makerHumanName:"Vertex",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!1,minBillingTier:"pro",info:{description:"Claude 3.5 Haiku, Anthropic’s fastest and most cost-effective model, excels at use cases like code and test case generation, sub-agents, and user-facing chatbots.",website:"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude",modelUrl:"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude#claude-3-5-haiku",contextWindow:2e5,pricing:{pricingUrl:"https://cloud.google.com/vertex-ai/generative-ai/pricing#claude-models",inputCostPerMil:"0.8",outputCostPerMil:"4",cachedInputCostPerMil:"0.08",cacheCreationInputCostPerMil:"1"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"Claude 3.5 Haiku (Vertex)",lastModifiedDate:"2025-05-18"},{id:"vertex:claude-3-opus-20240229",provider:"vertexAnthropic",providerHumanName:"Google Vertex AI",modelApiName:"claude-3-opus@20240229",makerHumanName:"Vertex",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!1,minBillingTier:"pro",info:{description:"Claude 3 Opus is a powerful AI model, with top-level performance on highly complex tasks. It can navigate open-ended prompts and sight-unseen scenarios with remarkable fluency and human-like understanding.",website:"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude",modelUrl:"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude#claude-3-opus",contextWindow:2e5,pricing:{pricingUrl:"https://cloud.google.com/vertex-ai/generative-ai/pricing#claude-models",inputCostPerMil:"15",outputCostPerMil:"75",cachedInputCostPerMil:"1.5",cacheCreationInputCostPerMil:"18.75"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"Claude 3 Opus (Vertex)",lastModifiedDate:"2025-05-18"},{id:"vertex:claude-4-opus-20250514",provider:"vertexAnthropic",providerHumanName:"Google Vertex AI",modelApiName:"claude-opus-4@20250514",makerHumanName:"Vertex",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!1,minBillingTier:"pro",new:!0,info:{description:"Claude Opus 4 is Anthropic's most powerful model yet and the state-of-the-art coding model. It delivers sustained performance on long-running tasks that require focused effort and thousands of steps, significantly expanding what AI agents can solve. Claude Opus 4 is ideal for powering frontier agent products and features.",website:"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude",modelUrl:"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude#claude-opus-4",contextWindow:2e5,pricing:{pricingUrl:"https://cloud.google.com/vertex-ai/generative-ai/pricing#claude-models",inputCostPerMil:"15",outputCostPerMil:"75",cachedInputCostPerMil:"1.5",cacheCreationInputCostPerMil:"18.75"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"Claude Opus 4 (Vertex)",lastModifiedDate:"2025-08-12"},{id:"vertex:claude-4-sonnet-20250514",provider:"vertexAnthropic",providerHumanName:"Google Vertex AI",modelApiName:"claude-sonnet-4@20250514",makerHumanName:"Vertex",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!1,minBillingTier:"pro",new:!0,info:{description:"Claude Sonnet 4 is Anthropic’s mid-size model with several major improvements—especially for coding. It is the ideal balance of performance and practicality for most internal and external use cases, including user-facing AI agents. The model is also more steerable than before, providing enhanced control over its eagerness to implement changes.",website:"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude",modelUrl:"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude#claude-sonnet-4",contextWindow:2e5,pricing:{pricingUrl:"https://cloud.google.com/vertex-ai/generative-ai/pricing#claude-models",inputCostPerMil:"3",outputCostPerMil:"15",cachedInputCostPerMil:"0.3",cacheCreationInputCostPerMil:"3.75"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"Claude Sonnet 4",lastModifiedDate:"2025-08-12"},{id:"vertex:claude-3-haiku-20240307",provider:"vertexAnthropic",providerHumanName:"Google Vertex AI",modelApiName:"claude-3-haiku@20240307",makerHumanName:"Vertex",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!1,minBillingTier:"pro",info:{description:"Claude 3 Haiku is Anthropic's fastest vision and text model for near-instant responses to simple queries, meant for seamless AI experiences mimicking human interactions.",website:"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude",modelUrl:"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude#claude-3-haiku",contextWindow:2e5,pricing:{pricingUrl:"https://cloud.google.com/vertex-ai/generative-ai/pricing#claude-models",inputCostPerMil:"0.25",outputCostPerMil:"1.25",cachedInputCostPerMil:"0.03",cacheCreationInputCostPerMil:"0.3"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"Claude 3 Haiku (Vertex)",lastModifiedDate:"2025-05-18"},{id:"vertex:claude-3-5-sonnet-20240620",provider:"vertexAnthropic",providerHumanName:"Google Vertex AI",modelApiName:"claude-3-5-sonnet@20240620",makerHumanName:"Vertex",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!1,minBillingTier:"pro",info:{description:"Claude 3.5 Sonnet outperforms Claude 3 Opus on a wide range of Anthropic’s evaluations with the speed and cost of Anthropic’s mid-tier model, Claude 3 Sonnet.",website:"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude",modelUrl:"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude#claude-3-5-sonnet",contextWindow:2e5,pricing:{pricingUrl:"https://cloud.google.com/vertex-ai/generative-ai/pricing#claude-models",inputCostPerMil:"3",outputCostPerMil:"15",cachedInputCostPerMil:"0.3",cacheCreationInputCostPerMil:"3.75"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"Claude 3.5 Sonnet 2024-06-20 (Vertex)",lastModifiedDate:"2025-10-03"},{id:"vertex:gemini-2.0-flash-001",provider:"vertex",providerHumanName:"Google Vertex AI",modelApiName:"gemini-2.0-flash-001",makerHumanName:"Vertex",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,minBillingTier:"hobby",info:{description:"Gemini 2.0 Flash delivers next-gen features and improved capabilities, including superior speed, built-in tool use, multimodal generation, and a 1M token context window.",website:"https://cloud.google.com/vertex-ai/generative-ai/docs/models",modelUrl:"https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-0-flash",contextWindow:1048576,pricing:{pricingUrl:"https://cloud.google.com/vertex-ai/generative-ai/pricing",inputCostPerMil:"0.15",outputCostPerMil:"0.6"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"Gemini 2.0 Flash (Vertex)",lastModifiedDate:"2025-05-18"},{id:"vertex:gemini-2.0-flash-lite-001",provider:"vertex",providerHumanName:"Google Vertex AI",modelApiName:"gemini-2.0-flash-lite-001",makerHumanName:"Vertex",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,minBillingTier:"hobby",info:{description:"Gemini 2.0 Flash delivers next-gen features and improved capabilities, including superior speed, built-in tool use, multimodal generation, and a 1M token context window.",website:"https://cloud.google.com/vertex-ai/generative-ai/docs/models",modelUrl:"https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-0-flash",contextWindow:1048576,pricing:{pricingUrl:"https://cloud.google.com/vertex-ai/generative-ai/pricing",inputCostPerMil:"0.075",outputCostPerMil:"0.3"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"Gemini 2.0 Flash Lite (Vertex)",lastModifiedDate:"2025-05-18"},{id:"vertex:gemini-2.5-flash-preview-04-17",provider:"vertexCentral",providerHumanName:"Google Vertex AI",modelApiName:"gemini-2.5-flash-preview-04-17",makerHumanName:"Vertex",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,minBillingTier:"pro",disableInGateway:!0,info:{description:"Gemini 2.5 Flash is a thinking model that offers great, well-rounded capabilities. It is designed to offer a balance between price and performance.",website:"https://cloud.google.com/vertex-ai/generative-ai/docs/models",modelUrl:"https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash",contextWindow:1048576,pricing:{pricingUrl:"https://cloud.google.com/vertex-ai/generative-ai/pricing",inputCostPerMil:"0.15",outputCostPerMil:"0.6"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"Gemini 2.5 Flash Preview (Vertex)",lastModifiedDate:"2025-05-18"},{id:"vertex:gemini-2.5-pro-preview-05-06",provider:"vertexCentral",providerHumanName:"Google Vertex AI",modelApiName:"gemini-2.5-pro-preview-05-06",makerHumanName:"Vertex",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,minBillingTier:"pro",disableInGateway:!0,info:{description:"Gemini 2.5 Pro is our most advanced reasoning Gemini model, capable of solving complex problems.",website:"https://cloud.google.com/vertex-ai/generative-ai/docs/models",modelUrl:"https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-pro",contextWindow:1048576,pricing:{pricingUrl:"https://cloud.google.com/vertex-ai/generative-ai/pricing",inputCostPerMil:"1.25",outputCostPerMil:"10"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"Gemini 2.5 Pro Preview (Vertex)",lastModifiedDate:"2025-05-18"},{id:"vertex:llama-3.3-70b-instruct-maas",provider:"vertexCentral",providerHumanName:"Google Vertex AI",modelApiName:"llama-3.3-70b-instruct-maas",makerHumanName:"Vertex",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,minBillingTier:"hobby",disableInGateway:!0,info:{description:"The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.",website:"https://cloud.google.com/vertex-ai/generative-ai/docs/models",modelUrl:"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/llama/llama3-3",contextWindow:128e3,pricing:{pricingUrl:"https://cloud.google.com/vertex-ai/generative-ai/pricing",inputCostPerMil:"0.72",outputCostPerMil:"0.72"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"Llama 3.3 70B (Vertex)",lastModifiedDate:"2025-05-18"},{id:"vertex:llama-4-scout-17b-16e-instruct-maas",provider:"vertex",providerHumanName:"Google Vertex AI",modelApiName:"llama-4-scout-17b-16e-instruct-maas",makerHumanName:"Vertex",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,minBillingTier:"hobby",info:{description:"Llama 4 Scout 17B 16E Instruct is a multimodal model that uses the Mixture-of-Experts (MoE) architecture and early fusion, delivering state-of-the-art results for its size class.",website:"https://cloud.google.com/vertex-ai/generative-ai/docs/models",modelUrl:"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/llama/llama4-scout",contextWindow:1310720,pricing:{pricingUrl:"https://cloud.google.com/vertex-ai/generative-ai/pricing",inputCostPerMil:"0.25",outputCostPerMil:"0.7"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"Llama 4 Scout 17B 16E Instruct (Vertex)",lastModifiedDate:"2025-05-18"},{id:"vertex:llama-4-maverick-17b-128e-instruct-maas",provider:"vertex",providerHumanName:"Google Vertex AI",modelApiName:"llama-4-maverick-17b-128e-instruct-maas",makerHumanName:"Vertex",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,minBillingTier:"hobby",info:{description:"Llama 4 Maverick 17B-128E is Llama 4's largest and most capable model. It uses the Mixture-of-Experts (MoE) architecture and early fusion to provide coding, reasoning, and image capabilities.",website:"https://cloud.google.com/vertex-ai/generative-ai/docs/models",modelUrl:"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/llama/llama4-maverick",contextWindow:1310720,pricing:{pricingUrl:"https://cloud.google.com/vertex-ai/generative-ai/pricing",inputCostPerMil:"0.35",outputCostPerMil:"1.15"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]},topP:{value:.9,range:[0,1]}},name:"Llama 4 Maverick 17B 128E Instruct (Vertex)",lastModifiedDate:"2025-05-18"},{id:"vertex:gemini-2.5-flash-lite",provider:"vertex",providerHumanName:"Google Vertex AI",modelApiName:"gemini-2.5-flash-lite",makerHumanName:"Vertex",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,minBillingTier:"hobby",new:!0,gateway:{chef:"google",model:{primary:"gemini-2.5-flash-lite"}},info:{description:"Gemini 2.5 Flash-Lite is a balanced, low-latency model with configurable thinking budgets and tool connectivity (e.g., Google Search grounding and code execution). It supports multimodal input and offers a 1M-token context window.",website:"https://cloud.google.com/vertex-ai/generative-ai/docs/models",modelUrl:"https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash-lite",contextWindow:1048576,pricing:{pricingUrl:"https://cloud.google.com/vertex-ai/generative-ai/pricing",inputCostPerMil:"0.1",outputCostPerMil:"0.4"}},parameters:{temperature:{value:1,range:[0,2]},maximumLength:{value:4096,range:[0,65536]},topP:{value:.95,range:[0,1]}},name:"Gemini 2.5 Flash Lite",lastModifiedDate:"2025-08-14"},{id:"vertex:claude-4.5-sonnet-20250929",provider:"vertexAnthropic",providerHumanName:"Google Vertex AI",modelApiName:"claude-sonnet-4-5@20250929",makerHumanName:"Vertex",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!1,minBillingTier:"hobby",new:!0,gateway:{chef:"anthropic",model:{primary:"claude-sonnet-4.5",secondary:["claude-4.5-sonnet","claude-4.5-sonnet-20250929","claude-sonnet-4.5-20250929","claude-4.5-sonnet-20250929-v1","claude-sonnet-4.5-20250929-v1","anthropic.claude-sonnet-4.5-20250929-v1:0","claude-sonnet-4.5@20250929"]}},info:{description:"Claude Sonnet 4.5 is the newest model in the Sonnet series, offering improvements and updates over Sonnet 4.",website:"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude",modelUrl:"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude#claude-sonnet-4.5",contextWindow:2e5,pricing:{pricingUrl:"https://cloud.google.com/vertex-ai/generative-ai/pricing#claude-models",inputCostPerMil:"3",outputCostPerMil:"15",cachedInputCostPerMil:"0.3",cacheCreationInputCostPerMil:"3.75"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:4096,range:[0,8192]}},name:"Claude Sonnet 4.5",lastModifiedDate:"2025-09-29"},{id:"vertex:claude-haiku-4.5-20251001",provider:"vertexAnthropic",providerHumanName:"Google Vertex AI",name:"Claude Haiku 4.5",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,modelApiName:"claude-haiku-4-5@20251001",makerHumanName:"Anthropic",minBillingTier:"hobby",new:!0,gateway:{chef:"anthropic",model:{primary:"claude-haiku-4.5",secondary:["claude-4.5-haiku","claude-4.5-haiku-20251001","claude-haiku-4.5-20251001","claude-4.5-haiku-20251001-v1","claude-haiku-4.5-20251001-v1","anthropic.claude-haiku-4.5-20251001-v1:0","claude-haiku-4.5@20251001"]}},info:{description:"Claude Haiku 4.5 matches Sonnet 4's performance on coding, computer use, and agent tasks at substantially lower cost and faster speeds. It delivers near-frontier performance and Claude’s unique character at a price point that works for scaled sub-agent deployments, free tier products, and intelligence-sensitive applications with budget constraints.",website:"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude",modelUrl:"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude#claude-haiku-4.5",contextWindow:2e5,pricing:{pricingUrl:"https://cloud.google.com/vertex-ai/generative-ai/pricing#claude-models",inputCostPerMil:"1",outputCostPerMil:"5",cacheCreationInputCostPerMil:"1.25",cachedInputCostPerMil:"0.1"}},parameters:{temperature:{value:1,range:[0,1]},maximumLength:{value:1024,range:[50,64e3]}},lastModifiedDate:"2025-10-14"},{id:"xai:grok-beta",name:"Grok Beta",provider:"xai",providerHumanName:"xAI",makerHumanName:"xAI",modelApiName:"grok-beta",minBillingTier:"pro",supportsStructuredOutput:!0,supportsToolCalling:!0,disabled:!0,info:{description:"Grok is an AI modeled after the Hitchhiker's Guide to the Galaxy. It is intended to answer almost anything and, far harder, even suggest what questions to ask!",website:"https://x.ai",modelUrl:"https://x.ai/blog/grok",contextWindow:131072,pricing:{pricingUrl:"https://console.x.ai",inputCostPerMil:"5",outputCostPerMil:"15"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:1024,range:[0,4e3]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-02-19"},{id:"xai:grok-vision-beta",name:"Grok Vision Beta",provider:"xai",providerHumanName:"xAI",makerHumanName:"xAI",modelApiName:"grok-vision-beta",minBillingTier:"pro",supportsVision:!0,supportsStructuredOutput:!1,supportsToolCalling:!1,disabled:!0,info:{description:"In addition to Grok's strong text capabilities, this multimodal model can now process a wide variety of visual information, including documents, diagrams, charts, screenshots, and photographs.",website:"https://x.ai",modelUrl:"https://x.ai/blog/grok",contextWindow:8192,pricing:{pricingUrl:"https://console.x.ai",inputCostPerMil:"5",outputCostPerMil:"15"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:1024,range:[0,4e3]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-02-19"},{id:"xai:grok-2-1212",name:"Grok 2",provider:"xai",providerHumanName:"xAI",makerHumanName:"xAI",modelApiName:"grok-2-1212",supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"Grok 2 is a frontier language model with state-of-the-art reasoning capabilities. It features advanced capabilities in chat, coding, and reasoning, outperforming both Claude 3.5 Sonnet and GPT-4-Turbo on the LMSYS leaderboard.",website:"https://x.ai",modelUrl:"https://x.ai/blog/grok-2",contextWindow:131072,pricing:{pricingUrl:"https://console.x.ai",inputCostPerMil:"2",outputCostPerMil:"10"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:1024,range:[0,4e3]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-02-18"},{id:"xai:grok-2-vision-1212",name:"Grok 2 Vision",provider:"xai",providerHumanName:"xAI",makerHumanName:"xAI",modelApiName:"grok-2-vision-1212",supportsVision:!0,supportsStructuredOutput:!0,supportsToolCalling:!0,info:{description:"Grok 2 vision model excels in vision-based tasks, delivering state-of-the-art performance in visual math reasoning (MathVista) and document-based question answering (DocVQA). It can process a wide variety of visual information including documents, diagrams, charts, screenshots, and photographs.",website:"https://x.ai",modelUrl:"https://x.ai/blog/grok-2",contextWindow:32768,pricing:{pricingUrl:"https://console.x.ai",inputCostPerMil:"2",outputCostPerMil:"10"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:1024,range:[0,4e3]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-02-18"},{id:"xai:grok-3-beta",name:"Grok 3 Beta",provider:"xai",providerHumanName:"xAI",makerHumanName:"xAI",modelApiName:"grok-3-beta",supportsVision:!1,supportsStructuredOutput:!0,supportsToolCalling:!0,new:!0,minBillingTier:"pro",info:{description:"xAI's flagship model that excels at enterprise use cases like data extraction, coding, and text summarization. Possesses deep domain knowledge in finance, healthcare, law, and science.",website:"https://x.ai",modelUrl:"https://x.ai/news/grok-3",contextWindow:131072,pricing:{pricingUrl:"https://console.x.ai",inputCostPerMil:"3",outputCostPerMil:"15"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:8192,range:[0,16384]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-04-09"},{id:"xai:grok-3-fast-beta",name:"Grok 3 Fast Beta",provider:"xai",providerHumanName:"xAI",makerHumanName:"xAI",modelApiName:"grok-3-fast-beta",supportsVision:!1,supportsStructuredOutput:!0,supportsToolCalling:!0,new:!0,minBillingTier:"pro",info:{description:"xAI's flagship model that excels at enterprise use cases like data extraction, coding, and text summarization. Possesses deep domain knowledge in finance, healthcare, law, and science. The fast model variant is served on faster infrastructure, offering response times that are significantly faster than the standard. The increased speed comes at a higher cost per output token.",website:"https://x.ai",modelUrl:"https://x.ai/news/grok-3",contextWindow:131072,pricing:{pricingUrl:"https://console.x.ai",inputCostPerMil:"5",outputCostPerMil:"25"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:8192,range:[0,16384]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-04-09"},{id:"xai:grok-3-mini-beta",name:"Grok 3 Mini Beta",provider:"xai",providerHumanName:"xAI",makerHumanName:"xAI",modelApiName:"grok-3-mini-beta",supportsVision:!1,supportsStructuredOutput:!0,supportsToolCalling:!0,new:!0,info:{description:"xAI's lightweight model that thinks before responding. Great for simple or logic-based tasks that do not require deep domain knowledge. The raw thinking traces are accessible.",website:"https://x.ai",modelUrl:"https://x.ai/news/grok-3",contextWindow:131072,pricing:{pricingUrl:"https://console.x.ai",inputCostPerMil:"0.3",outputCostPerMil:"0.5"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:8192,range:[0,16384]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-04-09"},{id:"xai:grok-3-mini-fast-beta",name:"Grok 3 Mini Fast Beta",provider:"xai",providerHumanName:"xAI",makerHumanName:"xAI",modelApiName:"grok-3-mini-fast-beta",supportsVision:!1,supportsStructuredOutput:!0,supportsToolCalling:!0,new:!0,minBillingTier:"pro",info:{description:"xAI's lightweight model that thinks before responding. Great for simple or logic-based tasks that do not require deep domain knowledge. The raw thinking traces are accessible. The fast model variant is served on faster infrastructure, offering response times that are significantly faster than the standard. The increased speed comes at a higher cost per output token.",website:"https://x.ai",modelUrl:"https://x.ai/news/grok-3",contextWindow:131072,pricing:{pricingUrl:"https://console.x.ai",inputCostPerMil:"0.6",outputCostPerMil:"4"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:8192,range:[0,16384]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-04-09"},{id:"xai:grok-4",name:"Grok 4",provider:"xai",providerHumanName:"xAI",makerHumanName:"xAI",modelApiName:"grok-4",supportsVision:!1,supportsStructuredOutput:!0,supportsToolCalling:!0,minBillingTier:"pro",info:{description:"xAI's latest and greatest flagship model, offering unparalleled performance in natural language, math and reasoning - the perfect jack of all trades.",website:"https://x.ai",modelUrl:"https://x.ai/news/",contextWindow:256e3,pricing:{pricingUrl:"https://console.x.ai",inputCostPerMil:"3",outputCostPerMil:"15"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:8192,range:[0,16384]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-07-09"},{id:"xai:grok-code-fast-1",name:"Grok Code Fast 1",provider:"xai",providerHumanName:"xAI",makerHumanName:"xAI",modelApiName:"grok-code-fast-1",supportsVision:!1,supportsStructuredOutput:!0,supportsToolCalling:!0,new:!0,minBillingTier:"hobby",gateway:{chef:"xai",model:{primary:"grok-code-fast-1",secondary:["grok-code-fast","grok-code-fast-1-0825"]}},info:{description:"xAI's latest coding model that offers fast agentic coding with a 256K context window.",website:"https://x.ai",modelUrl:"https://docs.x.ai/docs/models/grok-code-fast-1",contextWindow:256e3,pricing:{pricingUrl:"https://docs.x.ai/docs/models/grok-code-fast-1",inputCostPerMil:"0.2",outputCostPerMil:"1.5"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:8192,range:[0,256e3]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-08-26"},{id:"xai:grok-4-fast-non-reasoning",name:"Grok 4 Fast Non-Reasoning",provider:"xai",providerHumanName:"xAI",makerHumanName:"xAI",modelApiName:"grok-4-fast-non-reasoning",supportsVision:!1,supportsStructuredOutput:!0,supportsToolCalling:!0,new:!0,minBillingTier:"hobby",gateway:{chef:"xai",model:{primary:"grok-4-fast-non-reasoning"}},info:{description:"Grok 4 Fast is xAI's latest multimodal model with SOTA cost-efficiency and a 2M token context window. It comes in two flavors: non-reasoning and reasoning. Note: prompts and responses may be retained and used for training by the provider while the model is free.",website:"https://x.ai",modelUrl:"https://x.ai/news/",contextWindow:2e6,pricing:{pricingUrl:"https://docs.x.ai/docs/models",inputCostPerMil:"0",outputCostPerMil:"0"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:8192,range:[0,256e3]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-09-18"},{id:"xai:grok-4-fast-reasoning",name:"Grok 4 Fast Reasoning",provider:"xai",providerHumanName:"xAI",makerHumanName:"xAI",modelApiName:"grok-4-fast-reasoning",supportsVision:!1,supportsStructuredOutput:!0,supportsToolCalling:!0,new:!0,minBillingTier:"hobby",gateway:{chef:"xai",model:{primary:"grok-4-fast-reasoning"}},info:{description:"Grok 4 Fast is xAI's latest multimodal model with SOTA cost-efficiency and a 2M token context window. It comes in two flavors: non-reasoning and reasoning. Note: prompts and responses may be retained and used for training by the provider while the model is free.",website:"https://x.ai",modelUrl:"https://x.ai/news/",contextWindow:2e6,pricing:{pricingUrl:"https://docs.x.ai/docs/models",inputCostPerMil:"0",outputCostPerMil:"0"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:8192,range:[0,256e3]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-09-18"},{id:"zai:glm-4.5",name:"GLM 4.5",provider:"zai",providerHumanName:"Z.AI",makerHumanName:"Z.ai",modelApiName:"glm-4.5",minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"zai",model:{primary:"glm-4.5"}},info:{description:"GLM-4.5 and GLM-4.5-Air are our latest flagship models, purpose-built as foundational models for agent-oriented applications. Both leverage a Mixture-of-Experts (MoE) architecture. GLM-4.5 has a total parameter count of 355B with 32B active parameters per forward pass, while GLM-4.5-Air adopts a more streamlined design with 106B total parameters and 12B active parameters.",website:"https://z.ai",modelUrl:"https://z.ai/blog/glm-4.5",contextWindow:128e3,pricing:{pricingUrl:"https://docs.z.ai/guides/overview/pricing",inputCostPerMil:"0.6",outputCostPerMil:"2.2"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:1024,range:[0,96e3]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-07-28"},{id:"zai:glm-4.5-air",name:"GLM 4.5 Air",provider:"zai",providerHumanName:"Z.AI",makerHumanName:"Z.ai",modelApiName:"glm-4.5-air",minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"zai",model:{primary:"glm-4.5-air"}},info:{description:"GLM-4.5 and GLM-4.5-Air are our latest flagship models, purpose-built as foundational models for agent-oriented applications. Both leverage a Mixture-of-Experts (MoE) architecture. GLM-4.5 has a total parameter count of 355B with 32B active parameters per forward pass, while GLM-4.5-Air adopts a more streamlined design with 106B total parameters and 12B active parameters.",website:"https://z.ai",modelUrl:"https://z.ai/blog/glm-4.5",contextWindow:128e3,pricing:{pricingUrl:"https://docs.z.ai/guides/overview/pricing",inputCostPerMil:"0.2",outputCostPerMil:"1.1"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:1024,range:[0,96e3]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-07-28"},{id:"zai:glm-4.5v",name:"GLM 4.5V",provider:"zai",providerHumanName:"Z.AI",makerHumanName:"Z.ai",modelApiName:"glm-4.5v",minBillingTier:"hobby",supportsStructuredOutput:!1,supportsToolCalling:!0,supportsVision:!0,gateway:{chef:"zai",model:{primary:"glm-4.5v"}},info:{description:"Built on the GLM-4.5-Air base model, GLM-4.5V inherits proven techniques from GLM-4.1V-Thinking while achieving effective scaling through a powerful 106B-parameter MoE architecture.",website:"https://z.ai",modelUrl:"https://docs.z.ai/guides/vlm/glm-4.5v",contextWindow:66e3,pricing:{pricingUrl:"https://docs.z.ai/guides/overview/pricing",inputCostPerMil:"0.6",outputCostPerMil:"1.8"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:1024,range:[0,16e3]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-08-11"},{id:"zai:glm-4.6",name:"GLM 4.6",provider:"zai",providerHumanName:"Z.AI",makerHumanName:"Z.ai",modelApiName:"glm-4.6",minBillingTier:"hobby",supportsStructuredOutput:!0,supportsToolCalling:!0,gateway:{chef:"zai",model:{primary:"glm-4.6"}},info:{description:"As the latest iteration in the GLM series, GLM-4.6 achieves comprehensive enhancements across multiple domains, including real-world coding, long-context processing, reasoning, searching, writing, and agentic applications.",website:"https://z.ai",modelUrl:"https://z.ai/blog/glm-4.6",contextWindow:2e5,pricing:{pricingUrl:"https://docs.z.ai/guides/overview/pricing",inputCostPerMil:"0.6",outputCostPerMil:"2.2",cachedInputCostPerMil:"0.11"}},parameters:{temperature:{value:.7,range:[0,1]},maximumLength:{value:1024,range:[0,96e3]},topP:{value:1,range:[0,1]}},lastModifiedDate:"2025-09-30"},{id:"huggingface:bigscience/bloom",provider:"huggingface",providerHumanName:"HuggingFace",makerHumanName:"BigScience",modelApiName:"bigscience/bloom",supportsStructuredOutput:!1,supportsToolCalling:!0,disabled:!0,info:{website:"https://bigscience.huggingface.co/",modelUrl:"https://huggingface.co/bigscience/bloom",contextWindow:1024,description:"BLOOM is an autoregressive Large Language Model (LLM) created by BigScience using industrial-scale computational resources. With the ability to output coherent text in 46 languages and 13 programming languages that is hardly distinguishable from human-written text. BLOOM can also perform text tasks it hasn't been explicitly trained for by casting them as text generation tasks.",instructions:"Do NOT talk to Bloom as an entity, it's not a chatbot but a webpage/blog/article completion model. For the best results: mimic a few words of a webpage similar to the content you want to generate. Start a sentence as if YOU were writing a blog, webpage, math post, coding article and Bloom will generate a coherent follow-up."},parameters:{temperature:{value:.5,range:[.1,1]},maximumLength:{value:200,range:[50,1024]},topP:{value:.95,range:[.01,.99]},topK:{value:4,range:[1,500]},repetitionPenalty:{value:1.03,range:[.1,2]}},name:"bloom",lastModifiedDate:"2024-03-14"},{id:"huggingface:google/flan-t5-xxl",provider:"huggingface",makerHumanName:"Google",providerHumanName:"HuggingFace",modelApiName:"google/flan-t5-xxl",disabled:!0,supportsStructuredOutput:!1,supportsToolCalling:!0,name:"flan-t5-xxl",info:{description:"FLAN-T5 XXL is an enhanced Text-to-Text Transfer Transformer fine-tuned on over 1000 tasks across multiple languages. Its unified text-to-text format facilitates broad applicability and provides state-of-the-art performance.",modelUrl:"https://huggingface.co/google/flan-t5-xxl",website:"https://github.com/google-research/t5x",contextWindow:1024},parameters:{temperature:{value:.5,range:[.1,1]},maximumLength:{value:200,range:[50,1024]},topP:{value:.95,range:[.01,.99]},topK:{value:4,range:[1,500]},repetitionPenalty:{value:1.03,range:[.1,2]}},lastModifiedDate:"2024-03-14"},{id:"huggingface:google/gemma-2b-it",provider:"huggingface",makerHumanName:"Google",providerHumanName:"HuggingFace",modelApiName:"google/gemma-2b-it",name:"gemma-2b-it",supportsStructuredOutput:!1,supportsToolCalling:!0,disabled:!0,info:{description:"Gemma is a family of lightweight, state-of-the-art open models built from the same research and technology used to create the Gemini models.",modelUrl:"https://huggingface.co/google/gemma-2b-it",website:"https://blog.google/technology/developers/gemma-open-models/",datasetUrl:void 0,contextWindow:8e3},parameters:{temperature:{value:.5,range:[.1,1]},maximumLength:{value:1024,range:[50,1024]},topP:{value:.95,range:[.01,.99]},topK:{value:4,range:[1,500]},repetitionPenalty:{value:1.03,range:[.1,2]}},lastModifiedDate:"2024-03-14"},{id:"huggingface:google/gemma-7b-it",provider:"huggingface",makerHumanName:"Google",providerHumanName:"HuggingFace",modelApiName:"google/gemma-7b-it",name:"gemma-7b-it",supportsStructuredOutput:!1,supportsToolCalling:!0,disabled:!0,info:{description:"Gemma is a family of lightweight, state-of-the-art open models built from the same research and technology used to create the Gemini models.",modelUrl:"https://huggingface.co/google/gemma-7b-it",website:"https://blog.google/technology/developers/gemma-open-models/",datasetUrl:void 0,contextWindow:8e3},parameters:{temperature:{value:.5,range:[.1,1]},maximumLength:{value:1024,range:[50,1024]},topP:{value:.95,range:[.01,.99]},topK:{value:4,range:[1,500]},repetitionPenalty:{value:1.03,range:[.1,2]}},lastModifiedDate:"2024-03-14"},{id:"huggingface:EleutherAI/gpt-neox-20b",provider:"huggingface",modelApiName:"EleutherAI/gpt-neox-20b",providerHumanName:"HuggingFace",makerHumanName:"EleutherAI",info:{description:"GPT-NeoX-20B is a 20 billion parameter autoregressive language model trained on the Pile using the GPT-NeoX library. Its architecture closely resembles that of GPT-3 and GPT-J-6B.",website:"http://eleuther.ai/",modelUrl:"https://huggingface.co/EleutherAI/gpt-neox-20b",datasetUrl:"https://huggingface.co/EleutherAI/gpt-neox-20b#training-dataset",contextWindow:2048},parameters:{temperature:{value:.5,range:[.1,1]},maximumLength:{value:200,range:[50,1024]},topP:{value:.95,range:[.01,.99]},topK:{value:4,range:[1,500]},repetitionPenalty:{value:1.03,range:[.1,2]},stopSequences:{value:[],range:[]}},disabled:!0,name:"gpt-neox-20b",lastModifiedDate:"2024-03-14"},{id:"huggingface:OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5",provider:"huggingface",modelApiName:"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5",providerHumanName:"HuggingFace",makerHumanName:"OpenAssistant",info:{description:"This is the 4th iteration English supervised-fine-tuning (SFT) model of the Open-Assistant project.  It is based on a Pythia 12B, fine-tuned on human demonstrations of assistant conversations collected through the Open-Assistant human feedback web app before March 25, 2023.",modelUrl:"https://huggingface.co/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5",website:"https://open-assistant.io",contextWindow:1024,datasetUrl:"https://huggingface.co/datasets/OpenAssistant/oasst1"},parameters:{maximumLength:{value:200,range:[50,1024]},typicalP:{value:.2,range:[.1,.99]},repetitionPenalty:{value:1,range:[.1,2]}},disabled:!0,name:"oasst-sft-4-pythia-12b-epoch-3.5",lastModifiedDate:"2024-03-14"},{id:"huggingface:OpenAssistant/oasst-sft-1-pythia-12b",modelApiName:"OpenAssistant/oasst-sft-1-pythia-12b",provider:"huggingface",providerHumanName:"HuggingFace",makerHumanName:"OpenAssistant",info:{description:"This is the first iteration English supervised-fine-tuning (SFT) model of the Open-Assistant project. It is based on a Pythia 12B, fine-tuned on approximately 22k human demonstrations of assistant conversations collected through the Open-Assistant human feedback web app before March 7, 2023.",modelUrl:"https://huggingface.co/OpenAssistant/oasst-sft-1-pythia-12b",website:"https://open-assistant.io",contextWindow:1024,datasetUrl:"https://huggingface.co/datasets/OpenAssistant/oasst1"},parameters:{maximumLength:{value:200,range:[50,1024]},typicalP:{value:.2,range:[.1,.99]},repetitionPenalty:{value:1,range:[.1,2]}},disabled:!0,name:"oasst-sft-1-pythia-12b",lastModifiedDate:"2024-03-14"},{id:"huggingface:bigcode/santacoder",modelApiName:"bigcode/santacoder",provider:"huggingface",providerHumanName:"HuggingFace",makerHumanName:"BigCode",info:{description:"The SantaCoder models are a series of 1.1B parameter models trained on the Python, Java, and JavaScript subset of The Stack (v1.1).",instructions:'The model was trained on GitHub code. As such it is not an instruction model and commands like "Write a function that computes the square root." do not work well. You should phrase commands like they occur in source code such as comments (e.g. # the following function computes the sqrt) or write a function signature and docstring and let the model complete the function body.',contextWindow:2048,modelUrl:"https://huggingface.co/bigcode/santacoder",datasetUrl:"https://www.bigcode-project.org/docs/about/the-stack/",website:"https://www.bigcode-project.org"},parameters:{temperature:{value:.5,range:[.1,1]},maximumLength:{value:200,range:[50,1024]},topP:{value:.95,range:[.01,.99]},topK:{value:4,range:[1,500]},repetitionPenalty:{value:1.03,range:[.1,2]}},disabled:!0,name:"santacoder",lastModifiedDate:"2024-03-14"},{id:"nvidia:llama-3.1-nemotron-70b-instruct",makerHumanName:"Nvidia",supportsVision:!0,modelApiName:"nvidia/llama-3.1-nemotron-70b-instruct",provider:"nvidia",providerHumanName:"Nvidia",name:"llama-3.1-nemotron-70b-instruct",disableInGateway:!0,parameters:{maximumLength:{value:4096,range:[1,8192]},temperature:{value:.7,range:[0,2]},stopSequences:{value:[],range:[]},topP:{value:.4,range:[0,1]},topK:{value:32,range:[1,40]}},info:{contextWindow:1e6,description:"Llama-3.1-Nemotron-70B-Instruct is a large language model customized by NVIDIA to improve the helpfulness of LLM generated responses to user queries.",modelUrl:"https://build.nvidia.com/nvidia/llama-3_1-nemotron-70b-instruct/modelcard",website:"https://build.nvidia.com/nvidia/llama-3_1-nemotron-70b-instruct"},lastModifiedDate:"2024-03-14"}];Object.fromEntries(ea.map(e=>[e.id,e])),ea.filter(e=>!e.disabled);function et(e){var a,t;let i=null==e?void 0:null===(t=e.info)||void 0===t?void 0:null===(a=t.pricing)||void 0===a?void 0:a.outputCostPerMil;return void 0!==i?Number(i)>1?"pro":"hobby":(null==e?void 0:e.minBillingTier)||"hobby"}var ei=t(84524),er=t(94668);let eo=er.fC,en=er.xz,es=r.forwardRef((e,a)=>{let{className:t,align:r="center",sideOffset:o=4,...s}=e;return(0,i.jsx)(er.h_,{children:(0,i.jsx)(er.VY,{ref:a,align:r,sideOffset:o,style:{pointerEvents:"none"},className:(0,n.cn)("z-50 w-64 rounded-md border bg-white p-4 text-zinc-700 shadow-md outline-none data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2",t),...s})})});es.displayName=er.VY.displayName;var el=t(39960);function ep(e){let{id:a}=e,{data:t=[],isLoading:r}=(0,el.ZP)("/api/status?id=".concat(a),e=>fetch(e).then(e=>e.json())),o=(0,i.jsx)(i.Fragment,{children:Array(49).fill(null).map((e,a)=>(0,i.jsx)("div",{className:(0,n.cn)("w-auto h-4 mx-[1px] flex items-center bg-gray-100"),style:{flexBasis:"16px",flexGrow:0}},"status-".concat(a)))});if(!t&&r)return(0,i.jsxs)("div",{children:[(0,i.jsx)("div",{className:"flex items-center justify-between",children:o}),(0,i.jsxs)("div",{className:"flex items-center text-gray-400 text-[10px] space-x-2 mt-1",children:[(0,i.jsx)("div",{children:"12 hrs ago"}),(0,i.jsx)("div",{className:"flex-1 h-px bg-gray-200"}),(0,i.jsx)("div",{className:"w-[80px] inline-flex items-center justify-center",children:(0,i.jsx)("span",{children:"00.00% Uptime"})}),(0,i.jsx)("div",{className:"flex-1 h-px bg-gray-200"}),(0,i.jsx)("div",{children:"Now"})]})]});let s=0;for(let e=0;e<t.length;e++)"red"!==t[e].color&&s++;let l=t&&t.length>0?s/t.length*100:100;return(0,i.jsxs)("div",{children:[(0,i.jsx)("div",{className:"flex items-center justify-between",children:t&&t.length>0?t.map(e=>(0,i.jsx)(C.u,{content:(0,i.jsxs)("div",{className:"space-y-2",children:[(0,i.jsxs)("div",{className:"flex items-center",children:[(0,i.jsx)("div",{className:(0,n.cn)("rounded-full h-2 w-2 mr-2",{"bg-green-500":"green"===e.color,"bg-yellow-500":"yellow"===e.color,"bg-red-500":"red"===e.color,"bg-gray-100":!e.color})}),(0,i.jsxs)("div",{className:"text-xs",children:[e.error_rate.toFixed(2),"% Error Rate"]})]}),(0,i.jsx)("div",{className:"text-xs",children:e.window_start})]}),children:(0,i.jsx)("div",{className:(0,n.cn)("w-auto h-4 mx-[1px] flex items-center",{"bg-green-600":"green"===e.color,"bg-yellow-300":"yellow"===e.color,"bg-red-300":"red"===e.color,"bg-gray-300":!e.color}),style:{flexBasis:"16px",flexGrow:0}})},e.window_start)):o}),(0,i.jsxs)("div",{className:"flex items-center text-gray-700 text-[10px] space-x-2 mt-1",children:[(0,i.jsx)("div",{children:"12 hrs ago"}),(0,i.jsx)("div",{className:"flex-1 h-px bg-gray-200"}),(0,i.jsxs)("div",{className:"w-[80px] inline-flex items-center justify-center",children:[l.toFixed(2),"% Uptime"]}),(0,i.jsx)("div",{className:"flex-1 h-px bg-gray-200"}),(0,i.jsx)("div",{children:"Now"})]})]})}let em=function(e,a){let t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:{attributes:!0,characterData:!0,childList:!0,subtree:!0};r.useEffect(()=>{if(e.current){let i=new MutationObserver(a);return i.observe(e.current,t),()=>i.disconnect()}},[e,a,t])};var eu=t(69994),ed=t(90204),ec=t(46451),eg=t(41813),eh=t(96898);let ev=r.forwardRef((e,a)=>{let{className:t,...r}=e;return(0,i.jsx)(eg.mY,{ref:a,className:(0,n.cn)("flex h-full w-full flex-col overflow-hidden rounded-md bg-white text-zinc-600",t),...r})});ev.displayName=eg.mY.displayName;let ef=r.forwardRef((e,a)=>{let{className:t,...r}=e;return(0,i.jsxs)("div",{className:"flex items-center px-3 border-b","cmdk-input-wrapper":"",children:[(0,i.jsx)(eh.Z,{className:"w-4 h-4 mr-2 opacity-50 shrink-0"}),(0,i.jsx)(eg.mY.Input,{ref:a,className:(0,n.cn)("flex h-10 w-full  !border-0 rounded-md bg-transparent py-3 text-sm outline-none placeholder:text-muted-foreground disabled:cursor-not-allowed disabled:opacity-50",t),...r})]})});ef.displayName=eg.mY.Input.displayName;let ew=r.forwardRef((e,a)=>{let{className:t,...r}=e;return(0,i.jsx)(eg.mY.List,{ref:a,className:(0,n.cn)("max-h-[300px] overflow-y-auto overflow-x-hidden",t),...r})});ew.displayName=eg.mY.List.displayName;let eb=r.forwardRef((e,a)=>(0,i.jsx)(eg.mY.Empty,{ref:a,className:"py-6 text-sm text-center",...e}));eb.displayName=eg.mY.Empty.displayName;let ex=r.forwardRef((e,a)=>{let{className:t,...r}=e;return(0,i.jsx)(eg.mY.Group,{ref:a,className:(0,n.cn)("overflow-hidden p-1 text-foreground [&_[cmdk-group-heading]]:px-2 [&_[cmdk-group-heading]]:py-1.5 [&_[cmdk-group-heading]]:text-xs [&_[cmdk-group-heading]]:font-medium [&_[cmdk-group-heading]]:text-muted-foreground",t),...r})});ex.displayName=eg.mY.Group.displayName,r.forwardRef((e,a)=>{let{className:t,...r}=e;return(0,i.jsx)(eg.mY.Separator,{ref:a,className:(0,n.cn)("-mx-1 h-px bg-border",t),...r})}).displayName=eg.mY.Separator.displayName;let ey=r.forwardRef((e,a)=>{let{className:t,...r}=e;return(0,i.jsx)(eg.mY.Item,{ref:a,className:(0,n.cn)("relative flex cursor-default select-none items-center rounded-sm px-2 py-1.5 text-sm outline-none aria-selected:bg-accent aria-selected:text-accent-foreground data-[disabled]:pointer-events-none data-[disabled]:opacity-50",t),...r})});ey.displayName=eg.mY.Item.displayName;var ek=t(35396),eP=t(27928),eC=t(68949);let eM=eC.dy.Root,eN=eC.dy.Trigger,eA=(0,r.forwardRef)((e,a)=>{let{className:t,...r}=e;return(0,i.jsxs)(eC.dy.Portal,{children:[(0,i.jsx)(eC.dy.Overlay,{className:"fixed z-40 inset-0 bg-zinc-950/60"}),(0,i.jsx)(eC.dy.Content,{ref:a,className:(0,n.cn)("bg-white z-50 rounded-t-[10px] h-[96%] mt-24 fixed bottom-0 left-0 right-0",t),...r})]})});eA.displayName="DrawerContent";let eS=e=>{let{trigger:a,children:t,align:r="start",sideOffset:o=4,className:s,...l}=e,{isMobile:p}=(0,eu.Z)();return p||eP.tq?(0,i.jsxs)(eM,{...l,children:[(0,i.jsx)(eN,{asChild:!0,children:a}),(0,i.jsx)(eA,{className:(0,n.cn)("h-2/3",s),children:t})]}):(0,i.jsxs)(ek.fC,{...l,children:[(0,i.jsx)(ek.xz,{asChild:!0,children:a}),(0,i.jsx)(ek.h_,{children:(0,i.jsx)(ek.VY,{align:r,sideOffset:o,className:(0,n.cn)("z-50 w-72 rounded-md border bg-popover p-4 text-popover-foreground shadow-md outline-none data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2",s),children:t})})]})};function eT(e){return!!e.id.startsWith("custom:")}var eU=t(52318),eq=t(76927);let eH=e=>e?e.toLowerCase().replace(/[^a-z0-9\s]/g," ").trim().replace(/\s+/g," "):"",eI=e=>{let a=new Map;e.forEach(e=>{let t="".concat(e.makerHumanName,"|").concat(e.name);a.set(t,(a.get(t)||0)+1)});let t=new Map;return e.forEach(e=>{let i="".concat(e.makerHumanName,"|").concat(e.name);t.set(e.id,(a.get(i)||0)>1)}),t},eL=(e,a)=>a?e.providerHumanName:e.makerHumanName;function eO(e){var a;let{value:t,disabled:s,onValueChange:l,isModelDatabaseEnabled:p=!1}=e,{models:m,modelsById:u}=(0,eq.v)(),[d,c]=(0,r.useState)(!1),[g,h]=(0,r.useState)(u[t]),v=null!==(a=u[t])&&void 0!==a?a:u["fireworks:llama13b-v2-chat"],{isDesktop:f}=(0,eu.Z)(),[w,b]=(0,r.useState)(m),x=eI(m),y=x.get(v.id)||!1,k=p?et(v):v.minBillingTier;return(0,i.jsx)("div",{className:"flex flex-col flex-1",children:(0,i.jsxs)("div",{className:"relative grid gap-2",children:[(0,i.jsx)(eS,{open:d,onOpenChange:e=>{c(e),b(m)},align:"start",className:"p-0",trigger:(0,i.jsxs)("button",{disabled:s,"aria-label":"Select a model",type:"button",className:(0,n.cn)("inline-flex rounded-md py-0.5 px-3 items-center bg-white dark:bg-black border font-mono text-zinc-600 dark:text-zinc-300 leading-6 hover:bg-zinc-100 hover:dark:bg-zinc-900 transition-colors ease disabled:opacity-60 disabled:cursor-not-allowed justify-between w-full truncate max-w-[288px] h-[32px]"),children:[(0,i.jsxs)("span",{className:"inline-flex items-center gap-1.5 @sm:gap-2 text-[.8rem] truncate max-w-full text-xs",children:[(0,i.jsx)(o.L,{model:v,className:"@sm:size-4 size-4 align-middle block shrink-0"}),(0,i.jsx)("span",{className:"hidden @sm:block",children:eL(v,y)}),(0,i.jsx)("span",{className:(0,n.cn)("truncate @sm:max-w-[300px]"),children:v.name}),"pro"===k?(0,i.jsx)(ei.C,{variant:"pro",className:"font-sans",size:"small",children:"Pro"}):"hobby"===k?(0,i.jsx)(ei.C,{variant:"hobby",className:"font-sans",size:"small",children:"Hobby"}):null]}),(0,i.jsx)(ed.Z,{className:"size-4 ml-2 opacity-50 shrink-0"})]}),children:(0,i.jsxs)(ev,{loop:!0,shouldFilter:!1,className:"relative z-20 dark:bg-black",children:[(0,i.jsx)(ef,{className:"h-12 ring-0 focus:ring-0 focus:border-0 active:border-0 active:ring-0 md:h-10 dark:text-white",placeholder:"Search Models...",onValueChange:e=>{let a=eH(e);b(m.filter(e=>eH(e.name).includes(a)||eH(e.makerHumanName).includes(a)||eH(e.providerHumanName).includes(a)||eH(e.modelApiName).includes(a)))}}),(0,i.jsxs)(ew,{className:"h-[var(--cmdk-list-height)] dark:bg-black max-h-none md:max-h-[450px] z-40",children:[(0,i.jsx)(eb,{children:"No Models found."}),(0,i.jsx)(ex,{className:"p-0 md:p-1.5",children:w.filter(e=>!e.disabled).sort((e,a)=>e.provider===a.provider?e.modelApiName.localeCompare(a.modelApiName):e.provider.localeCompare(a.provider)).map(e=>{let a=x.get(e.id)||!1;return(0,i.jsx)(eB,{model:e,hasDuplicate:a,isSelected:(null==v?void 0:v.id)===e.id,onPeek:e=>h(e),onSelect:()=>{l(e.id),c(!1)},isModelDatabaseEnabled:p},e.id)})})]})]})}),(0,i.jsx)("nav",{className:"hidden","aria-hidden":!0,children:(0,i.jsx)("ul",{children:m.map(e=>(0,i.jsx)("li",{children:(0,i.jsxs)("a",{href:(0,eU.E)(e.id),children:[e.name," by ",e.makerHumanName]})},e.id))})}),f&&(0,i.jsxs)(eo,{open:d,children:[(0,i.jsx)(en,{className:"sr-only h-9 bg-red-500 w-[290px]"}),(0,i.jsx)(es,{side:"right",align:"start",alignOffset:48,className:"hidden md:block min-h-[280px] z-10 min-w-[320px] dark:bg-black",children:(0,i.jsx)(ez,{model:g})})]})]})})}let eD=(0,r.memo)(eO,(e,a)=>e.value===a.value&&e.disabled===a.disabled&&e.isModelDatabaseEnabled===a.isModelDatabaseEnabled);function eB(e){let{model:a,hasDuplicate:t,isSelected:s,onSelect:l,onPeek:p,isModelDatabaseEnabled:m}=e,u=(0,r.useRef)(null);em(u,e=>{for(let t of e)"attributes"===t.type&&"aria-selected"===t.attributeName&&t.target instanceof Element&&t.target.getAttribute("aria-selected")&&p(a)});let d=m?et(a):a.minBillingTier;return(0,i.jsxs)(ey,{onSelect:l,ref:u,className:"text-xs aria-selected:bg-zinc-100 dark:aria-selected:bg-zinc-800 aria-selected:text-black dark:aria-selected:text-white dark:text-zinc-400",children:[(0,i.jsxs)("span",{className:"inline-flex items-center  mt-1 font-mono gap-1.5 @sm:gap-2 text-[.8rem] max-w-full",children:[(0,i.jsx)(o.L,{model:a,className:"@sm:size-4 size-4 align-middle block shrink-0"}),(0,i.jsx)("span",{className:(0,n.cn)(t?"block":"hidden @sm:block"),children:eL(a,t)}),(0,i.jsx)("span",{className:"text-xs truncate",children:a.name}),"pro"===d?(0,i.jsx)(ei.C,{variant:"pro",className:"font-sans",size:"small",children:"Pro"}):"hobby"===d?(0,i.jsx)(ei.C,{variant:"hobby",className:"font-sans",size:"small",children:"Hobby"}):null]}),(0,i.jsx)(ec.Z,{className:(0,n.cn)("ml-auto h-4 w-4",s?"opacity-100":"opacity-0")})]},a.id)}function ej(e){let{title:a,value:t}=e;return t?(0,i.jsxs)("div",{className:"flex items-start py-3 text-[.5rem]",children:[(0,i.jsx)("div",{className:"w-24 text-xs font-medium dark:text-zinc-500",children:a}),(0,i.jsx)("div",{className:"flex-1 text-xs text-zinc-600 dark:text-zinc-400",children:t})]}):null}function ez(e){var a,t,r,s,l,p,m,u,d,c,g;let{model:h}=e;return(0,i.jsxs)("div",{children:[(0,i.jsxs)("div",{className:"p-1 text-sm bg-white dark:bg-black rounded-t-lg",children:[(0,i.jsxs)("div",{className:"flex items-center",children:[(0,i.jsx)(o.L,{model:h,className:"size-4 mr-2 dark:text-white"}),(0,i.jsxs)("div",{className:"space-x-1",children:[(0,i.jsx)("span",{className:"text-zinc-600 dark:text-zinc-400",children:h.makerHumanName}),(0,i.jsx)("span",{className:"text-zinc-400 dark:text-zinc-600",children:"/"}),(0,i.jsx)("span",{className:"font-medium text-zinc-900 dark:text-zinc-100",children:h.name})]})]}),(0,i.jsx)("div",{className:"mt-4 text-xs text-zinc-500 dark:text-zinc-400",children:null===(a=h.info)||void 0===a?void 0:a.description})]}),(0,i.jsxs)("div",{className:"p-1 text-xs bg-white dark:bg-black divide-y",children:[(null===(t=h.info)||void 0===t?void 0:t.contextWindow)?(0,i.jsx)(ej,{title:"Context",value:"".concat(null===(r=h.info)||void 0===r?void 0:r.contextWindow.toLocaleString()," tokens")}):null,(null===(l=h.info)||void 0===l?void 0:null===(s=l.pricing)||void 0===s?void 0:s.inputCostPerMil)?(0,i.jsx)(ej,{title:"Input Pricing",value:"".concat((0,n.T4)(null===(m=h.info)||void 0===m?void 0:null===(p=m.pricing)||void 0===p?void 0:p.inputCostPerMil,"usd")," / million tokens")}):null,(null===(d=h.info)||void 0===d?void 0:null===(u=d.pricing)||void 0===u?void 0:u.outputCostPerMil)?(0,i.jsx)(ej,{title:"Output Pricing",value:"".concat((0,n.T4)(null===(g=h.info)||void 0===g?void 0:null===(c=g.pricing)||void 0===c?void 0:c.outputCostPerMil,"usd")," / million tokens")}):null,eT(h)?null:(0,i.jsx)(ej,{title:"Uptime",value:(0,i.jsx)(ep,{id:h.id})})]})]})}var eG=t(66529);function eW(e){let{onClick:a,canAddNewChat:t,isLoading:r}=e;return(0,i.jsx)(b.u,{delay:!0,text:t?"Add model for comparison":"Cannot add more models",position:"bottom",tabIndex:-1,desktopOnly:!0,children:(0,i.jsx)(w.Button,{type:"tertiary",size:"small",shape:"square",svgOnly:!0,"aria-label":"Add Model",disabled:r||!t,onClick:a,children:(0,i.jsx)("span",{className:"flex p-2 text-gray-900 rounded-md pointer-events-none",children:(0,i.jsx)(eG.v,{})})})})}let eV=(0,r.memo)(eW,(e,a)=>e.canAddNewChat===a.canAddNewChat&&e.isLoading===a.isLoading);var eE=t(50263),eF=t(78827),e_=t(1320);function eK(e){let{isOpen:a,setIsOpen:t,readonly:r,content:o}=e;return(0,i.jsx)(b.u,{text:"Configure model",delay:!0,position:"bottom",tabIndex:-1,forceHide:a,desktopOnly:!0,children:(0,i.jsx)("div",{children:(0,i.jsx)(eE.Z,{openPopover:a,setOpenPopover:t,align:"end",content:(0,i.jsx)("div",{className:"px-3 pt-2 pb-6 md:pt-3 md:px-4 md:w-[300px] max-w-full space-y-4",children:o}),children:(0,i.jsx)(w.Button,{svgOnly:!0,type:"tertiary",size:"small",shape:"square","aria-label":"Model Settings",onClick:e=>t(e=>!e),children:(0,i.jsx)("span",{className:"flex p-2 text-gray-900 rounded-md pointer-events-none",children:r?(0,i.jsx)(eF.k,{}):(0,i.jsx)(e_.S,{})})})})})})}let eQ=(0,r.memo)(eK,(e,a)=>e.isOpen===a.isOpen&&e.readonly===a.readonly&&e.content===a.content);var eR=t(67801),eZ=t(68077),eY=t(90610);function eJ(e){var a;let{session:t,chatState:i,isModelDatabaseEnabled:r=!1}=e,o=r?et(i):null!==(a=i.minBillingTier)&&void 0!==a?a:"hobby";return t?"pro"===o&&"hobby"===t.user.plan&&((0,eZ.mutateUpgradeGateModal)(!0),u.ZP.track("Upgrade to Pro Modal View",{isLoggedIn:!!t,...t?{userId:null==t?void 0:t.user.id,name:null==t?void 0:t.user.name}:{}}),!0):("hobby"===o&&u.ZP.track("Upgrade to Hobby Modal View"),(0,eY.mutateLoginGateModal)(!0),!0)}var e$=t(14627);function eX(e){return null!=e&&["image/jpeg","image/png"].includes(e)}let e0=4194304;function e1(e){return e<=e0}var e2=t(773),e5=t(54695);function e3(e){let{isSynced:a,setIsSynced:t}=e;return(0,i.jsxs)("div",{className:"flex items-center",children:[a&&(0,i.jsx)("div",{className:"items-center hidden h-6 px-3 mx-2 text-sm text-gray-900 bg-gray-200 rounded-full lg:flex text-md",children:"Synced"}),(0,i.jsx)(b.u,{text:"Sync chat messages with other models",delay:!0,position:"bottom",tabIndex:-1,desktopOnly:!0,children:(0,i.jsx)("div",{children:(0,i.jsx)(w.Button,{svgOnly:!0,type:"tertiary",size:"small",shape:"square","aria-label":"Model Settings",onClick:()=>t(!a),children:(0,i.jsx)("span",{className:"flex p-2 text-gray-900 rounded-md pointer-events-none",children:a?(0,i.jsx)(e5.O,{}):(0,i.jsx)(e2.z,{})})})})})]})}let e4=(0,r.memo)(e3,(e,a)=>e.isSynced===a.isSynced);var e8=t(33042),e9=t(91534);function e7(e){let{index:a,chatCount:t}=e,r=0===a,o=a===t-1,n=e=>{let t="left"===e?-1:1,i=document.querySelector("#chats-index-".concat(a+t));i&&i.scrollIntoView({behavior:"instant"})};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(b.u,{delay:!0,text:"Scroll Left",position:"bottom",tabIndex:-1,desktopOnly:!0,children:(0,i.jsx)(w.Button,{type:"tertiary",size:"small",shape:"square",svgOnly:!0,"aria-label":"Scroll Left",disabled:r,onClick:()=>n("left"),children:(0,i.jsx)("span",{className:"flex p-2 text-gray-900 rounded-md pointer-events-none",children:(0,i.jsx)(e8.X,{})})})}),(0,i.jsx)(b.u,{delay:!0,text:"Scroll Right",position:"bottom",tabIndex:-1,desktopOnly:!0,children:(0,i.jsx)(w.Button,{type:"tertiary",size:"small",shape:"square",svgOnly:!0,"aria-label":"Scroll Right",disabled:o,onClick:()=>n("right"),children:(0,i.jsx)("span",{className:"flex p-2 text-gray-900 rounded-md pointer-events-none",children:(0,i.jsx)(e9.o,{})})})})]})}var e6=t(29886),ae=t(79771),aa=t(35402),at=t(32386),ai=t(36750),ar=t(69743),ao=t(7518),an=t(52282),as=t(71773);function al(e){let{index:a,id:t,onClearChat:r}=e,o=(0,ar.e1)(),{data:n,addChat:s,removeChat:l,updateChat:p}=(0,$.Do)(),m=0===a,u=a===n.length-1;return(0,i.jsxs)(aa.F,{children:[(0,i.jsx)(ae.j,{type:"tertiary",size:"small",shape:"square",svgOnly:!0,"aria-label":"Add Model",children:(0,i.jsx)("span",{className:"flex p-2 text-gray-900 rounded-md pointer-events-none",children:(0,i.jsx)(ao.y,{})})}),(0,i.jsxs)(e6.v,{width:200,children:[(0,i.jsx)(ai.s,{prefix:(0,i.jsx)(an.w,{}),onClick:r,children:"Clear Chat"}),(0,i.jsx)(ai.s,{prefix:(0,i.jsx)(e9.o,{}),disabled:u,onClick:()=>{p(e=>{let t=[...e],i=t.splice(a,1)[0];return t.splice(a+1,0,i),t})},children:"Move Right"}),(0,i.jsx)(ai.s,{prefix:(0,i.jsx)(e8.X,{}),disabled:m,onClick:()=>{p(e=>{let t=[...e],i=t.splice(a,1)[0];return t.splice(a-1,0,i),t})},children:"Move Left"}),(0,i.jsx)(at.R,{}),(0,i.jsx)(ai.s,{prefix:(0,i.jsx)(as.r,{}),type:"error",onClick:()=>{stop(),l(t),o.message({text:"Model removed"})},children:"Delete Chat"})]})]})}let ap=(0,r.memo)(al,(e,a)=>e.id===a.id&&e.index===a.index);var am=t(79122),au=t(16302);let ad=e=>{let{setIsCodeSheetOpen:a}=e;return(0,i.jsx)(b.u,{delay:!0,text:"View code to integrate this model into your project using Vercel AI SDK",position:"bottom",tabIndex:-1,desktopOnly:!0,children:(0,i.jsx)(w.Button,{type:"tertiary",size:"small",shape:"square",svgOnly:!0,"aria-label":"Add Model",disabled:!1,onClick:()=>{a(!0)},children:(0,i.jsx)("span",{className:"flex p-2 text-gray-900 rounded-md pointer-events-none",children:(0,i.jsx)(au.d,{})})})})},ac=(0,r.memo)(ad,(e,a)=>e.setIsCodeSheetOpen===a.setIsCodeSheetOpen);var ag=t(98025);function ah(e){let{inputRef:a,disabled:t,handleInputChange:r}=e;return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(b.u,{delay:!0,text:"Attach Image",boxAlign:"right",position:"top",tabIndex:-1,desktopOnly:!0,children:(0,i.jsx)(w.Button,{type:"tertiary",typeName:"button",size:"small",disabled:t,shape:"square",svgOnly:!0,"aria-label":"Attach Image",onClick:()=>{var e;return null===(e=a.current)||void 0===e?void 0:e.click()},children:(0,i.jsx)("span",{className:"flex p-2 text-gray-900 rounded-md pointer-events-none",children:(0,i.jsx)(ag.E,{})})})}),(0,i.jsx)("input",{type:"file",className:"hidden",ref:a,onChange:r,accept:"image/jpeg, image/png, image/gif, image/webp"})]})}function av(e){let{onSelectImage:a}=e,[t,i]=(0,r.useState)(!1),o=(0,r.useRef)(null),n=(0,r.useRef)(null);async function s(e){var t;(0,u.j)("Upload"),a(null===(t=e.target.files)||void 0===t?void 0:t[0])}function l(){i(!1)}function p(e){i(!0),e.preventDefault(),e.stopPropagation(),e.dataTransfer&&(e.dataTransfer.dropEffect="copy")}let m=(0,r.useCallback)(e=>{var t,r;(0,u.j)("Drop"),e.preventDefault(),e.stopPropagation(),i(!1),a(null===(r=e.dataTransfer)||void 0===r?void 0:null===(t=r.files)||void 0===t?void 0:t[0]),n.current&&(n.current.value="")},[a]),d=(0,r.useCallback)(e=>{var t,i;(0,u.j)("Paste");let r=null===(i=e.clipboardData)||void 0===i?void 0:null===(t=i.files)||void 0===t?void 0:t[0];n.current&&(n.current.value=""),a(r)},[a]);return(0,r.useEffect)(()=>{let e=o.current;return null==e||e.addEventListener("paste",d),null==e||e.addEventListener("drop",m),null==e||e.addEventListener("dragover",p),null==e||e.addEventListener("dragleave",l),()=>{null==e||e.removeEventListener("paste",d),null==e||e.removeEventListener("drop",m),null==e||e.removeEventListener("dragover",p),null==e||e.removeEventListener("dragleave",l)}},[m,d,a]),{inputRef:n,containerRef:o,handleInputChange:s,isDraggingOver:t}}let af=(0,r.memo)(ah,(e,a)=>e.disabled===a.disabled);function aw(){let e=(0,r.useRef)(null),a=(0,r.useRef)(null);return(0,r.useEffect)(()=>{let t=e.current,i=a.current;if(t&&i){let e=new MutationObserver(()=>{i.scrollIntoView({behavior:"instant",block:"end"})});return e.observe(t,{childList:!0,subtree:!0,attributes:!0,characterData:!0}),()=>e.disconnect()}},[]),[e,a]}var ab=t(93355);let ax="pre-auth-input",ay="pre-auth-attachments";function ak(){return{savePreAuthInput:(e,a)=>{localStorage.setItem(ax,e),localStorage.setItem(ay,JSON.stringify(a))},cleanupPreAuthInput:()=>{localStorage.removeItem(ax),localStorage.removeItem(ay)}}}let aP=(0,r.createContext)(null),aC=e=>{let{children:a}=e,[t]=(0,r.useState)(()=>({id:Math.random(),emitters:new Set}));return(0,r.createElement)(aP.Provider,{value:t},a)},aM={getItem:e=>localStorage.getItem(e)};function aN(e){var a;let{enabled:t,sendMessage:i,chatModelState:o,playgroundId:n,chatId:s,chatIds:l,canUseCodeExecution:p=!1}=e,[m]=(0,J.j)(),u=(0,r.useContext)(aP);if(!u)throw Error("useSyncedMessages must be used within <SyncedMessagesProvider>");let{data:d,mutate:c}=(0,el.ZP)(["synced-input",u.id],null,{fallbackData:null!==(a=aM.getItem(ax))&&void 0!==a?a:""}),{data:g,mutate:h}=(0,el.ZP)(["synced-attachments",u.id],null,{fallbackData:(()=>{let e=aM.getItem(ay);return e?JSON.parse(e):[]})()}),{data:v,mutate:f}=(0,el.ZP)(["synced-enqueue-submit",u.id],null,{fallbackData:!1}),w=(0,r.useRef)(t);return(0,r.useEffect)(()=>{w.current=t},[t]),(0,r.useEffect)(()=>{let e=(e,a,t)=>{if("submit-synced-message"===e&&w.current&&a&&"user"===a.role){var r,u,d,c,g,h,v,f,b,x;let e={...null!==(b=null==t?void 0:t.headers)&&void 0!==b?b:{},"Content-Type":"application/json","Custom-Encoding":m},w=null!==(x=null==t?void 0:t.body)&&void 0!==x?x:o?{playgroundId:n,chatId:s,chatIds:l,model:o.id,temperature:null===(r=o.parameters.temperature)||void 0===r?void 0:r.value,maxTokens:o.parameters.maximumLength.value,topK:null===(u=o.parameters.topK)||void 0===u?void 0:u.value,topP:null===(d=o.parameters.topP)||void 0===d?void 0:d.value,frequencyPenalty:null===(c=o.parameters.frequencyPenalty)||void 0===c?void 0:c.value,presencePenalty:null===(g=o.parameters.presencePenalty)||void 0===g?void 0:g.value,stopSequences:null===(h=o.parameters.stopSequences)||void 0===h?void 0:h.value,typicalP:null===(v=o.parameters.typicalP)||void 0===v?void 0:v.value,repetitionPenalty:null===(f=o.parameters.repetitionPenalty)||void 0===f?void 0:f.value,isCodeExecutionEnabled:p}:{};i(a,{...null!=t?t:{},headers:e,body:w})}};return u.emitters.add(e),()=>{u.emitters.delete(e)}},[u.emitters,o,i,n,s,l,p,m]),{submitMessage:(0,r.useCallback)((e,a)=>{u.emitters.forEach(t=>{t("submit-synced-message",e,a)})},[u.emitters]),input:null!=d?d:"",setInput:c,attachments:g,setAttachments:h,enqueueSubmit:v,setEnqueueSubmit:f}}var aA=t(97717);let aS={alibaba:{termsOfServiceUrl:"https://www.alibabacloud.com/help/en/legal/latest/alibaba-cloud-international-website-product-terms-of-service-v-3-8-0",privacyPolicyUrl:"https://www.alibabacloud.com/help/en/legal/latest/alibaba-cloud-international-website-privacy-policy"},anthropic:{termsOfServiceUrl:"https://www.anthropic.com/legal/commercial-terms",privacyPolicyUrl:"https://privacy.anthropic.com/en/"},azure:{termsOfServiceUrl:"https://learn.microsoft.com/en-us/legal/cognitive-services/openai/code-of-conduct",privacyPolicyUrl:"https://privacy.microsoft.com/en-us/privacystatement"},bedrock:{termsOfServiceUrl:"https://aws.amazon.com/service-terms/",privacyPolicyUrl:"https://aws.amazon.com/privacy/"},baseten:{termsOfServiceUrl:"https://www.baseten.co/terms-and-conditions/",privacyPolicyUrl:"https://www.baseten.co/privacy-policy/"},cerebras:{termsOfServiceUrl:"https://www.cerebras.ai/terms-of-service",privacyPolicyUrl:"https://www.cerebras.ai/privacy-policy"},chutes:{termsOfServiceUrl:"https://chutes.ai/terms",privacyPolicyUrl:"https://chutes.ai/privacy"},cohere:{termsOfServiceUrl:"https://cohere.com/terms-of-use",privacyPolicyUrl:"https://cohere.com/privacy"},deepinfra:{termsOfServiceUrl:"https://deepinfra.com/terms",privacyPolicyUrl:"https://deepinfra.com/privacy"},deepseek:{termsOfServiceUrl:"https://cdn.deepseek.com/policies/en-US/deepseek-terms-of-use.html",privacyPolicyUrl:"https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy.html"},fireworks:{termsOfServiceUrl:"https://fireworks.ai/terms-of-service",privacyPolicyUrl:"https://fireworks.ai/privacy-policy"},google:{termsOfServiceUrl:"https://policies.google.com/terms/generative-ai",privacyPolicyUrl:"https://policies.google.com/privacy"},groq:{termsOfServiceUrl:"https://console.groq.com/docs/terms-of-sale",privacyPolicyUrl:"https://groq.com/privacy-policy"},inception:{termsOfServiceUrl:"https://www.inceptionlabs.ai/terms",privacyPolicyUrl:"https://www.inceptionlabs.ai/terms"},mistral:{termsOfServiceUrl:"https://mistral.ai/terms",privacyPolicyUrl:"https://mistral.ai/terms#privacy-policy"},moonshotai:{termsOfServiceUrl:"https://platform.moonshot.ai/docs/agreement/modeluse.en-US",privacyPolicyUrl:"https://platform.moonshot.ai/docs/agreement/userprivacy.en-US"},morph:{termsOfServiceUrl:"https://www.morph.ai/terms",privacyPolicyUrl:"https://www.morph.ai/terms"},novita:{termsOfServiceUrl:"https://novita.ai/legal/terms-of-service",privacyPolicyUrl:"https://novita.ai/legal/privacy-policy"},openai:{termsOfServiceUrl:"https://openai.com/policies/terms-of-use",privacyPolicyUrl:"https://openai.com/policies/privacy-policy"},parasail:{termsOfServiceUrl:"https://www.parasail.io/legal/terms",privacyPolicyUrl:"https://www.saas.parasail.io/assets/privacy-policy.html"},perplexity:{termsOfServiceUrl:"https://www.perplexity.ai/terms",privacyPolicyUrl:"https://www.perplexity.ai/privacy"},vercel:{termsOfServiceUrl:"https://vercel.com/legal/terms",privacyPolicyUrl:"https://vercel.com/legal/privacy-policy"},vertex:{termsOfServiceUrl:"https://cloud.google.com/terms/service-terms",privacyPolicyUrl:"https://cloud.google.com/privacy"},vertexAnthropic:{termsOfServiceUrl:"https://cloud.google.com/terms",privacyPolicyUrl:"https://cloud.google.com/terms/cloud-privacy-notice"},xai:{termsOfServiceUrl:"https://x.ai/legal/terms-of-service",privacyPolicyUrl:"https://x.ai/legal/privacy-policy"},zai:{termsOfServiceUrl:"https://docs.z.ai/legal-agreement/terms-of-use",privacyPolicyUrl:"https://docs.z.ai/legal-agreement/privacy-policy"}},aT=(0,r.memo)(function(e){var a,t,r,s,l,p,m,u,d,c,g,h,v,f,w,b,x,y,k,P,C,M;let{model:N}=e;return(0,i.jsx)("div",{className:"flex items-center justify-center size-full px-4",children:(0,i.jsxs)("div",{className:"w-full max-w-2xl border rounded-lg shadow-xs",children:[(0,i.jsxs)("div",{className:"px-6 pt-5 text-sm bg-white dark:bg-black rounded-t-lg",children:[(0,i.jsxs)("div",{className:"flex items-center",children:[(0,i.jsx)(o.L,{model:N,className:"size-4 mr-2"}),(0,i.jsxs)("div",{className:"space-x-1",children:[(0,i.jsx)("span",{className:"text-zinc-600 dark:text-zinc-400",children:N.makerHumanName}),(0,i.jsx)("span",{className:"text-zinc-400 dark:text-zinc-600",children:"/"}),(0,i.jsx)("span",{className:"font-medium text-zinc-900 dark:text-zinc-100",children:N.name})]})]}),(null===(a=N.info)||void 0===a?void 0:a.description)?(0,i.jsx)("div",{className:"mt-4 text-xs text-zinc-500 dark:text-zinc-400",children:null===(t=N.info)||void 0===t?void 0:t.description}):null]}),(0,i.jsxs)("div",{className:"px-6 py-5 text-xs bg-white dark:bg-black divide-y",children:[(null===(r=N.info)||void 0===r?void 0:r.contextWindow)?(0,i.jsx)(aU,{title:"Context",value:"".concat(null===(s=N.info)||void 0===s?void 0:s.contextWindow.toLocaleString()," tokens")}):null,(null===(p=N.info)||void 0===p?void 0:null===(l=p.pricing)||void 0===l?void 0:l.inputCostPerMil)?(0,i.jsx)(aU,{title:"Input Pricing",value:"".concat((0,n.T4)(null===(u=N.info)||void 0===u?void 0:null===(m=u.pricing)||void 0===m?void 0:m.inputCostPerMil,"usd")," / million tokens")}):null,(null===(c=N.info)||void 0===c?void 0:null===(d=c.pricing)||void 0===d?void 0:d.outputCostPerMil)?(0,i.jsx)(aU,{title:"Output Pricing",value:"".concat((0,n.T4)(null===(h=N.info)||void 0===h?void 0:null===(g=h.pricing)||void 0===g?void 0:g.outputCostPerMil,"usd")," / million tokens")}):null]}),(0,i.jsx)("div",{className:"px-6 py-5 text-xs font-medium border-t rounded-b-lg bg-zinc-100/75 dark:bg-zinc-900/75 h-28 lg:h-auto",children:(0,i.jsxs)("div",{className:"flex items-start lg:items-center justify-between",children:[(0,i.jsxs)("div",{className:"flex flex-col lg:flex-row lg:items-center justify-between gap-2 lg:gap-4",children:[(null===(v=N.info)||void 0===v?void 0:v.modelUrl)?(0,i.jsxs)("a",{href:null===(f=N.info)||void 0===f?void 0:f.modelUrl,className:"inline-flex items-center text-zinc-500 dark:text-zinc-400",children:[(0,i.jsx)("span",{children:"Model Page"}),(0,i.jsx)(aA.h,{className:"size-3 ml-1","aria-hidden":"true"})]}):null,(null===(w=N.info)||void 0===w?void 0:w.datasetUrl)?(0,i.jsxs)("a",{href:null===(b=N.info)||void 0===b?void 0:b.datasetUrl,className:"inline-flex items-center text-zinc-500 dark:text-zinc-400",children:[(0,i.jsx)("span",{children:"Dataset Page"}),(0,i.jsx)(aA.h,{className:"size-3 ml-1","aria-hidden":"true"})]}):null,(null===(y=N.info)||void 0===y?void 0:null===(x=y.pricing)||void 0===x?void 0:x.pricingUrl)?(0,i.jsxs)("a",{href:null===(P=N.info)||void 0===P?void 0:null===(k=P.pricing)||void 0===k?void 0:k.pricingUrl,className:"inline-flex items-center text-zinc-500 dark:text-zinc-400",children:[(0,i.jsx)("span",{children:"Pricing"}),(0,i.jsx)(aA.h,{className:"size-3 ml-1","aria-hidden":"true"})]}):null]}),(0,i.jsxs)("div",{className:"flex flex-col lg:flex-row lg:items-center justify-between gap-2 lg:gap-4",children:[aS[N.provider]?(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)("a",{href:aS[N.provider].termsOfServiceUrl,className:"inline-flex items-center text-zinc-500 dark:text-zinc-400",children:[(0,i.jsx)("span",{children:"Terms"}),(0,i.jsx)(aA.h,{className:"size-3 ml-1","aria-hidden":"true"})]}),(0,i.jsxs)("a",{href:aS[N.provider].privacyPolicyUrl,className:"inline-flex items-center text-zinc-500 dark:text-zinc-400",children:[(0,i.jsx)("span",{children:"Privacy"}),(0,i.jsx)(aA.h,{className:"size-3 ml-1","aria-hidden":"true"})]})]}):null,(null===(C=N.info)||void 0===C?void 0:C.website)?(0,i.jsxs)("a",{href:null===(M=N.info)||void 0===M?void 0:M.website,className:"inline-flex items-center text-zinc-500 dark:text-zinc-400",children:[(0,i.jsx)("span",{children:"Website"}),(0,i.jsx)(aA.h,{className:"size-3 ml-1","aria-hidden":"true"})]}):null]})]})})]})})});function aU(e){let{title:a,value:t}=e;return t?(0,i.jsxs)("div",{className:"flex items-start py-3",children:[(0,i.jsx)("div",{className:"font-medium w-28",children:a}),(0,i.jsx)("div",{className:"flex-1 text-zinc-600 dark:text-zinc-400",children:t})]}):null}aT.displayName="ModelCard";var aq=t(54568),aH=t(24904),aI=t(12001),aL=t(886),aO=t(28378);function aD(e){let{variant:a}=e,{resolvedTheme:t}=(0,aH.F)(),r=(0,aO.usePathname)();return(0,i.jsxs)(aa.F,{children:[(0,i.jsxs)(ae.j,{className:(0,n.cn)("!text-xs text-gray-900 hover:opacity-80 transition-opacity",{"lg:!hidden":"model-card"===a,"!hidden":!r.startsWith("/playground")}),variant:"unstyled",children:["light"===t?(0,i.jsx)(aI.d,{size:14}):(0,i.jsx)(aL.F,{size:14}),(0,i.jsx)("span",{className:"ml-2",children:"Legal"})]}),(0,i.jsxs)(e6.v,{width:200,children:[(0,i.jsx)(aq.U,{href:"https://vercel.com/legal/terms",children:"Terms of Service"}),(0,i.jsx)(aq.U,{href:"https://vercel.com/legal/privacy",children:"Privacy Policy"})]})]})}let aB=(0,e$.default)(()=>Promise.all([t.e(4030),t.e(465),t.e(8332),t.e(6214)]).then(t.bind(t,44324)).then(e=>e.CodeGeneratorModal),{loadableGenerated:{webpack:()=>[44324]},ssr:!1});function aj(e){var a,t;let{id:l,index:p,requestCount:m,chatCount:c,session:h,readonly:v,initialChatData:f,chatAvatar:w,playgroundId:b,isModelDatabaseEnabled:x}=e,k=l,{modelsById:C}=(0,eq.v)(),{chatModelState:M,updateChatModelState:N,updateChatModelStateParameter:A}=(0,$.RN)(b,k,f),{synced:S,...T}=null!=M?M:{},U=null==M?void 0:M.supportsVision,q=null==M?void 0:M.supportsToolCalling,{inputRef:H,containerRef:I,handleInputChange:L,isDraggingOver:O}=av({onSelectImage:eL}),{refreshHistory:D}=(0,am.v)(),{data:B,addChat:z}=(0,$.Do)(),[W,V]=(0,r.useState)([]),[E,F]=(0,J.j)(),[_,Q]=(0,r.useState)(!1),[ee,ea]=(0,r.useState)(!1),{setShowUpgradeGateModal:et}=(0,eZ.useUpgradeGateModal)(),{setShowLoginGateModal:ei}=(0,eY.useLoginGateModal)(),{mutate:er}=(0,el.ZP)("showTokenExpiredModal",null),{mutate:eo}=(0,el.ZP)("showLimitModal",null),{data:en,mutate:es}=(0,el.ZP)("reqAmount",null,{fallbackData:m}),ep=(0,r.useRef)(null),{formRef:em,onKeyDown:eu}=(0,d.F)(),ed=B.length<8,ec=0===p,eg=(0,r.useRef)(null),[eh,ev]=(0,r.useState)(!1),ef="".concat(b,"_").concat(k),{messages:ew,sendMessage:eb,regenerate:ex,status:ey,setMessages:ek,stop:eC}=(0,Z.RJ)({id:ef,experimental_throttle:250,messages:null!==(a=null==f?void 0:f.messages)&&void 0!==a?a:[],onFinish:async e=>{var a,t,i;let{message:r}=e;F();let o=en||0,n=null===(a=r.metadata)||void 0===a?void 0:a.rateLimit.count,s=null===(t=r.metadata)||void 0===t?void 0:t.rateLimit.limit;if(n&&s&&(o=Math.max(o,Number.parseInt(n,10))),es(o),(null==h?void 0:h.user)&&b&&ec){let e=null===(i=r.metadata)||void 0===i?void 0:i.shareId;e&&window.history.replaceState(null,"","/playground/s/".concat(e)),D()}},onError:e=>{let a=en||0;if("token expired"===e.message){er(!0),(0,u.j)("Token Expired Modal",{isLoggedIn:!!h,...h?{userId:null==h?void 0:h.user.id,name:null==h?void 0:h.user.name}:{},reqAmount:a});return}if("Rate limit exceeded"===e.message){eo(!0),(0,u.j)("Rate Limit Reached",{isLoggedIn:!!h,...h?{userId:null==h?void 0:h.user.id,name:null==h?void 0:h.user.name}:{},reqAmount:a});return}if("Timeout"===e.message){eR.ZP.error("The LLM provider took too long to respond");return}if("Prompt too long"===e.message){eR.ZP.error("Prompt size is too large, please try again with a shorter prompt");return}if("Login or signup"===e.message){(null==h?void 0:h.user)?"hobby"===h.user.plan&&et(!0):ei(!0);return}if("Not authorized"!==e.message&&"Rate limit exceeded"!==e.message){if("Upgrade to pro"===e.message){et(!0);return}"Missing model"!==e.message&&eR.ZP.error("Something went wrong.")}},transport:new ab.PD({api:"/api/generate",fetch:(e,a)=>window.fetch(e,a)})}),[eM,eN]=(0,r.useState)(""),eA="ready"!==ey,eS=aN({sendMessage:eb,enabled:null==M?void 0:M.synced,chatModelState:M,chatId:l,playgroundId:b,chatIds:B,canUseCodeExecution:q}),{savePreAuthInput:eT,cleanupPreAuthInput:eU}=ak(),[eH,eI]=aw();async function eL(e){var a;if(!e)return;if(!eX(e.type))return eR.ZP.error("Unsupported format. Only JPEG, PNG files are supported.");if(!e1(e.size))return eR.ZP.error("Image too large, maximum file size is 4MB.");if(W.length>=3||eS.attachments&&(null===(a=eS.attachments)||void 0===a?void 0:a.length)>=3)return eR.ZP.error("Only 3 images can be uploaded at a time.");let t=(0,Y.Z)();(null==M?void 0:M.synced)?eS.setAttachments(a=>[...null!=a?a:[],{id:t,url:URL.createObjectURL(e),status:"uploading",contentType:e.type,name:e.name}]):V(a=>[...a,{id:t,url:URL.createObjectURL(e),status:"uploading",contentType:e.type,name:e.name}]);let i=await fetch("/api/upload",{method:"POST",body:e,headers:{"Content-Type":e.type}});i.ok||eR.ZP.error("Error uploading image");let{url:r,contentType:o,pathname:n}=await i.json();(null==M?void 0:M.synced)?eS.setAttachments(e=>{var a;return[...null!==(a=null==e?void 0:e.filter(e=>e.id!==t))&&void 0!==a?a:[],{id:t,url:r,status:"uploaded",contentType:o,name:n}]}):V(e=>[...e.filter(e=>e.id!==t),{id:t,url:r,status:"uploaded",contentType:o,name:n}])}let eO=(0,r.useRef)(null);(0,r.useEffect)(()=>{let e=e=>{if("/"===e.key&&document.activeElement!==ep.current){var a;e.preventDefault(),null===(a=ep.current)||void 0===a||a.focus()}},a=eO.current;return null==a||a.addEventListener("keydown",e),()=>{null==a||a.removeEventListener("keydown",e)}},[eO]);let eB=(null==M?void 0:M.synced)?null!==(t=eS.attachments)&&void 0!==t?t:[]:W,ej=!1,ez=(0,r.useCallback)(e=>{eS.setEnqueueSubmit(e),ev(e)},[eS]),eG=(0,r.useCallback)(()=>{var e,a,t,i,r,o,n,s,p,m;if(!M)return null;let u=M.synced?eS.input:eM;if(!u.trim())return;if(ej){ez(!0);return}if(ez(!1),h||eT(u,M.synced?null!==(p=eS.attachments)&&void 0!==p?p:[]:W),eJ({chatState:M,session:h,isModelDatabaseEnabled:x}))return;eU();let d={headers:{"Content-Type":"application/json","Custom-Encoding":E},body:{playgroundId:b,chatId:l,chatIds:B,model:M.id,temperature:null===(e=M.parameters.temperature)||void 0===e?void 0:e.value,maxTokens:M.parameters.maximumLength.value,topK:null===(a=M.parameters.topK)||void 0===a?void 0:a.value,topP:null===(t=M.parameters.topP)||void 0===t?void 0:t.value,frequencyPenalty:null===(i=M.parameters.frequencyPenalty)||void 0===i?void 0:i.value,presencePenalty:null===(r=M.parameters.presencePenalty)||void 0===r?void 0:r.value,stopSequences:null===(o=M.parameters.stopSequences)||void 0===o?void 0:o.value,typicalP:null===(n=M.parameters.typicalP)||void 0===n?void 0:n.value,repetitionPenalty:null===(s=M.parameters.repetitionPenalty)||void 0===s?void 0:s.value,isCodeExecutionEnabled:q}};M.synced?(eS.submitMessage({id:(0,Y.Z)(),role:"user",parts:[{type:"text",text:u},...(null===(m=eS.attachments)||void 0===m?void 0:m.length)?[...eS.attachments.map(e=>{var a;return{type:"file",mediaType:null!==(a=e.contentType)&&void 0!==a?a:"unknown/type",url:e.url}})]:[]]},d),eS.setInput(""),eS.setAttachments([])):(eb({id:(0,Y.Z)(),role:"user",parts:[{type:"text",text:u},...W.length?W.map(e=>{var a;return{type:"file",mediaType:null!==(a=e.contentType)&&void 0!==a?a:"image/jpg",url:e.url}}):[]]},d),eN(""),V([]))},[M,eS,eM,ej,h,x,eU,ez,eT,W,eb,eN]),eW=(0,r.useCallback)(e=>{(null==M?void 0:M.synced)?eS.setAttachments(a=>null==a?void 0:a.filter(a=>a.url!==e.url)):V(a=>a.filter(a=>a.url!==e.url))},[M,eS]),eE=(null==M?void 0:M.synced)?eS.enqueueSubmit:eh;(0,r.useEffect)(()=>{!ej&&W.length>0&&eh&&(ez(!1),eG())},[eG,W,eh,ej,ez]);let eF=(0,r.useMemo)(()=>(0,i.jsx)(o.L,{model:T,className:"size-5"}),[T]),e_=(0,r.useCallback)((e,a)=>{let t=(0,G.RR)(e)===j,r=a===ew.length-1,o=eA&&r;return(0,i.jsx)(R,{message:e,modelName:(null==M?void 0:M.name)||"",modelId:(null==M?void 0:M.id)||"",messageIndex:a,readonly:v,modelIcon:eF,chatAvatar:w,modelSupportsVision:!!(null==M?void 0:M.supportsVision),onSubmit:t=>{if(v)return;let i={...e,id:(0,Y.Z)(),parts:[{type:"text",text:t}]};ek(e=>[...e.slice(0,a),i]),ex()},isLastMessage:a===ew.length-1,supportsDownvote:(null==M?void 0:M.provider)==="vercel",messages:ew,chatId:l,playgroundId:null!=b?b:"",session:h,messageLoading:o||t},"".concat(e.id,"-").concat(a))},[eF,w,M,l,eA,ew,b,v,ex,h,ek]);if(!M)return null;let eK=(0,i.jsxs)("div",{ref:U?I:void 0,className:"size-full rounded-md border border-gray-alpha-400",children:[(0,i.jsxs)(s,{ref:eg,shouldAutoScroll:!v,id:"".concat(b,"_").concat(l),className:"rounded-md",overscrollBehavior:"auto",header:(0,i.jsxs)("div",{className:"flex items-center bg-background-200 backdrop-blur shadow-[0_1px_rgba(202,206,214,.3),0_5px_10px_-5px_rgba(0,0,0,.05)] dark:shadow-[0_1px_rgba(255,255,255,0.15)] justify-between py-3 pl-3 pr-2",children:[(0,i.jsx)("div",{className:"flex-1 min-w-0",children:(0,i.jsx)(eD,{value:M.id,disabled:v,isModelDatabaseEnabled:x,onValueChange:e=>{var a;v||(eC(),N(a=>({...C[e],synced:a.synced,isCodeExecutionEnabled:a.isCodeExecutionEnabled})),null===(a=ep.current)||void 0===a||a.focus())}})}),(0,i.jsxs)("div",{className:"flex items-center ml-1",children:[v||!(c>1)||M.disabled?null:(0,i.jsx)(e4,{isSynced:M.synced,setIsSynced:e=>{if(e)(eM||W.length>0)&&(eS.setInput(eM),eS.setAttachments(W));else{var a;eN(eS.input),V(null!==(a=eS.attachments)&&void 0!==a?a:[])}N({...M,synced:e},!0)}}),(0,i.jsx)("div",{className:"hidden lg:block",children:(0,i.jsx)(ac,{setIsCodeSheetOpen:ea})}),(0,i.jsx)(eQ,{content:(0,i.jsx)(X.JC,{readonly:v,model:M,updateModelParameter_:A}),isOpen:_,setIsOpen:Q,readonly:v}),!v&&(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(eV,{onClick:()=>{z($.JI),eR.ZP.success("Model added",{position:"bottom-center"})},canAddNewChat:ed,isLoading:eA}),(0,i.jsx)(ap,{index:p,id:l,onClearChat:()=>ek([])})]}),(0,i.jsx)("span",{className:"flex lg:hidden",children:(0,i.jsx)(e7,{index:p,chatCount:c})})]})]}),footer:v?null:(0,i.jsxs)("div",{className:"p-3 pr-2.5 bg-background-200 border-t border-gray-alpha-400 flex items-center gap-2",onClick:e=>{ep.current&&e.target===e.currentTarget&&ep.current.focus()},children:[(0,i.jsxs)("form",{ref:em,onSubmit:e=>{var a;if(e.preventDefault(),null===(a=eg.current)||void 0===a||a.startAutoScroll(),eA)return eC();eG()},className:"relative flex items-center w-full",children:[(0,i.jsx)("div",{className:"absolute left-3 top-3 flex flex-row gap-3",children:U&&eB.length>0?eB.map(e=>(0,i.jsx)(K,{attachment:e,removeAttachment:eW},e.name)):null}),(0,i.jsx)("div",{className:"absolute inline-flex items-center justify-end bottom-[3px] left-1 gap-1",children:U&&(0,i.jsx)(af,{disabled:!!eE,inputRef:H,handleInputChange:L})}),(0,i.jsx)(g,{disabled:M.disabled||eE,value:M.disabled?void 0:M.synced?eS.input:eM,onKeyDown:eu,onChange:e=>M.synced?eS.setInput(e.target.value):eN(e.target.value),placeholder:M.disabled?"This model has been removed":"Type your message…",className:(0,n.cn)("rounded-md min-h-[38px] flex-1 max-h-[30vh] text-sm placeholder-gray-600 placeholder-opacity-100 bg-background-100 border border-gray-alpha-400 resize-none scroll-m-2 h-[34px] focus:border-zinc-400 dark:focus:border-zinc-600 focus:ring-0 focus:outline-none",{"pt-24":U&&eB.length>0,"pr-9":!0,"pl-9":U,"bg-gray-alpha-100":eE}),style:{fontFamily:P.W3},autoFocus:!eP.tq&&!M.disabled&&ec}),(0,i.jsx)("div",{className:"absolute inline-flex items-center justify-end bottom-[3px] right-1",children:(0,i.jsx)(y,{onStop:eC,className:"self-end",loading:eA,disabled:!eA&&(M.synced?0===eS.input.length:0===eM.length)||eE,onClick:()=>{var e;(0,u.j)("generate text clicked",{type:"chat",model:M.id,userId:(null==h?void 0:null===(e=h.user)||void 0===e?void 0:e.id)||"anonymous"})}})})]}),(0,i.jsx)(aD,{variant:"model-card"})]}),children:[(0,i.jsx)("div",{className:(0,n.cn)("absolute h-full flex justify-center items-center w-full transition-opacity duration-300 ease-in-out z-40 bg-black",{"bg-opacity-50 opacity-50":O,"pointer-events-none bg-opacity-0 opacity-0":!O}),children:(0,i.jsx)("div",{className:"p-6 text-lg font-medium text-center text-white pointer-events-none",children:(0,i.jsx)("p",{children:"Drop Image"})})}),(0,i.jsxs)("div",{className:(0,n.cn)("h-full divide-y ",{"pb-0":ew.length}),children:[!ew.length&&eA?(0,i.jsx)("span",{className:"text-zinc-500",children:"Loading..."}):null,ew&&ew.length>0?(0,i.jsxs)("div",{className:"h-full",ref:eH,children:[az(ew,eA).map((e,a)=>e_(e,a)),(0,i.jsx)("div",{ref:eI,className:"h-20"})]}):(0,i.jsx)(aT,{model:M})]})]},l+!!ew.length),(0,i.jsx)(aB,{isOpen:ee,setIsOpen:ea,model:T,chatOrCompletion:"chat"})]});return v?(0,i.jsx)("div",{id:"chats-index-".concat(p),className:"@container shrink-0 lg:shrink snap-center min-w-96 min-h-[250px] bg-background-100 size-full",children:eK}):(0,i.jsx)("div",{id:"chats-index-".concat(p),className:"@container shrink-0 lg:shrink lg:min-w-96 snap-center rounded-md min-h-[250px] bg-background-100 size-full",ref:eO,tabIndex:-1,children:eK})}function az(e,a){if(!a)return e;let t=e[e.length-1];return t&&"user"===t.role?[...e,z()]:e}function aG(){return(0,i.jsx)("div",{className:"w-full border rounded-md border-gray-alpha-400 bg-background-100",children:(0,i.jsx)("div",{className:"flex w-full h-14 rounded-t-md items-center bg-background-200 backdrop-blur shadow-[0_1px_rgba(202,206,214,.3),0_5px_10px_-5px_rgba(0,0,0,.05)] dark:shadow-[0_1px_rgba(255,255,255,0.15)] justify-between py-3 pl-3 pr-2"})})}function aW(e){let{session:a,chatIds:t,requestCount:r,initialChatData:o,chatAvatar:n,readonly:s,playgroundId:l,isModelDatabaseEnabled:p}=e,u=t.length?(0,i.jsx)(m,{ids:t.map(e=>"".concat(l,"_").concat(e)),shouldAutoScroll:!s,children:t.map((e,m)=>{var u;return(0,i.jsx)(aj,{id:e,index:m,requestCount:r,session:a,readonly:s,playgroundId:l,chatCount:t.length,initialChatData:(0,G.qO)(null==o?void 0:null===(u=o[e])||void 0===u?void 0:u.data),chatAvatar:n,isModelDatabaseEnabled:p},e)})}):(0,i.jsx)(i.Fragment,{children:(0,i.jsx)(aG,{})});return(0,i.jsx)("div",{className:"flex flex-col flex-1 h-full overflow-x-auto bg-background-100",children:(0,i.jsx)(aC,{children:(0,i.jsx)("div",{className:"flex size-full p-2 space-x-2 overflow-x-auto snap-x snap-mandatory lg:snap-none lg:overflow-y-hidden",children:u})})})}aj.displayName="Completion"},90610:(e,a,t)=>{"use strict";t.r(a),t.d(a,{LoginGateModal:()=>u,mutateLoginGateModal:()=>m,useLoginGateModal:()=>p});var i=t(66195),r=t(54113),o=t(97137),n=t(50130),s=t(39960),l=t(49458);function p(){let{data:e,mutate:a}=(0,s.ZP)("showLoginGateModal",null,{fallbackData:!1});return{showLoginGateModal:e,setShowLoginGateModal:a}}function m(e){return(0,l.j)("showLoginGateModal",e)}function u(){let{showLoginGateModal:e,setShowLoginGateModal:a}=p();return(0,i.jsx)(o.Z,{showModal:e,setShowModal:a,children:(0,i.jsxs)("div",{className:"w-full overflow-hidden md:max-w-md",children:[(0,i.jsxs)("div",{className:"flex flex-col items-center justify-center px-4 py-6 pt-8 space-y-3 text-center bg-white border-b border-gray-200 dark:bg-black dark:border-gray-200 md:px-16",children:[(0,i.jsx)("a",{href:"https://vercel.com",children:(0,i.jsx)(r.Y,{className:"size-10"})}),(0,i.jsx)("h3",{className:"text-2xl font-bold tracking-tight font-display",children:"Sign in to Vercel"}),(0,i.jsx)("p",{className:"text-sm font-medium tracking-tight text-gray-700",children:"To gain access to advanced models and more free credits, sign in to Vercel."})]}),(0,i.jsx)("div",{className:"flex flex-col px-4 py-8 space-y-4 bg-gray-50 dark:bg-neutral-950 md:px-16",children:(0,i.jsx)(n.m,{})})]})})}},68077:(e,a,t)=>{"use strict";t.r(a),t.d(a,{UpgradeGateModal:()=>d,mutateUpgradeGateModal:()=>u,useUpgradeGateModal:()=>m});var i=t(66195),r=t(39960),o=t(49458),n=t(43465),s=t(45654),l=t(54113),p=t(97137);function m(){let{data:e,mutate:a}=(0,r.ZP)("showUpgradeGateModal",null,{fallbackData:!1});return{showUpgradeGateModal:e,setShowUpgradeGateModal:a}}function u(e){return(0,o.j)("showUpgradeGateModal",e)}function d(e){let{streamOnClick:a}=e,{showUpgradeGateModal:t,setShowUpgradeGateModal:r}=m();async function o(){await (0,s.w)(),await (0,n.z)()}return(0,i.jsx)(p.Z,{showModal:t,setShowModal:r,children:(0,i.jsxs)("div",{className:"w-full overflow-hidden shadow-xl md:max-w-md",children:[(0,i.jsxs)("div",{className:"flex flex-col items-center justify-center px-4 py-6 pt-8 space-y-3 text-center bg-white border-b border-gray-200 dark:bg-black md:px-16",children:[(0,i.jsx)("a",{href:"https://vercel.com",children:(0,i.jsx)(l.Y,{className:"size-10"})}),(0,i.jsx)("h3",{className:"text-2xl font-bold font-display",children:"Upgrade to Vercel Pro"}),(0,i.jsx)("p",{className:"text-sm font-medium tracking-tight text-gray-700",children:"To gain access to premium models and more credits, upgrade your Vercel account to a paid Pro account."})]}),(0,i.jsxs)("div",{className:"flex flex-col px-4 py-8 space-y-4 bg-gray-50 dark:bg-gray-100 md:px-16",children:[(0,i.jsxs)("a",{className:"flex items-center justify-center w-full h-10 space-x-3 text-sm text-white transition-all duration-75 bg-black border rounded-md shadow-sm dark:bg-white dark:text-black focus:outline-none",href:"https://vercel.com/dashboard?createProTeam=source-for-analytics",target:"_blank",onClick:()=>{a("Upgrade to Vercel Pro")},children:[(0,i.jsx)(l.Y,{className:"size-5 mr-0.5 -ml-1"}),(0,i.jsx)("span",{className:"font-semibold",children:"Upgrade to Vercel Pro"})]}),(0,i.jsx)("button",{className:"text-sm text-blue-600",onClick:o,type:"button",children:"Already Pro? Click here to login again."})]})]})})}},50220:(e,a,t)=>{"use strict";t.d(a,{PageviewTracker:()=>o});var i=t(73762);async function r(e){let a={...e,page_path:window.location.href,page_title:document.title,query_string:window.location.search||null,referrer:document.referrer||null},t=await fetch("/api/track",{method:"POST",headers:{"content-type":"application/json"},body:JSON.stringify(a)});t.ok||console.error("Failed to send event to Snowflake",t)}function o(){return(0,i.useEffect)(()=>{r({action:"pageview"})},[]),null}},73541:(e,a,t)=>{"use strict";t.r(a),t.d(a,{default:()=>U,programmingLanguages:()=>A});var i=t(66195),r=t(73762),o=t(14627),n=t(48565),s=t(60038),l=t(76404),p=t(81045),m=t(56699),u=t(10746),d=t(96915),c=t(46681),g=t(15727),h=t.n(g);let v=Array(12).fill(0),f=e=>"number"==typeof e?"".concat(e,"px"):e;function w(e){let{color:a,className:t,size:r=20,wrapperClassName:o}=e;return(0,i.jsx)("div",{className:(0,c.W)(h().wrapper,o),"data-version":"v1",style:{"--spinner-size":f(r),"--spinner-color":a},children:(0,i.jsx)("div",{className:(0,c.W)(h().spinner,t),children:v.map((e,a)=>(0,i.jsx)("div",{className:h().bar},"spinner-bar-".concat(a)))})})}var b=t(67801);let x={javascript:"JavaScript",typescript:"TypeScript",jsx:"jsx",tsx:"tsx",html:"HTML",css:"css",python:"Python",java:"Java",c:"C",cpp:"C++","c++":"C++","c#":"C#",ruby:"Ruby",php:"PHP",swift:"Swift","objective-c":"Objective-C",kotlin:"Kotlin",go:"Go",perl:"Perl",rust:"Rust",scala:"Scala",haskell:"Haskell",lua:"Lua",shell:"Shell",sql:"SQL",json:"JSON",yaml:"YAML",markdown:"Markdown"},y="File";function k(e){let a=null==e?void 0:e.toLowerCase();return a?x[a]||(e?e.charAt(0).toUpperCase()+e.slice(1):y):y}let P=(0,o.default)(()=>Promise.all([t.e(4756),t.e(3054),t.e(4059),t.e(3739),t.e(7360),t.e(4141),t.e(3233),t.e(8841),t.e(9020)]).then(t.bind(t,9020)).then(e=>e.SyntaxHighlighter),{loadableGenerated:{webpack:()=>[9020]},ssr:!1,loading:()=>(0,i.jsx)("div",{className:"animate-pulse border-transparent p-4",children:(0,i.jsxs)("div",{className:"space-y-3",children:[(0,i.jsx)("div",{className:"h-4 bg-gray-100 dark:bg-zinc-800 rounded w-3/4"}),(0,i.jsx)("div",{className:"h-4 bg-gray-100 dark:bg-zinc-800 rounded w-1/2"}),(0,i.jsx)("div",{className:"h-4 bg-gray-100 dark:bg-zinc-800 rounded w-5/6"}),(0,i.jsx)("div",{className:"h-4 bg-gray-100 dark:bg-zinc-800 rounded w-2/3"})]})})}),C={javascript:/(const|let|var|function|\=>|\bconsole\.log|import\s+.*\s+from\s+['"]|export\s+|async|await)/,typescript:/(interface\s+[A-Z][a-zA-Z0-9_]*|type\s+[A-Z][a-zA-Z0-9_]*|:\s*(string|number|boolean|any|void|never)|implements|extends|namespace|declare)/,python:/(import\s+[a-zA-Z_][a-zA-Z0-9_]*|from\s+[a-zA-Z_][a-zA-Z0-9_]*\s+import|def\s+[a-zA-Z_][a-zA-Z0-9_]*\s*\(|class\s+[A-Z][a-zA-Z0-9_]*|print\s*\()/,java:/(public\s+class|private\s+|protected\s+|package\s+|import\s+java\.|@Override|void\s+main|System\.out)/,c:/(#include\s*[<"].*[>"]|int\s+main\s*\(|printf\s*\(|scanf\s*\(|malloc\s*\(|void\s+\*)/,cpp:/(#include\s*[<"].*[>"]|namespace\s+|::\s*|template\s*<|std::|cout\s*<<|cin\s*>>)/,"c#":/(using\s+System|namespace\s+|public\s+class|private\s+|protected\s+|Console\.|async\s+Task)/,ruby:/(require\s+['"]|def\s+|class\s+[A-Z]|module\s+[A-Z]|attr_|puts\s+|gem\s+['"])/,php:/(<\?php|\$[a-zA-Z_]|namespace\s+|use\s+|class\s+|public\s+function|echo\s+|require_once)/,swift:/(import\s+Foundation|var\s+|let\s+|func\s+|class\s+|struct\s+|protocol\s+|extension\s+|guard\s+let)/,kotlin:/(fun\s+|val\s+|var\s+|class\s+|object\s+|suspend\s+|companion\s+object|override\s+|private\s+|import\s+)/,go:/(package\s+|import\s+|func\s+|type\s+|struct\s+|interface\s+|go\s+|chan\s+|defer\s+|goroutine)/,rust:/(fn\s+|let\s+mut|use\s+|pub\s+|impl\s+|struct\s+|enum\s+|match\s+|Option<|Result<)/,scala:/(object\s+|class\s+|trait\s+|def\s+|val\s+|var\s+|import\s+|package\s+|case\s+class|extends\s+)/,perl:/(use\s+|sub\s+|my\s+|package\s+|\$[a-zA-Z_]|\@[a-zA-Z_]|print\s+|require\s+)/,haskell:/(module\s+|import\s+|data\s+|type\s+|class\s+|instance\s+|where\s+|let\s+|in\s+|do\s+)/,lua:/(function\s+|local\s+|require\s+|module\s*\(|end\s*$|then\s*$|pairs\s*\(|ipairs\s*\()/,shell:/(#!\/bin\/|apt-get|yum install|brew install|npm|yarn|echo\s+|export\s+|source\s+|\$\{|\$\()/,sql:/(SELECT\s+|INSERT\s+INTO|UPDATE\s+|DELETE\s+FROM|CREATE\s+TABLE|ALTER\s+TABLE|DROP\s+TABLE|JOIN\s+|WHERE\s+)/i,html:/(<[^>]*>|<!DOCTYPE\s+html|<html|<script|<style|<link|<meta|<body|<head)/i,css:/(@media|@keyframes|@import|@layer|@supports|\{[^}]*:[^}]*\}|^\s*[.#][a-zA-Z])/},M=new Map;function N(e){let a=M.get(e);if(a)return a;for(let[a,t]of Object.entries(C))if(t.test(e))return M.set(e,a),a;return"file"}let A={javascript:".js",python:".py",java:".java",c:".c",cpp:".cpp","c++":".cpp","c#":".cs",ruby:".rb",php:".php",swift:".swift","objective-c":".m",kotlin:".kt",typescript:".ts",go:".go",perl:".pl",rust:".rs",scala:".scala",haskell:".hs",lua:".lua",shell:".sh",sql:".sql",html:".html",css:".css"},S=e=>{let{className:a,language:t,value:o,overrideHeader:c=!1,showLineNumbers:g=!1,title:h,enableRun:v=!1,isStreaming:f=!1,codeContainerClassName:x}=e,{isCopied:y,copyToClipboard:C}=(0,n.m)({timeout:2e3}),[M,S]=r.useState(!1),[T,U]=r.useState(null),q=k(t),H=t||(o?N(o):"file"),I=e=>Array.isArray(e)?e.join("\n"):e,L=e=>e.split("\n").filter((e,a,t)=>a!==t.length-1||""!==e.trim()).map((e,a)=>(0,i.jsxs)("div",{className:"flex gap-4",children:[(0,i.jsx)("span",{className:"text-[#8f8f8f] dark:text-[#444d56]",children:">"}),(0,i.jsx)("span",{children:e})]},a)),O=async()=>{S(!0);try{let e=await fetch("/api/sandbox",{method:"POST",headers:{"Content-Type":"application/json"},body:JSON.stringify({sourceCode:o,lang:"javascript"===t?"nodejs":t})}),a=await e.json();U(a)}catch(e){b.Am.error("Failed to execute code")}finally{S(!1)}},D=()=>{if(!o||"string"!=typeof o)return;let e=t&&A[t]||".txt",a="".concat(h?h.toLowerCase().replace(/\s+/g,"-"):"file-".concat((0,s.zs)(3,!0))).concat(e),i=window.prompt("Enter file name",a);if(!i)return;let r=new Blob([o],{type:"text/plain"}),n=URL.createObjectURL(r),l=document.createElement("a");l.download=i,l.href=n,l.style.display="none",document.body.appendChild(l),l.click(),document.body.removeChild(l),URL.revokeObjectURL(n)};return(0,i.jsxs)("div",{className:(0,s.cn)("not-prose relative overflow-hidden rounded-lg border border-gray-200",a),children:[c||(0,i.jsxs)("div",{className:"bg-gray-50 dark:bg-zinc-900 font-sans flex items-center justify-between gap-2 px-4 py-3.5 border-b border-gray-200 dark:border-zinc-800 text-gray-900 dark:text-white",children:[v?(0,i.jsx)("span",{className:"text-sm px-1.5 py-1 rounded-md bg-gray-100 dark:bg-zinc-800 max-w-[250px] truncate",children:h}):(0,i.jsx)("span",{className:"text-sm",children:q}),(0,i.jsx)("div",{className:"flex gap-2 items-center",children:f?(0,i.jsx)(w,{size:16,className:"text-gray-900 dark:text-white"}):(0,i.jsxs)(i.Fragment,{children:[v&&(0,i.jsx)("span",{className:"text-sm",children:q}),(0,i.jsx)(d.z,{className:"p-1 border-none bg-gray-50 dark:bg-zinc-900 hover:bg-gray-100 dark:hover:bg-zinc-800 text-gray-900 dark:text-gray-600",onClick:D,children:(0,i.jsx)(m.U,{className:"h-4 w-4"})}),(0,i.jsx)(d.z,{className:"p-1 border-none bg-gray-50 dark:bg-zinc-900 hover:bg-gray-100 dark:hover:bg-zinc-800 text-gray-900 dark:text-gray-600",onClick:()=>C(o||""),children:y?(0,i.jsx)(l.J,{className:"h-4 w-4 text-gray-900 dark:fill-white"}):(0,i.jsx)(p.C,{className:"h-4 w-4 text-gray-900 dark:fill-white"})}),v&&(0,i.jsx)(d.z,{className:"p-1 border-none bg-zinc-900 hover:bg-zinc-800 dark:bg-white dark:hover:bg-gray-50 text-white dark:text-zinc-900 ml-1",onClick:O,disabled:M,children:M?(0,i.jsx)(w,{size:16}):(0,i.jsx)(u.k,{className:"h-4 w-4"})})]})})]}),(0,i.jsx)("div",{className:(0,s.cn)("overflow-x-auto bg-white dark:bg-[#0A0A0A] text-sm transition-colors duration-200",x),children:(0,i.jsx)(P,{language:H,showLineNumbers:g,children:o||""})}),T&&(0,i.jsx)("div",{className:"border-t border-gray-100 dark:border-zinc-800/50 bg-gray-50 dark:bg-zinc-900 text-sm text-black dark:text-white p-4 font-mono max-h-[300px] overflow-y-auto",children:0===T.$$attachments.length&&0===T.stdout.length&&0===T.stderr.length?(0,i.jsx)("div",{children:"Nothing was printed to console"}):(0,i.jsxs)(i.Fragment,{children:[T.$$attachments.length>0&&(0,i.jsx)("div",{className:"space-y-0.5",children:T.$$attachments.map((e,a)=>{let[t,r]=e;return(0,i.jsxs)("div",{className:"flex gap-4",children:[(0,i.jsx)("span",{className:"text-[#8f8f8f] dark:text-[#444d56]",children:">"}),(0,i.jsxs)("span",{children:[t,": ",r]})]},a)})}),T.stdout.length>0&&(0,i.jsx)("div",{className:"space-y-0.5",children:T.stdout.map((e,a)=>(0,i.jsx)("div",{className:"space-y-0.5",children:L(I(e))},a))}),T.stderr.length>0&&(0,i.jsx)("div",{className:"space-y-0.5",children:T.stderr.map((e,a)=>(0,i.jsx)("div",{className:"space-y-0.5",children:L(I(e))},a))})]})})]})},T=(0,r.memo)(S,(e,a)=>{var t,i;return e.language===a.language&&e.overrideHeader===a.overrideHeader&&!(((null===(t=e.value)||void 0===t?void 0:t.length)||0)<((null===(i=a.value)||void 0===i?void 0:i.length)||0))&&e.value===a.value});T.displayName="CodeBlock";let U=T},35146:(e,a,t)=>{"use strict";t.d(a,{L:()=>l});var i=t(66195),r=t(18431),o=t(5427);let n={Cohere:{url:"/icons/cohere.svg"},OpenAI:{url:"/icons/openai.svg",logo:o.IB},Parasail:{url:"/icons/parasail.svg",logo:o.OI},"Hugging Face":{url:"/icons/huggingface.svg"},Alibaba:{url:"/icons/qwen.svg"},"Alibaba Cloud":{url:"/icons/alibaba.svg",logo:o.RE},Anthropic:{url:"/icons/anthropic.svg",logo:o.Vw},Azure:{url:"/icons/azure.svg",logo:o.Nz},Meta:{url:"/icons/meta.svg"},Mistral:{url:"/icons/mistral.svg",logo:o._p},Fireworks:{url:"/icons/fireworks.svg",logo:o.Sl},"DeepSeek hosted on Fireworks":{url:"/icons/fireworks.svg",logo:o.Sl},Google:{url:"/icons/google.svg",logo:o._8},Groq:{url:"/icons/groq.svg"},Custom:{url:"/icons/custom.svg"},Amazon:{url:"/icons/nova.svg"},Bedrock:{url:"/icons/nova.svg"},Vercel:{url:"/icons/vercel.svg",logo:o.UE},Nvidia:{url:"/icons/nvidia.svg"},xAI:{url:"/icons/xai.svg",logo:o.Ar},DeepSeek:{url:"/icons/deepseek.svg",logo:o.vi},Perplexity:{url:"/icons/perplexity.svg"},Qwen:{url:"/icons/qwen.svg"},DeepInfra:{url:"/icons/deepinfra.svg",logo:o.C1},Inception:{url:"/icons/inception.svg",logo:o._z},Cerebras:{url:"/icons/cerebras.svg",logo:o.iI},Vertex:{url:"/icons/vertex.svg"},"Google Vertex AI":{url:"/icons/vertex.svg"},Morph:{url:"/icons/morph.png"},"Moonshot AI":{url:"/icons/moonshotai.png",logo:o.gB},"Z.ai":{url:"/icons/zai.svg"},"Z.AI":{url:"/icons/zai.svg"},Baseten:{url:"/icons/baseten.png"},Novita:{url:"/icons/novita.svg"},"Novita AI":{url:"/icons/novita.svg"},Stealth:{url:"/icons/stealth.svg",logo:o.qv},Meituan:{url:"/icons/meituan.svg",logo:o.Pk},"Meituan LongCat":{url:"/icons/meituan.svg",logo:o.Pk},"Voyage AI":{url:"/icons/voyageai.svg"},Chutes:{url:"",logo:o.fP}};function s(e){let a="huggingface"===e.provider?"Hugging Face":e.makerHumanName;return n[a]||(console.warn("No icon found for ".concat(a)),{url:"/icons/custom.svg"})}function l(e){let a=s(e.model);return a.logo?(0,i.jsx)("div",{className:e.className,children:(0,i.jsx)(a.logo,{size:16})}):(0,i.jsx)(r.default,{src:a.url,className:"w-4 h-4",alt:"",height:16,width:16,...e})}},29374:(e,a,t)=>{"use strict";t.r(a),t.d(a,{MemoizedReactMarkdown:()=>u});var i=t(66195),r=t(73762),o=t(75342),n=t(70133),s=t(56555),l=t(73541);let p={code:c,table:d,th:g,td:h,p:e=>{let{node:a,children:t,...r}=e,o="string"==typeof t?t.trim():t;return(0,i.jsx)("p",{...r,children:o})}},m=[[n.Z,{}],[s.Z,{}]],u=(0,r.memo)(e=>{let{...a}=e;return(0,i.jsx)(o.D,{...a,components:p,remarkPlugins:m})},(e,a)=>e.children===a.children);function d(e){let{children:a}=e;return(0,i.jsx)("table",{className:"px-3 py-1 border border-collapse border-black ",children:a})}function c(e){let{node:a,inline:t,className:r,children:o,...n}=e;if(o.length){if("▍"===o[0])return(0,i.jsx)("span",{className:"mt-1 cursor-default animate-pulse",children:"▍"});o[0]=o[0].replace("`▍`","▍")}let s=/language-(\w+)/.exec(r||"");if(t)return(0,i.jsx)("code",{className:r,...n,children:o});let p=String(o).replace(/\n$/,"");return(0,i.jsx)(l.default,{language:(null==s?void 0:s[1])||"",value:p,...n},Math.random())}function g(e){let{children:a}=e;return(0,i.jsx)("th",{className:"px-3 py-1 text-white break-words bg-gray-500 border border-black ",children:a})}function h(e){let{children:a}=e;return(0,i.jsx)("td",{className:"px-3 py-1 break-words border border-black",children:a})}u.displayName="MemoizedReactMarkdown"},42813:(e,a,t)=>{"use strict";t.d(a,{gN:()=>d,JC:()=>c});var i=t(66195),r=t(73762),o=t(84627),n=t(60038);let s=r.forwardRef((e,a)=>{let{className:t,...r}=e;return(0,i.jsxs)(o.fC,{ref:a,className:(0,n.cn)("relative flex w-full touch-none select-none items-center",t),...r,children:[(0,i.jsx)(o.fQ,{className:"relative h-[5px] w-full grow overflow-hidden rounded-full bg-zinc-200 dark:bg-zinc-800",children:(0,i.jsx)(o.e6,{className:"absolute h-full bg-zinc-900 dark:bg-zinc-100 rounded-full"})}),(0,i.jsx)(o.bU,{className:"block h-4 w-4 rounded-full shadow-[0_4px_6px_-1px_rgba(0,0,0,.1),0_2px_4px_-2px_rgba(0,0,0,.1),0_0_0_1px_rgba(0,0,0,0.2)] bg-white transition focus:outline-none focus:ring-[3px] focus:ring-zinc-600/40 disabled:pointer-events-none disabled:opacity-50 "})]})});s.displayName=o.fC.displayName;var l=t(13458),p=t(32964),m=t(59454);function u(e){return e%1==0?e:e.toFixed(2)}function d(e){let{parameters:a,id:t,name:r,updateModelParameter:o,step:d,sliderStep:c=d,readonly:g,description:h}=e,v=a[t];if(void 0===v)throw Error("Expected parameter for field '".concat(t,"' to be defined."));return(0,i.jsxs)("div",{className:"space-y-2",children:[(0,i.jsxs)("div",{className:"flex items-center justify-between",children:[(0,i.jsxs)("label",{htmlFor:t,className:"inline-flex items-center text-sm font-medium text-zinc-700 dark:text-zinc-200",children:[r,h?(0,i.jsx)(p.u,{content:h,children:(0,i.jsx)(m.Z,{className:"w-4 h-4 mt-[2px] ml-0.5 text-zinc-500 dark:text-zinc-400"})}):null]}),(0,i.jsx)("input",{id:t,type:"number",className:(0,n.cn)("py-0 px-1 text-right tabular-nums border border-transparent w-[64px] font-medium text-zinc-700 text-sm tracking-tight rounded dark:bg-black dark:text-zinc-300",g?"focus:outline-none focus:ring-0 focus:border-transparent cursor-default":"focus:outline-none focus:ring-0 hover:border-zinc-200 focus:border-zinc-400 dark:hover:border-zinc-700 dark:focus:border-zinc-500"),style:{fontFamily:l.u},value:u(v.value),min:v.range[0],max:v.range[1],step:d,onChange:e=>o(t,Number(e.target.value)),readOnly:g})]}),(0,i.jsx)(s,{value:[v.value],step:c,min:v.range[0],max:v.range[1],onValueChange:e=>o(t,e[0])})]})}function c(e){let{readonly:a,model:t,updateModelParameter_:r}=e,o=a?()=>{}:r;return(0,i.jsxs)(i.Fragment,{children:[t.parameters.maximumLength?(0,i.jsx)(d,{id:"maximumLength",name:"Max Output Tokens",parameters:t.parameters,updateModelParameter:o,step:1,sliderStep:24,readonly:a,description:g.maxLengthDescription}):null,t.parameters.temperature?(0,i.jsx)(d,{id:"temperature",name:"Temperature",parameters:t.parameters,updateModelParameter:o,step:.01,readonly:a,description:g.temperatureDescription}):null,t.parameters.typicalP?(0,i.jsx)(d,{id:"typicalP",name:"Typical P",parameters:t.parameters,updateModelParameter:o,step:.01,readonly:a,description:g.typicalPDescription}):null,t.parameters.topP?(0,i.jsx)(d,{id:"topP",name:"Top P",parameters:t.parameters,updateModelParameter:o,step:.01,readonly:a,description:g.topPDescription}):null,t.parameters.topK?(0,i.jsx)(d,{id:"topK",name:"Top K",parameters:t.parameters,updateModelParameter:o,step:1,readonly:a,description:g.topKDescription}):null,t.parameters.frequencyPenalty?(0,i.jsx)(d,{id:"frequencyPenalty",name:"Frequency Penalty",parameters:t.parameters,updateModelParameter:o,step:.01,readonly:a,description:g.frequencyPenaltyDescription}):null,t.parameters.presencePenalty?(0,i.jsx)(d,{id:"presencePenalty",name:"Presence Penalty",parameters:t.parameters,updateModelParameter:o,step:.01,readonly:a,description:g.presencePenaltyDescription}):null,t.parameters.repetitionPenalty?(0,i.jsx)(d,{id:"repetitionPenalty",name:"Repetition Penalty",parameters:t.parameters,updateModelParameter:o,step:.01,readonly:a,description:g.repetitionPenaltyDescription}):null]})}let g={maxLengthDescription:"The maximum number of tokens to return",temperatureDescription:"Controls the randomness of the returned text; lower is less random",typicalPDescription:"How similar the conditional probability for predicting the next target token is",topPDescription:"The cumulative probability of the most likely tokens to return",topKDescription:"The size of the set of most-likely tokens to sample from",frequencyPenaltyDescription:"How much to penalize tokens based on their frequency in the text so far",presencePenaltyDescription:"How much to penalize tokens based on if they have appeared in the text so far",repetitionPenaltyDescription:"How much to penalize tokens based on if they have appeared in the text so far"}},44869:(e,a,t)=>{"use strict";t.d(a,{$:()=>o});var i=t(66195),r=t(60038);function o(e){let{className:a="w-6 h-6"}=e;return(0,i.jsxs)("svg",{xmlns:"http://www.w3.org/2000/svg",className:(0,r.cn)("absolute inset-0 m-auto flex-1 animate-spin stroke-current stroke-[2]",a),fill:"none",viewBox:"0 0 24 24",children:[(0,i.jsx)("path",{d:"M12 22C17.5228 22 22 17.5228 22 12C22 6.47715 17.5228 2 12 2C6.47715 2 2 6.47715 2 12C2 17.5228 6.47715 22 12 22Z",className:"stroke-current opacity-25"}),(0,i.jsx)("path",{d:"M12 2C6.47715 2 2 6.47715 2 12C2 14.7255 3.09032 17.1962 4.85857 19"})]})}},32964:(e,a,t)=>{"use strict";t.d(a,{u:()=>o});var i=t(66195);t(73762);var r=t(18477);function o(e){let{content:a,children:t}=e;return(0,i.jsx)(r.zt,{delayDuration:300,skipDelayDuration:300,children:(0,i.jsxs)(r.fC,{children:[(0,i.jsx)(r.xz,{asChild:!0,children:t}),(0,i.jsx)(r.h_,{children:(0,i.jsx)(r.VY,{className:"data-[state=delayed-open]:data-[side=top]:animate-slideDownAndFade data-[state=closed]:data-[side=top]:animate-slideUpAndFadeOut select-none rounded text-zinc-600 bg-white/80 backdrop-blur px-3 py-2.5 text-xs font-medium leading-none z-50 shadow-[hsl(206_22%_7%_/_35%)_0px_10px_38px_-10px,_hsl(206_22%_7%_/_20%)_0px_10px_20px_-15px,0_0_0_1px_hsl(206_22%_7%_/_8%)] dark:text-zinc-300 dark:bg-zinc-800/80 dark:shadow-[hsl(206_22%_7%_/_35%)_0px_10px_38px_-10px,_hsl(206_22%_7%_/_20%)_0px_10px_20px_-15px,0_0_0_1px_hsl(206_22%_7%_/_30%)] hidden md:block",sideOffset:5,collisionPadding:10,children:a})})]})})}},13458:(e,a,t)=>{"use strict";t.d(a,{W3:()=>i,u:()=>r});let i="var(--font-geist-sans)",r="var(--font-geist-mono)"},63528:(__unused_webpack_module,__webpack_exports__,__webpack_require__)=>{"use strict";__webpack_require__.d(__webpack_exports__,{j:()=>useAntibotToken});var swr__WEBPACK_IMPORTED_MODULE_0__=__webpack_require__(39960);function useAntibotToken(){let{data,mutate,isValidating}=(0,swr__WEBPACK_IMPORTED_MODULE_0__.ZP)("at",async()=>{let response=await fetch("/openai.jpeg"),text=await response.text(),json=fromBinary(text),data=JSON.parse(json),ret=eval("(".concat(data.c,")(data.a)"));return toBinary(JSON.stringify({r:ret,t:data.t}))},{fallbackData:"",refreshInterval:6e4,dedupingInterval:2e3});return[data,mutate]}function toBinary(e){let a=new Uint16Array(e.length);for(let t=0;t<a.length;t++)a[t]=e.charCodeAt(t);return btoa(String.fromCharCode(...new Uint8Array(a.buffer)))}function fromBinary(e){let a=atob(e),t=new Uint8Array(a.length);for(let e=0;e<t.length;e++)t[e]=a.charCodeAt(e);return String.fromCharCode(...new Uint16Array(t.buffer))}},15727:e=>{e.exports={wrapper:"geist-spinner_wrapper__P_AnS",spinner:"geist-spinner_spinner__FRTTp",bar:"geist-spinner_bar__G0s_L",spin:"geist-spinner_spin__OwAcC"}}}]);