AI SDK 5 is available now.










Menu






















































































































































































































































































































































































































































































# [DeepSeek Provider](#deepseek-provider)



## [Setup](#setup)

The DeepSeek provider is available via the `@ai-sdk/deepseek` module. You can install it with:






pnpm







npm







yarn







bun








``` geist-overflow-scroll-y
pnpm add @ai-sdk/deepseek
```












## [Provider Instance](#provider-instance)

You can import the default provider instance `deepseek` from `@ai-sdk/deepseek`:



``` ts
import  from '@ai-sdk/deepseek';
```


For custom configuration, you can import `createDeepSeek` and create a provider instance with your settings:



``` ts
import  from '@ai-sdk/deepseek';
const deepseek = createDeepSeek();
```


You can use the following optional settings to customize the DeepSeek provider instance:

- **baseURL** *string*

  Use a different URL prefix for API calls. The default prefix is `https://api.deepseek.com/v1`.

- **apiKey** *string*

  API key that is being sent using the `Authorization` header. It defaults to the `DEEPSEEK_API_KEY` environment variable.

- **headers** *Record\<string,string\>*

  Custom headers to include in the requests.

- **fetch** *(input: RequestInfo, init?: RequestInit) =\> Promise\<Response\>*


## [Language Models](#language-models)

You can create language models using a provider instance:



``` ts
import  from '@ai-sdk/deepseek';import  from 'ai';
const  = await generateText();
```


You can also use the `.chat()` or `.languageModel()` factory methods:



``` ts
const model = deepseek.chat('deepseek-chat');// orconst model = deepseek.languageModel('deepseek-chat');
```


DeepSeek language models can be used in the `streamText` function (see [AI SDK Core](../../docs/ai-sdk-core.html)).

### [Reasoning](#reasoning)

DeepSeek has reasoning support for the `deepseek-reasoner` model. The reasoning is exposed through streaming:



``` ts
import  from '@ai-sdk/deepseek';import  from 'ai';
const result = streamText();
for await (const part of result.fullStream)  else if (part.type === 'text') }
```


See [AI SDK UI: Chatbot](../../docs/ai-sdk-ui/chatbot.html#reasoning) for more details on how to integrate reasoning into your chatbot.

### [Cache Token Usage](#cache-token-usage)

DeepSeek provides context caching on disk technology that can significantly reduce token costs for repeated content. You can access the cache hit/miss metrics through the `providerMetadata` property in the response:



``` ts
import  from '@ai-sdk/deepseek';import  from 'ai';
const result = await generateText();
console.log(result.providerMetadata);// Example output:  }
```


The metrics include:

- `promptCacheHitTokens`: Number of input tokens that were cached
- `promptCacheMissTokens`: Number of input tokens that were not cached







## [Model Capabilities](#model-capabilities)


| Model | Text Generation | Object Generation | Image Input | Tool Usage | Tool Streaming |
|----|----|----|----|----|----|























On this page

































Vercel delivers the infrastructure and developer experience you need to ship reliable AI-powered applications at scale.

Trusted by industry leaders:















#### Resources




#### More




#### About Vercel




#### Legal







Â© 2025 Vercel, Inc.