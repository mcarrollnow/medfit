AI SDK 5 is available now.










Menu






















































































































































































































































































































































































































































































# [Ollama Provider](#ollama-provider)


## [Setup](#setup)

The Ollama provider is available in the `ollama-ai-provider-v2` module. You can install it with






pnpm







npm







yarn







bun








``` geist-overflow-scroll-y
pnpm add ollama-ai-provider-v2
```












## [Provider Instance](#provider-instance)

You can import the default provider instance `ollama` from `ollama-ai-provider-v2`:



``` ts
import  from 'ollama-ai-provider-v2';
```


If you need a customized setup, you can import `createOllama` from `ollama-ai-provider-v2` and create a provider instance with your settings:



``` ts
import  from 'ollama-ai-provider-v2';
const ollama = createOllama();
```


You can use the following optional settings to customize the Ollama provider instance:

- **baseURL** *string*

  Use a different URL prefix for API calls, e.g. to use proxy servers. The default prefix is `http://localhost:11434/api`.

- **headers** *Record\<string,string\>*

  Custom headers to include in the requests.

## [Language Models](#language-models)




``` ts
const model = ollama('phi3');
```



### [Model Capabilities](#model-capabilities)

This provider is capable of using hybrid reasoning models such as qwen3, allowing toggling of reasoning between messages.



``` ts
import  from 'ollama-ai-provider-v2';import  from 'ai';
const  = await generateText( },  prompt:    'Write a vegetarian lasagna recipe for 4 people, but really think about it',});
```


## [Embedding Models](#embedding-models)




``` ts
const model = ollama.textEmbeddingModel('nomic-embed-text');
const  = await embedMany();
console.log(  `cosine similarity: $`,);
```


## [Alternative Providers](#alternative-providers)


Key differences:

- Uses the official `ollama` npm package for communication
- Provides automatic environment detection (Node.js vs browser)
- Includes built-in error handling and retries via the official client
- Supports both CommonJS and ESM module formats
- Full TypeScript support with type-safe Ollama-specific options via `providerOptions.ollama`

This approach leverages Ollama's official client library, which may provide better compatibility with Ollama updates and additional features like model management. Both providers implement the AI SDK specification, so you can choose based on your specific requirements and preferences.
















On this page

































Vercel delivers the infrastructure and developer experience you need to ship reliable AI-powered applications at scale.

Trusted by industry leaders:















#### Resources




#### More




#### About Vercel




#### Legal







Â© 2025 Vercel, Inc.