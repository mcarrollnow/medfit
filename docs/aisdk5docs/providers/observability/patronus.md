AI SDK 5 is available now.










Menu






















































































































































































































































































































































































































































































# [Patronus Observability](#patronus-observability)


When you build with the AI SDK, you can stream OpenTelemetry (OTEL) traces straight into Patronus and pair every generation with rich automatic evaluations.

## [Setup](#setup)

### [1. OpenTelemetry](#1-opentelemetry)

Patronus exposes a fully‑managed OTEL endpoint. Configure an **OTLP exporter** to point at it, pass your API key, and you’re done—Patronus will automatically convert LLM spans into prompt/response records you can explore and evaluate.

#### [Environment variables (recommended)](#environment-variables-recommended)













``` bash
OTEL_EXPORTER_OTLP_ENDPOINT=https://otel.patronus.ai/v1/tracesOTEL_EXPORTER_OTLP_HEADERS="x-api-key:<PATRONUS_API_KEY>"
```


#### [With `@vercel/otel`](#with-vercelotel)












``` ts
import  from '@vercel/otel';import  from '@opentelemetry/exporter-trace-otlp-http';import  from '@opentelemetry/sdk-trace-node';
export function register() ,        }),      ),    ],  });}
```





If you need gRPC instead of HTTP, swap the exporter for `@opentelemetry/exporter-trace-otlp-grpc` and use `https://otel.patronus.ai:4317`.



### [2. Enable telemetry on individual calls](#2-enable-telemetry-on-individual-calls)

The AI SDK emits a span only when you opt in with `experimental_telemetry`:



``` ts
import  from 'ai';import  from '@ai-sdk/openai';
const result = await generateText(,  },});
```


Every attribute inside `metadata` becomes an OTEL attribute and is indexed by Patronus for filtering.

## [Example — tracing and automated evaluation](#example--tracing-and-automated-evaluation)












``` ts
import  from '@opentelemetry/api';import  from 'ai';import  from '@ai-sdk/openai';
export async function POST(req: Request) ,      });
      /* 2️⃣ run Patronus evaluation inside the same trace */      await fetch('https://api.patronus.ai/v1/evaluate', ,        body: JSON.stringify(,          ],          evaluated_model_input: body.prompt,          evaluated_model_output: answer.text,          trace_id: span.spanContext().traceId,          span_id: span.spanContext().spanId,        }),      });
      return new Response(answer.text);    } finally   });}
```


Result: a single trace containing the root HTTP request, the LLM generation span, and your evaluation span—**all visible in Patronus** with the hallucination score attached.

## [Once you've traced](#once-youve-traced)


## [Resources](#resources)

















On this page









































Vercel delivers the infrastructure and developer experience you need to ship reliable AI-powered applications at scale.

Trusted by industry leaders:















#### Resources




#### More




#### About Vercel




#### Legal







© 2025 Vercel, Inc.