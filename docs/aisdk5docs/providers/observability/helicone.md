AI SDK 5 is available now.










Menu






















































































































































































































































































































































































































































































# [Helicone Observability](#helicone-observability)


## [Setup](#setup)

Setting up Helicone:


2.  Set your API key as an environment variable:













    ``` bash
    HELICONE_API_KEY=your-helicone-api-key
    ```


3.  Update your model provider configuration to use Helicone's proxy:


    ``` javascript
    import  from '@ai-sdk/openai';
    const openai = createOpenAI(`,  },});
    // Use normally with AI SDKconst response = await generateText();
    ```


That's it! Your requests are now being logged and monitored through Helicone.


## [Integration Approach](#integration-approach)

While other observability solutions require OpenTelemetry instrumentation, Helicone uses a simple proxy approach:






Helicone Proxy (3 lines)







Typical OTEL Setup (simplified)








``` javascript
const openai = createOpenAI(` },});
```




**Characteristics of Helicone's Proxy Approach:**

- No additional packages required
- Compatible with JavaScript environments
- Minimal code changes to existing implementations
- Supports features such as caching and rate limiting


## [Core Features](#core-features)

### [User Tracking](#user-tracking)

Monitor how individual users interact with your AI application:



``` javascript
const response = await generateText(,});
```



### [Custom Properties](#custom-properties)

Add structured metadata to filter and analyze requests:



``` javascript
const response = await generateText(,});
```



### [Session Tracking](#session-tracking)

Group related requests into coherent conversations:



``` javascript
const response = await generateText(,});
```



## [Advanced Configuration](#advanced-configuration)

### [Request Caching](#request-caching)

Reduce costs by caching identical requests:



``` javascript
const response = await generateText(,});
```



### [Rate Limiting](#rate-limiting)

Control usage by adding a rate limit policy:



``` javascript
const response = await generateText(,});
```


Format: `[quota];w=[time_window];u=[unit];s=[segment]` where:

- `quota`: Maximum requests allowed in the time window
- `w`: Time window in seconds (minimum 60s)
- `u`: Optional unit - "request" (default) or "cents"
- `s`: Optional segment - "user", custom property, or global (default)


### [LLM Security](#llm-security)

Protect against prompt injection, jailbreaking, and other LLM-specific threats:



``` javascript
const response = await generateText(,});
```


Protects against multiple attack vectors in 8 languages with minimal latency. Advanced mode adds protection across 14 threat categories.


## [Resources](#resources)

















On this page


















































Vercel delivers the infrastructure and developer experience you need to ship reliable AI-powered applications at scale.

Trusted by industry leaders:















#### Resources




#### More




#### About Vercel




#### Legal







Â© 2025 Vercel, Inc.