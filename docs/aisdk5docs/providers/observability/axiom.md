AI SDK 5 is available now.










Menu






















































































































































































































































































































































































































































































# [Axiom Observability](#axiom-observability)

**Axiom** is a data platform with specialized features for **AI engineering workflows**, helping you build sophisticated AI systems with confidence.

Axiom’s integration with the AI SDK uses a model wrapper to automatically capture detailed traces for every LLM call, giving you immediate visibility into your application's performance, cost, and behavior.

## [Setup](#setup)

### [1. Configure Axiom](#1-configure-axiom)

First, you'll need an Axiom organization, a dataset to send traces to, and an API token.


### [2. Install the Axiom SDK](#2-install-the-axiom-sdk)

Install the Axiom package in your project:






pnpm







npm







yarn







bun








``` geist-overflow-scroll-y
pnpm add axiom
```












### [3. Set Environment Variables](#3-set-environment-variables)

Configure your environment variables in a `.env` file. This uses the standard OpenTelemetry configuration to send traces directly to your Axiom dataset.













``` bash
# Axiom ConfigurationAXIOM_TOKEN="YOUR_AXIOM_API_TOKEN"AXIOM_DATASET="your-axiom-dataset-name"
# Vercel and OpenTelemetry ConfigurationOTEL_SERVICE_NAME="my-ai-app"OTEL_EXPORTER_OTLP_ENDPOINT="https://api.axiom.co/v1/traces"OTEL_EXPORTER_OTLP_HEADERS="Authorization=Bearer YOUR_AXIOM_API_TOKEN,X-Axiom-Dataset=your-axiom-dataset-name"
# Your AI Provider KeyOPENAI_API_KEY="YOUR_OPENAI_API_KEY"
```


Replace the placeholder values with your actual Axiom token and dataset name.

### [4. Set Up Instrumentation](#4-set-up-instrumentation)

To send data to Axiom, configure a tracer. For example, use a dedicated instrumentation file and load it before the rest of your app. An example configuration for a Node.js environment:

1.  Install dependencies:






pnpm







npm







yarn







bun








``` geist-overflow-scroll-y
pnpm i dotenv @opentelemetry/exporter-trace-otlp-http @opentelemetry/resources @opentelemetry/sdk-node @opentelemetry/sdk-trace-node @opentelemetry/semantic-conventions @opentelemetry/api
```












2.  Create instrumentation file:












``` typescript
import  from '@opentelemetry/api';import  from '@opentelemetry/exporter-trace-otlp-http';import type  from '@opentelemetry/resources';import  from '@opentelemetry/resources';import  from '@opentelemetry/sdk-node';import  from '@opentelemetry/sdk-trace-node';import  from '@opentelemetry/semantic-conventions';import  from 'axiom/ai';
const tracer = trace.getTracer('my-tracer');
const sdk = new NodeSDK() as Resource,  spanProcessor: new SimpleSpanProcessor(    new OTLPTraceExporter(`,        'X-Axiom-Dataset': process.env.AXIOM_DATASET,      },    }),  ),});
sdk.start();
initAxiomAI();
```


### [5. Wrap and Use the AI Model](#5-wrap-and-use-the-ai-model)

In your application code, import `wrapAISDKModel` from Axiom and use it to wrap your existing AI SDK model client.



``` typescript
import  from '@ai-sdk/openai';import  from 'ai';import  from 'axiom/ai';
// 1. Create your standard AI model providerconst openaiProvider = createOpenAI();
// 2. Wrap the model to enable automatic tracingconst tracedGpt4o = wrapAISDKModel(openaiProvider('gpt-4o'));
// 3. Use the wrapped model as you normally wouldconst  = await generateText();
console.log(text);
```


Any calls made using the `tracedGpt4o` model will now automatically send detailed traces to your Axiom dataset.

## [What You'll See in Axiom](#what-youll-see-in-axiom)

Once integrated, your Axiom dataset will include:

- **AI Trace Waterfall:** A dedicated view to visualize single and multi-step LLM workflows.
- **Gen AI Dashboard:** A pre-built dashboard to monitor cost, latency, token usage, and error rates.
- **Detailed Spans:** Rich telemetry for every call, including the full prompt and completion, token counts, and model information.

## [Advanced Usage](#advanced-usage)

Axiom’s AI SDK offers more advanced instrumentation for deeper visibility:

- **Business Context:** Use the `withSpan` function to group LLM calls under a specific business capability (e.g., `customer_support_agent`).
- **Tool Tracing:** Use the `wrapTool` helper to automatically trace the execution of tools your AI model calls.


## [Additional Resources](#additional-resources)

















On this page





















































Vercel delivers the infrastructure and developer experience you need to ship reliable AI-powered applications at scale.

Trusted by industry leaders:















#### Resources




#### More




#### About Vercel




#### Legal







© 2025 Vercel, Inc.