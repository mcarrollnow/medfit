AI SDK 5 is available now.










Menu






















































































































































































































































































































































































































































































# [LangWatch Observability](#langwatch-observability)


## [Setup](#setup)







pnpm







npm







yarn







bun








``` geist-overflow-scroll-y
pnpm add langwatch
```












Ensure `LANGWATCH_API_KEY` is set:






Environment variables







Client parameters


















``` bash
LANGWATCH_API_KEY='your_api_key_here'
```




## [Basic Concepts](#basic-concepts)

  - A span can be an LLM call, a database query for a RAG retrieval, or a simple function transformation.

## [Configuration](#configuration)

The AI SDK supports tracing via Next.js OpenTelemetry integration. By using the `LangWatchExporter`, you can automatically collect those traces to LangWatch.

First, you need to install the necessary dependencies:



``` bash
npm install @vercel/otel langwatch @opentelemetry/api-logs @opentelemetry/instrumentation @opentelemetry/sdk-logs
```


Then, set up the OpenTelemetry for your application, follow one of the tabs below depending whether you are using AI SDK with Next.js or on Node.js:






Next.js







Node.js






You need to enable the `instrumentationHook` in your `next.config.js` file if you haven't already:



``` javascript
/** @type  */const nextConfig = ,};
module.exports = nextConfig;
```


Next, you need to create a file named `instrumentation.ts` (or `.js`) in the **root directory** of the project (or inside `src` folder if using one), with `LangWatchExporter` as the traceExporter:



``` typescript
import  from '@vercel/otel';import  from 'langwatch';
export function register() );}
```



Finally, enable `experimental_telemetry` tracking on the AI SDK calls you want to trace:



``` typescript
const result = await generateText(,  },});
```




That's it! Your messages will now be visible on LangWatch:

![AI SDK](../../../mintlify.s3.us-west-1.amazonaws.com/langwatch/images/integration/vercel-ai-sdk.png)

### [Example Project](#example-project)


### [Manual Integration](#manual-integration)

The docs from here below are for manual integration, in case you are not using the AI SDK OpenTelemetry integration, you can manually start a trace to capture your messages:



``` typescript
import  from 'langwatch';
const langwatch = new LangWatch();
const trace = langwatch.getTrace(,});
```


Then, you can start an LLM span inside the trace with the input about to be sent to the LLM.



``` typescript
const span = trace.startLLMSpan(,});
```


This will capture the LLM input and register the time the call started. Once the LLM call is done, end the span to get the finish timestamp to be registered, and capture the output and the token metrics, which will be used for cost calculation, e.g.:



``` typescript
span.end(,  metrics: ,});
```


## [Resources](#resources)

For more information and examples, you can read more below:


## [Support](#support)

If you have questions or need help, join our community:

- [Email support](mailto:support@langwatch.ai)
















On this page




































Vercel delivers the infrastructure and developer experience you need to ship reliable AI-powered applications at scale.

Trusted by industry leaders:















#### Resources




#### More




#### About Vercel




#### Legal







Â© 2025 Vercel, Inc.