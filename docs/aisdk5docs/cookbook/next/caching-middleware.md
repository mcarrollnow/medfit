AI SDK 5 is available now.










Menu


























































































































































































































































































































































































# [Caching Middleware](#caching-middleware)






Let's create a simple chat interface that uses [`LanguageModelMiddleware`](../../docs/ai-sdk-core/middleware.html) to cache the assistant's responses in fast KV storage.

## [Client](#client)

Let's create a simple chat interface that allows users to send messages to the assistant and receive responses. You will integrate the `useChat` hook from `@ai-sdk/react` to stream responses.












``` tsx
'use client';
import  from '@ai-sdk/react';
export default function Chat()  = useChat();  if (error) return <div></div>;
  return (    <div className="flex flex-col w-full max-w-md py-24 mx-auto stretch">      <div className="space-y-4">         className="whitespace-pre-wrap">            <div>              <div className="font-bold"></div>              </pre>              ) : (                <p></p>              )}            </div>          </div>        ))}      </div>
      <form onSubmit=>        <input          className="fixed bottom-0 w-full max-w-md p-2 mb-8 border border-gray-300 rounded shadow-xl"          value=          placeholder="Say something..."          onChange=        />      </form>    </div>  );}
```


## [Middleware](#middleware)

Next, you will create a `LanguageModelMiddleware` that caches the assistant's responses in KV storage. `LanguageModelMiddleware` has two methods: `wrapGenerate` and `wrapStream`. `wrapGenerate` is called when using [`generateText`](../../docs/reference/ai-sdk-core/generate-text.html) and [`generateObject`](../../docs/reference/ai-sdk-core/generate-object.html), while `wrapStream` is called when using [`streamText`](../../docs/reference/ai-sdk-core/stream-text.html) and [`streamObject`](../../docs/reference/ai-sdk-core/stream-object.html).

For `wrapGenerate`, you can cache the response directly. Instead, for `wrapStream`, you cache an array of the stream parts, which can then be used with [`simulateReadableStream`](../../docs/reference/ai-sdk-core/simulate-readable-stream.html) function to create a simulated `ReadableStream` that returns the cached response. In this way, the cached response is returned chunk-by-chunk as if it were being generated by the model. You can control the initial delay and delay between chunks by adjusting the `initialDelayInMs` and `chunkDelayInMs` parameters of `simulateReadableStream`.












``` tsx
import  from '@upstash/redis';import  from 'ai';
const redis = new Redis();
export const cacheMiddleware: LanguageModelV2Middleware = ) => ,      };    }
    const result = await doGenerate();
    redis.set(cacheKey, result);
    return result;  },  wrapStream: async () => ;        } else return p;      });      return ),      };    }
    // If not cached, proceed with streaming    const  = await doStream();
    const fullResponse: LanguageModelV1StreamPart[] = [];
    const transformStream = new TransformStream<      LanguageModelV1StreamPart,      LanguageModelV1StreamPart    >(,      flush() ,    });
    return ;  },};
```





This example uses `@upstash/redis` to store and retrieve the assistant's responses but you can use any KV storage provider you would like.



## [Server](#server)

Finally, you will create an API route for `api/chat` to handle the assistant's messages and responses. You can use your cache middleware by wrapping the model with `wrapLanguageModel` and passing the middleware as an argument.












``` tsx
import  from '@/ai/middleware';import  from '@ai-sdk/openai';import  from 'ai';import  from 'zod';
const wrappedModel = wrapLanguageModel();
export async function POST(req: Request)  = await req.json();
  const result = streamText(),        execute: async () => (),      }),    },  });  return result.toUIMessageStreamResponse();}
```

















On this page






















Vercel delivers the infrastructure and developer experience you need to ship reliable AI-powered applications at scale.

Trusted by industry leaders:















#### Resources




#### More




#### About Vercel




#### Legal







Â© 2025 Vercel, Inc.