AI SDK 5 is available now.










Menu















































































































































































































































































































































































































# [Caching Responses](#caching-responses)

Depending on the type of application you're building, you may want to cache the responses you receive from your AI provider, at least temporarily.

## [Using Language Model Middleware (Recommended)](#using-language-model-middleware-recommended)

The recommended approach to caching responses is using [language model middleware](../ai-sdk-core/middleware.html) and the [`simulateReadableStream`](../reference/ai-sdk-core/simulate-readable-stream.html) function.

Language model middleware is a way to enhance the behavior of language models by intercepting and modifying the calls to the language model. Let's see how you can use language model middleware to cache responses.












``` ts
import  from '@upstash/redis';import  from 'ai';
const redis = new Redis();
export const cacheMiddleware: LanguageModelV2Middleware = ) => ,      };    }
    const result = await doGenerate();
    redis.set(cacheKey, result);
    return result;  },  wrapStream: async () => ;        } else return p;      });      return ),      };    }
    // If not cached, proceed with streaming    const  = await doStream();
    const fullResponse: LanguageModelV2StreamPart[] = [];
    const transformStream = new TransformStream<      LanguageModelV2StreamPart,      LanguageModelV2StreamPart    >(,      flush() ,    });
    return ;  },};
```





This example uses `@upstash/redis` to store and retrieve the assistant's responses but you can use any KV storage provider you would like.



`LanguageModelMiddleware` has two methods: `wrapGenerate` and `wrapStream`. `wrapGenerate` is called when using [`generateText`](../reference/ai-sdk-core/generate-text.html) and [`generateObject`](../reference/ai-sdk-core/generate-object.html), while `wrapStream` is called when using [`streamText`](../reference/ai-sdk-core/stream-text.html) and [`streamObject`](../reference/ai-sdk-core/stream-object.html).


You can see a full example of caching with Redis in a Next.js application in our [Caching Middleware Recipe](../../cookbook/next/caching-middleware.html).

## [Using Lifecycle Callbacks](#using-lifecycle-callbacks)

Alternatively, each AI SDK Core function has special lifecycle callbacks you can use. The one of interest is likely `onFinish`, which is called when the generation is complete. This is where you can cache the full response.

Here's an example of how you can implement caching using Vercel KV and Next.js to cache the OpenAI response for 1 hour:













``` tsx
import  from '@ai-sdk/openai';import  from 'ai';import  from '@upstash/redis';
// Allow streaming responses up to 30 secondsexport const maxDuration = 30;
const redis = new Redis();
export async function POST(req: Request) :  = await req.json();
  // come up with a key based on the request:  const key = JSON.stringify(messages);
  // Check if we have a cached response  const cached = await redis.get(key);  if (cached != null) ,    });  }
  // Call the language model:  const result = streamText() ,  });
  // Respond with the stream  return result.toUIMessageStreamResponse();}
```

















On this page



















Vercel delivers the infrastructure and developer experience you need to ship reliable AI-powered applications at scale.

Trusted by industry leaders:















#### Resources




#### More




#### About Vercel




#### Legal







Â© 2025 Vercel, Inc.